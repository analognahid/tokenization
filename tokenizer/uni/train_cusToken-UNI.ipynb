{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a59060-e593-41b3-a4ce-4bb340ef07b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os,re\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from num2words import num2words\n",
    "\n",
    "from trl import SFTTrainer\n",
    "# import torch\n",
    "\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "# from torch.distributed.fsdp.wrap import auto_wrap\n",
    "\n",
    "\n",
    "login(token = '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')\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "779c65c3-daa0-4075-9c9a-c0eb7ea73af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions Count:  81107 20000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "DATA_PATH = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions'\n",
    "\n",
    "TRAIN_DATA_PATH  ='/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/train/'\n",
    "\n",
    "TEST_DATA_PATH   = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/test/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_json_files = [os.path.join(TRAIN_DATA_PATH, f) for f in os.listdir(TRAIN_DATA_PATH) ]\n",
    "\n",
    "test_json_files = [os.path.join(TEST_DATA_PATH, f) for f in os.listdir(TEST_DATA_PATH) ]\n",
    "\n",
    "# disassembly_decimal disassembly_all_number_to_words disassembly_decimal\n",
    "data_key = \"disassembly_decimal\"\n",
    "\n",
    "def read_corpus(json_files):\n",
    "\n",
    "    all = []\n",
    "\n",
    "    for k, j_file in enumerate(json_files):\n",
    "        # if k>20:\n",
    "        #     break\n",
    "        try:\n",
    "\n",
    "            with open(j_file, 'r') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                funct = data[data_key]['input']\n",
    "                \n",
    "                all.append(funct)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "    return all\n",
    "    \n",
    "\n",
    "\n",
    "train_text = read_corpus(train_json_files)\n",
    "test_text  = read_corpus(test_json_files)\n",
    "\n",
    "\n",
    "        \n",
    "# text = text[0:5000]\n",
    "print(\"Functions Count: \",len(train_text), len(test_text) ,'\\n')\n",
    "example = train_text[10]\n",
    "text = train_text + test_text\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(text), 1000):\n",
    "        yield text[i : i + 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41fe2949-b651-43ac-b90e-5b10ccae8e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDBR64\n",
      "PUSH RBP\n",
      "PUSH RBX\n",
      "MOV RBX,RDI\n",
      "SUB RSP,8\n",
      "CMP qword ptr [RDI + 16],0\n",
      "JZ 5484\n",
      "LEA RBP,[8241]\n",
      "NOP dword ptr [RAX + RAX*1]\n",
      "MOV EDX,dword ptr [RBX]\n",
      "XOR EAX,EAX\n",
      "MOV RSI,RBP\n",
      "MOV EDI,2\n",
      "CALL 4288\n",
      "MOV RBX,qword ptr [RBX + 16]\n",
      "CMP qword ptr [RBX + 16],0\n",
      "JNZ 5456\n",
      "MOV EDX,dword ptr [RBX]\n",
      "ADD RSP,8\n",
      "LEA RSI,[8247]\n",
      "XOR EAX,EAX\n",
      "POP RBX\n",
      "MOV EDI,2\n",
      "POP RBP\n",
      "JMP 4288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c0932c-2cde-4148-85b7-0feb9504b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.Unigram())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "625a52cb-1c25-40e1-9600-d2f1db2ac56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7986e18-4308-462b-ae63-a6f3aea135d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Regex\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"``\", '\"'),\n",
    "        normalizers.Replace(\"''\", '\"'),\n",
    "        normalizers.NFKD(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "    ]\n",
    ")\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ac41cd-b3ab-4b8c-a000-53885499335d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDBR64\n",
      "PUSH RBP\n",
      "PUSH RBX\n",
      "MOV RBX,RDI\n",
      "SUB RSP,8\n",
      "CMP qword ptr [RDI + 16],0\n",
      "JZ 5484\n",
      "LEA RBP,[8241]\n",
      "NOP dword ptr [RAX + RAX*1]\n",
      "MOV EDX,dword ptr [RBX]\n",
      "XOR EAX,EAX\n",
      "MOV RSI,RBP\n",
      "MOV EDI,2\n",
      "CALL 4288\n",
      "MOV RBX,qword ptr [RBX + 16]\n",
      "CMP qword ptr [RBX + 16],0\n",
      "JNZ 5456\n",
      "MOV EDX,dword ptr [RBX]\n",
      "ADD RSP,8\n",
      "LEA RSI,[8247]\n",
      "XOR EAX,EAX\n",
      "POP RBX\n",
      "MOV EDI,2\n",
      "POP RBP\n",
      "JMP 4288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.normalizer.normalize_str(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "144134ac-02c0-49ae-9ea0-d5ae5b40ef1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁ENDBR64\\nPUSH', (0, 12)),\n",
       " ('▁RBP\\nPUSH', (12, 21)),\n",
       " ('▁RBX\\nMOV', (21, 29)),\n",
       " ('▁RBX,RDI\\nSUB', (29, 41)),\n",
       " ('▁RSP,8\\nCMP', (41, 51)),\n",
       " ('▁qword', (51, 57)),\n",
       " ('▁ptr', (57, 61)),\n",
       " ('▁[RDI', (61, 66)),\n",
       " ('▁+', (66, 68)),\n",
       " ('▁16],0\\nJZ', (68, 77)),\n",
       " ('▁5484\\nLEA', (77, 86)),\n",
       " ('▁RBP,[8241]\\nNOP', (86, 101)),\n",
       " ('▁dword', (101, 107)),\n",
       " ('▁ptr', (107, 111)),\n",
       " ('▁[RAX', (111, 116)),\n",
       " ('▁+', (116, 118)),\n",
       " ('▁RAX*1]\\nMOV', (118, 129)),\n",
       " ('▁EDX,dword', (129, 139)),\n",
       " ('▁ptr', (139, 143)),\n",
       " ('▁[RBX]\\nXOR', (143, 153)),\n",
       " ('▁EAX,EAX\\nMOV', (153, 165)),\n",
       " ('▁RSI,RBP\\nMOV', (165, 177)),\n",
       " ('▁EDI,2\\nCALL', (177, 188)),\n",
       " ('▁4288\\nMOV', (188, 197)),\n",
       " ('▁RBX,qword', (197, 207)),\n",
       " ('▁ptr', (207, 211)),\n",
       " ('▁[RBX', (211, 216)),\n",
       " ('▁+', (216, 218)),\n",
       " ('▁16]\\nCMP', (218, 226)),\n",
       " ('▁qword', (226, 232)),\n",
       " ('▁ptr', (232, 236)),\n",
       " ('▁[RBX', (236, 241)),\n",
       " ('▁+', (241, 243)),\n",
       " ('▁16],0\\nJNZ', (243, 253)),\n",
       " ('▁5456\\nMOV', (253, 262)),\n",
       " ('▁EDX,dword', (262, 272)),\n",
       " ('▁ptr', (272, 276)),\n",
       " ('▁[RBX]\\nADD', (276, 286)),\n",
       " ('▁RSP,8\\nLEA', (286, 296)),\n",
       " ('▁RSI,[8247]\\nXOR', (296, 311)),\n",
       " ('▁EAX,EAX\\nPOP', (311, 323)),\n",
       " ('▁RBX\\nMOV', (323, 331)),\n",
       " ('▁EDI,2\\nPOP', (331, 341)),\n",
       " ('▁RBP\\nJMP', (341, 349)),\n",
       " ('▁4288\\n', (349, 355))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "472f62d4-c2ad-4ad6-859c-a99348df4470",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9da034a8-b62c-4272-a8f0-1189b7e3a00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88a13051-7ac1-4541-b069-cf0855d7a021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁ENDBR64\\n', 'PUSH', '▁RBP\\n', 'PUSH', '▁RBX\\nMOV', '▁RBX,RDI\\n', 'SUB', '▁RSP,8\\n', 'CMP', '▁', 'q', 'w', 'o', 'r', 'd', '▁', 'p', 't', 'r', '▁[RDI', '▁', '+', '▁16],0\\nJ', 'Z', '▁5484\\n', 'LEA', '▁RBP,[8241]\\n', 'NOP', '▁', 'd', 'w', 'o', 'r', 'd', '▁', 'p', 't', 'r', '▁[RAX', '▁', '+', '▁RAX*1]\\nMOV', '▁EDX,', 'd', 'w', 'o', 'r', 'd', '▁', 'p', 't', 'r', '▁[RBX]\\n', 'XOR', '▁EAX,EAX\\nMOV', '▁RSI,RBP\\nMOV', '▁EDI,2\\nC', 'A', 'LL', '▁4288\\nMOV', '▁RBX,', 'q', 'w', 'o', 'r', 'd', '▁', 'p', 't', 'r', '▁[RBX', '▁', '+', '▁16]\\n', 'CMP', '▁', 'q', 'w', 'o', 'r', 'd', '▁', 'p', 't', 'r', '▁[RBX', '▁', '+', '▁16],0\\nJ', 'N', 'Z', '▁5456\\nMOV', '▁EDX,', 'd', 'w', 'o', 'r', 'd', '▁', 'p', 't', 'r', '▁[RBX]\\nADD', '▁RSP,8\\n', 'LEA', '▁RSI,[8247]\\n', 'XOR', '▁EAX,EAX', '\\nPOP', '▁RBX\\nMOV', '▁EDI,2\\n', 'POP', '▁', 'RBP\\nJ', 'MP', '▁4288\\n']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(example)\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "808a999b-58e2-4c51-b082-96a7a1925058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=116, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cfaa1ef-67ea-4835-ab95-c0eea6c307b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.special_tokens_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb3f087f-3f61-4f7b-a335-ffe92be7b0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32,\n",
       " 20,\n",
       " 40,\n",
       " 20,\n",
       " 52,\n",
       " 141,\n",
       " 42,\n",
       " 66,\n",
       " 23,\n",
       " 6,\n",
       " 16,\n",
       " 12,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 54,\n",
       " 6,\n",
       " 13,\n",
       " 1723,\n",
       " 19,\n",
       " 9543,\n",
       " 17,\n",
       " 6962,\n",
       " 96,\n",
       " 6,\n",
       " 8,\n",
       " 12,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 44,\n",
       " 6,\n",
       " 13,\n",
       " 79,\n",
       " 46,\n",
       " 8,\n",
       " 12,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 277,\n",
       " 18,\n",
       " 49,\n",
       " 93,\n",
       " 76,\n",
       " 14,\n",
       " 15,\n",
       " 147,\n",
       " 87,\n",
       " 16,\n",
       " 12,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 55,\n",
       " 6,\n",
       " 13,\n",
       " 144,\n",
       " 23,\n",
       " 6,\n",
       " 16,\n",
       " 12,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 55,\n",
       " 6,\n",
       " 13,\n",
       " 1723,\n",
       " 43,\n",
       " 19,\n",
       " 1632,\n",
       " 46,\n",
       " 8,\n",
       " 12,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 581,\n",
       " 66,\n",
       " 17,\n",
       " 6046,\n",
       " 18,\n",
       " 64,\n",
       " 27,\n",
       " 52,\n",
       " 41,\n",
       " 30,\n",
       " 6,\n",
       " 201,\n",
       " 28,\n",
       " 137]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "821e59ee-86bc-40b2-b2e0-5857ba597bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "307aa99f-7d78-4c3a-a1ef-a513370d8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54a48f0c-b963-4208-bdac-1543bedaae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b2328c2-d35a-40f4-808d-234e888c31d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 32, 20, 40, 20, 52, 141, 42, 66, 23, 6, 16, 12, 11, 7, 8, 6, 10, 9, 7, 54, 6, 13, 1723, 19, 9543, 17, 6962, 96, 6, 8, 12, 11, 7, 8, 6, 10, 9, 7, 44, 6, 13, 79, 46, 8, 12, 11, 7, 8, 6, 10, 9, 7, 277, 18, 49, 93, 76, 14, 15, 147, 87, 16, 12, 11, 7, 8, 6, 10, 9, 7, 55, 6, 13, 144, 23, 6, 16, 12, 11, 7, 8, 6, 10, 9, 7, 55, 6, 13, 1723, 43, 19, 1632, 46, 8, 12, 11, 7, 8, 6, 10, 9, 7, 581, 66, 17, 6046, 18, 64, 27, 52, 41, 30, 6, 201, 28, 137, 4], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4d5a481-a7b4-466d-9517-6870f41a7ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ENDBR64\\nPUSH RBP\\nPUSH RBX\\nMOV RBX,RDI\\nSUB RSP,8\\nCMP qword ptr [RDI + 16],0\\nJZ 5484\\nLEA RBP,[8241]\\nNOP dword ptr [RAX + RAX*1]\\nMOV EDX,dword ptr [RBX]\\nXOR EAX,EAX\\nMOV RSI,RBP\\nMOV EDI,2\\nCALL 4288\\nMOV RBX,qword ptr [RBX + 16]\\nCMP qword ptr [RBX + 16],0\\nJNZ 5456\\nMOV EDX,dword ptr [RBX]\\nADD RSP,8\\nLEA RSI,[8247]\\nXOR EAX,EAX\\nPOP RBX\\nMOV EDI,2\\nPOP RBP\\nJMP 4288\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e731187-a02c-4c92-a00f-5b7e7a4ed0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./../models/cusTokenizer_UNI_25k_ASIS/tokenizer_config.json',\n",
       " './../models/cusTokenizer_UNI_25k_ASIS/special_tokens_map.json',\n",
       " './../models/cusTokenizer_UNI_25k_ASIS/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.save_pretrained(\"./../models/cusTokenizer_UNI_25k_ASIS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1a5eedd-7b7a-44c6-aa8a-044acab0adbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
