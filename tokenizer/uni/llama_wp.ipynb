{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "#!/usr/bin/env python3\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import sys,os\n",
    "from elftools.elf.elffile import ELFFile\n",
    "from elftools.elf.segments import Segment\n",
    "from capstone import *\n",
    "from capstone.x86 import *\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments,LlamaModel,LlamaForSequenceClassification,LlamaTokenizerFast\n",
    "import os\n",
    "import json \n",
    "import torch, os,re\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from num2words import num2words\n",
    "\n",
    "from trl import SFTTrainer\n",
    "# import torch\n",
    "\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "# from torch.distributed.fsdp.wrap import auto_wrap\n",
    "\n",
    "\n",
    "login(token = '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')\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import sys,os\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MAX_TOKEN_LEN = 1024\n",
    "BATCH_SIZE =5\n",
    "EXPERIMENT_NAME = 'cusTokenizer_WP_35k_ATW'\n",
    "\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction,BertForPreTraining\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"./../../models/\" + EXPERIMENT_NAME)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Load the model\n",
    "model = LlamaForCausalLM.from_pretrained('meta-llama/Llama-3.2-1B')\n",
    " \n",
    "\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "Functions Count:  40001 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_key = \"disassembly_addresses_to_words\"\n",
    "DATA_PATH = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions'\n",
    "\n",
    "TRAIN_DATA_PATH  ='/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/train/'\n",
    "\n",
    "TEST_DATA_PATH   = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/test/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_json_files = [os.path.join(TRAIN_DATA_PATH, f) for f in os.listdir(TRAIN_DATA_PATH) ]\n",
    "\n",
    "test_json_files = [os.path.join(TEST_DATA_PATH, f) for f in os.listdir(TEST_DATA_PATH) ]\n",
    "\n",
    "\n",
    "print(len(train_json_files))\n",
    "def read_corpus(json_files):\n",
    "\n",
    "    all = []\n",
    "\n",
    "    for k, j_file in enumerate(json_files):\n",
    "        if k>40000:\n",
    "            break\n",
    "        try:\n",
    "\n",
    "            with open(j_file, 'r') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                funct = data[data_key]['input']\n",
    "                \n",
    "                all.append(funct)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "    return all\n",
    "    \n",
    "\n",
    "\n",
    "train_text = read_corpus(train_json_files)\n",
    "test_text  = read_corpus(test_json_files)\n",
    "\n",
    "\n",
    "        \n",
    "# text = text[0:5000]\n",
    "print(\"Functions Count: \",len(train_text), '\\n')\n",
    "example = train_text[10]\n",
    "text = train_text + test_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDBR64\n",
      "PUSH R15\n",
      "LEA RDI,[8270]\n",
      "PUSH R14\n",
      "PUSH R13\n",
      "PUSH R12\n",
      "PUSH RBP\n",
      "PUSH RBX\n",
      "SUB RSP,8\n",
      "CALL addr0\n",
      "MOVSXD RCX,dword ptr [addr8]\n",
      "CMP ECX,1\n",
      "JLE addr6\n",
      "XOR R13D,R13D\n",
      "LEA RBX,[16576]\n",
      "LEA R14,[16480]\n",
      "LEA RBP,[8297]\n",
      "LEA R12,[16640]\n",
      "NOP dword ptr [RAX]\n",
      "XOR EAX,EAX\n",
      "MOV R15D,4294967295\n",
      "MOV EDX,987654321\n",
      "NOP dword ptr [RAX]\n",
      "MOV ESI,dword ptr [RBX + RAX*4]\n",
      "TEST ESI,ESI\n",
      "JNZ addr5\n",
      "MOV ESI,dword ptr [R12 + RAX*4]\n",
      "CMP EDX,ESI\n",
      "CMOVG R15D,EAX\n",
      "CMOVG EDX,ESI\n",
      "ADD RAX,1\n",
      "CMP RCX,RAX\n",
      "JNZ addr4\n",
      "MOVSXD RAX,R15D\n",
      "ADD dword ptr [addr7],EDX\n",
      "MOV RSI,RBP\n",
      "MOV EDX,R15D\n",
      "MOV ECX,dword ptr [R14 + RAX*8]\n",
      "MOV R8D,dword ptr [R14 + RAX*8 + 4]\n",
      "MOV dword ptr [RBX + RAX*4],1\n",
      "XOR EAX,EAX\n",
      "MOV EDI,2\n",
      "ADD R13D,1\n",
      "CALL addr1\n",
      "MOV EDI,R15D\n",
      "CALL addr2\n",
      "MOVSXD RCX,dword ptr [addr8]\n",
      "LEA EAX,[RCX + -1]\n",
      "CMP R13D,EAX\n",
      "JL addr3\n",
      "MOV EDX,dword ptr [addr7]\n",
      "ADD RSP,8\n",
      "LEA RSI,[8321]\n",
      "XOR EAX,EAX\n",
      "POP RBX\n",
      "MOV EDI,2\n",
      "POP RBP\n",
      "POP R12\n",
      "POP R13\n",
      "POP R14\n",
      "POP R15\n",
      "JMP addr1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 27880 2 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "mask_token_id= tokenizer.mask_token_id\n",
    "\n",
    "print(sep_token_id,pad_token_id,cls_token_id ,mask_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we create our 50/50 NIP training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready for tokenization, this time we truncate/pad each token to the same length of *1024* tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = tokenizer(text,return_tensors='pt',max_length=MAX_TOKEN_LEN, truncation=True, padding=True)\n",
    "ground_truth = inputs.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the *token_type_ids* tensors have been built correctly (eg **1** indicating sentence B tokens) by checking the first instance of *token_type_ids*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
      "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27879,   168,\n",
      "          128,   121,   111,   120,   112,  3261,    22,    88,   124,     7,\n",
      "           12,   111,   121,   112,   589,    22,   128,   118,   111,   118,\n",
      "          112,   638,    22,   128,   108,   115,   144,     7,   144,   107,\n",
      "          129,   154,   153,    88,   116,     7,   118,    91,   144,     7,\n",
      "           11,   107,   141,   134,   144,     7,   701,   155,   160,   134,\n",
      "          144,     7,   291,   140,   146,    88,   116,     7,   121,    88,\n",
      "          144,     7,   599,   107,   141,    88,   116,     7,   118,   107,\n",
      "          141,   154,   146,   123,   108,   115,    94,     7,    94,   123,\n",
      "          118,   123,   121,   162])\n",
      "</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> <s> endbr64 push r12 lea rsi ,[ 4608 ] mov edi , 2 lea r12 ,[ 8253 ] push rbp lea rbp ,[ 8262 ] push rbx xor ebx , ebx call addr1 jmp addr3 mov rdi , rbp add ebx , 1 call addr0 cmp ebx , 10000 jz addr4 cmp ebx , 100 jnz addr2 mov rdi , r12 mov ebx , 101 call addr0 mov rdi , rbp call addr0 jmp addr2 pop rbx xor eax , eax pop rbp pop r12 ret\n",
      "\n",
      "-->>>>\n",
      " ENDBR64\n",
      "PUSH R12\n",
      "LEA RSI,[4608]\n",
      "MOV EDI,2\n",
      "LEA R12,[8253]\n",
      "PUSH RBP\n",
      "LEA RBP,[8262]\n",
      "PUSH RBX\n",
      "XOR EBX,EBX\n",
      "CALL addr1\n",
      "JMP addr3\n",
      "MOV RDI,RBP\n",
      "ADD EBX,1\n",
      "CALL addr0\n",
      "CMP EBX,10000\n",
      "JZ addr4\n",
      "CMP EBX,100\n",
      "JNZ addr2\n",
      "MOV RDI,R12\n",
      "MOV EBX,101\n",
      "CALL addr0\n",
      "MOV RDI,RBP\n",
      "CALL addr0\n",
      "JMP addr2\n",
      "POP RBX\n",
      "XOR EAX,EAX\n",
      "POP RBP\n",
      "POP R12\n",
      "RET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inputs.input_ids[0])\n",
    "print(tokenizer.decode(inputs.input_ids[0]))\n",
    "print('\\n-->>>>\\n',text[0])\n",
    "# inputs.token_type_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **0** tokens following our sentence B tokens correspond to *PAD* tokens.\n",
    "\n",
    "Alongside this, we need to create a *labels* tensor too - which corresponds to the values contained within our `label` variable. Our *labels* tensor must be a *LongTensor*, and we will need to transpose the tensor so that it matches our other tensors' dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs['labels'] = inputs.input_ids.copy()\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27879,   168,\n",
       "          128,   174,   111,   116,   112,   706,    22,   128,   152,   128,\n",
       "          135,   128,   121,   128,   118,   128,   108,   138,   104,     7,\n",
       "           18,   107,   141,   185,   161,     7,   119,    97,    21,   191,\n",
       "           22,   134,   145,     7,    11,   190,   175,   115,   200,     7,\n",
       "          200,   111,   108,   112,   815,    22,   111,   152,   112,   435,\n",
       "           22,   111,   118,   112,   822,    22,   111,   121,   112,  1116,\n",
       "           22,   176,   119,    97,    21,   102,    22,   115,    94,     7,\n",
       "           94,    88,   230,     7,   306,    88,   126,     7,  5412,   176,\n",
       "          119,    97,    21,   102,    22,    88,   143,     7,   119,    97,\n",
       "           21,   108,     6,   102,     5,    14,    22,   151,   143,     7,\n",
       "          143,   140,   170,    88,   143,     7,   119,    97,    21,   121,\n",
       "            6,   102,     5,    14,    22,   134,   126,     7,   143,   390,\n",
       "          230,     7,    94,   390,   126,     7,   143,    91,   102,     7,\n",
       "           11,   134,   161,     7,   102,   140,   160,   185,   102,     7,\n",
       "          230,    91,   119,    97,    21,   182,   122,   126,    88,   120,\n",
       "            7,   118,    88,   126,     7,   230,    88,   145,     7,   119,\n",
       "           97,    21,   152,     6,   102,     5,    18,    22,    88,   196,\n",
       "            7,   119,    97,    21,   152,     6,   102,     5,    18,     6,\n",
       "           14,    22,    88,   119,    97,    21,   108,     6,   102,     5,\n",
       "           14,   122,    11,   115,    94,     7,    94,    88,   124,     7,\n",
       "           12,    91,   200,     7,    11,   107,   129,    88,   124,     7,\n",
       "          230,   107,   146,   185,   161,     7,   119,    97,    21,   191,\n",
       "           22,   111,    94,   112,   161,     6,     8,    11,    22,   134,\n",
       "          200,     7,    94,   184,   153,    88,   126,     7,   119,    97,\n",
       "           21,   182,    22,    91,   104,     7,    18,   111,   120,   112,\n",
       "          931,    22,   115,    94,     7,    94,   123,   108,    88,   124,\n",
       "            7,    12,   123,   118,   123,   121,   123,   135,   123,   152,\n",
       "          123,   174,   154,   129])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[-1]\n",
    "# tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we mask tokens in the input_ids tensor using the 15% probability for MLM - ensuring we don't mask CLS, SEP, or PAD tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "# create mask array\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now take the indices of each True value within each vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = []\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60001,\n",
       " 60001,\n",
       " [[771, 784, 788, 796, 802, 819, 837, 853],\n",
       "  [411,\n",
       "   416,\n",
       "   426,\n",
       "   428,\n",
       "   429,\n",
       "   440,\n",
       "   445,\n",
       "   449,\n",
       "   457,\n",
       "   468,\n",
       "   469,\n",
       "   481,\n",
       "   489,\n",
       "   495,\n",
       "   504,\n",
       "   506,\n",
       "   510,\n",
       "   522,\n",
       "   530,\n",
       "   538,\n",
       "   540,\n",
       "   541,\n",
       "   549,\n",
       "   563,\n",
       "   568,\n",
       "   602,\n",
       "   607,\n",
       "   617,\n",
       "   646,\n",
       "   652,\n",
       "   656,\n",
       "   661,\n",
       "   681,\n",
       "   695,\n",
       "   698,\n",
       "   701,\n",
       "   716,\n",
       "   719,\n",
       "   722,\n",
       "   723,\n",
       "   726,\n",
       "   733,\n",
       "   734,\n",
       "   753,\n",
       "   765,\n",
       "   769,\n",
       "   774,\n",
       "   778,\n",
       "   779,\n",
       "   784,\n",
       "   789,\n",
       "   790,\n",
       "   800,\n",
       "   805,\n",
       "   809,\n",
       "   829,\n",
       "   846,\n",
       "   850,\n",
       "   852,\n",
       "   853],\n",
       "  [638,\n",
       "   644,\n",
       "   645,\n",
       "   647,\n",
       "   648,\n",
       "   655,\n",
       "   671,\n",
       "   675,\n",
       "   676,\n",
       "   680,\n",
       "   681,\n",
       "   688,\n",
       "   695,\n",
       "   717,\n",
       "   721,\n",
       "   725,\n",
       "   728,\n",
       "   729,\n",
       "   744,\n",
       "   746,\n",
       "   752,\n",
       "   760,\n",
       "   769,\n",
       "   783,\n",
       "   791,\n",
       "   792,\n",
       "   793,\n",
       "   805,\n",
       "   816,\n",
       "   817,\n",
       "   819,\n",
       "   844,\n",
       "   845,\n",
       "   852]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (selection) , len(inputs.input_ids), selection[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then apply these indices to each row in input_ids, assigning each value at these indices a value of 103."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([121, 589, 111, 144, 153, 134,   7, 162])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[0][selection[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_labels = []\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    masked_labels.append(inputs.input_ids[i, selection[i]])\n",
    "    inputs.input_ids[i, selection[i]] = mask_token_id\n",
    "\n",
    "inputs[\"mask_arr\"] = mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,\n",
       "        27880, 27880, 27880, 27880, 27880, 27880, 27880, 27880,     4,   168,\n",
       "          128,     4,   111,   116,   112,   706,    22,   128,   152,   128,\n",
       "            4,   128,   121,   128,   118,     4,   108,   138,     4,     7,\n",
       "           18,   107,   141,     4,   161,     7,   119,    97,     4,   191,\n",
       "           22,   134,   145,     7,     4,   190,   175,   115,   200,     7,\n",
       "          200,   111,   108,     4,   815,    22,   111,   152,   112,   435,\n",
       "            4,   111,   118,   112,     4,     4,   111,   121,   112,  1116,\n",
       "            4,   176,   119,    97,     4,   102,    22,   115,    94,     7,\n",
       "            4,    88,   230,     7,   306,    88,   126,     7,     4,     4,\n",
       "          119,    97,    21,   102,     4,    88,   143,     7,     4,    97,\n",
       "           21,   108,     6,   102,     5,     4,    22,   151,   143,     7,\n",
       "            4,     4,     4,    88,   143,     4,   119,    97,    21,   121,\n",
       "            6,   102,     4,    14,    22,     4,   126,     7,   143,   390,\n",
       "          230,     7,    94,   390,     4,     7,   143,    91,   102,     7,\n",
       "           11,   134,   161,     7,   102,   140,     4,     4,     4,     7,\n",
       "          230,    91,   119,    97,     4,   182,   122,   126,     4,   120,\n",
       "            7,   118,     4,   126,     7,   230,    88,   145,     7,   119,\n",
       "           97,    21,   152,     6,   102,     5,    18,    22,    88,   196,\n",
       "            7,     4,    97,    21,   152,     6,   102,     5,    18,     6,\n",
       "           14,    22,    88,   119,    97,    21,   108,     6,   102,     4,\n",
       "           14,   122,    11,     4,    94,     7,    94,    88,   124,     7,\n",
       "           12,    91,     4,     7,    11,   107,     4,    88,   124,     7,\n",
       "          230,     4,   146,   185,   161,     7,   119,    97,     4,     4,\n",
       "           22,   111,    94,   112,   161,     4,     8,    11,    22,   134,\n",
       "          200,     7,    94,   184,   153,    88,   126,     4,   119,    97,\n",
       "            4,     4,    22,    91,   104,     7,    18,     4,     4,   112,\n",
       "          931,    22,     4,    94,     7,    94,     4,   108,    88,   124,\n",
       "            7,    12,   123,   118,   123,   121,   123,   135,   123,   152,\n",
       "            4,   174,   154,   129])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inputs` tensors are now ready, and we can begin building the model input pipeline for training. We first create a PyTorch dataset from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeditationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our data using the `MeditationDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MeditationsDataset(inputs)\n",
    "# print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60001\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 40001) range(40001, 60001)\n"
     ]
    }
   ],
   "source": [
    "print( range(len(train_text)), range(len(train_text) , len(dataset)  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40001, 20000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dataset  = torch.utils.data.Subset(dataset, range(len(train_text)))\n",
    "validation_dataset = torch.utils.data.Subset(dataset, range(len(train_text) , len(dataset)))\n",
    "\n",
    "len(train_dataset) , len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And initialize the dataloader, which we'll be using to load our data into the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader      = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto setting up the training loop. First we setup GPU/CPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the training mode of our model, and initialize our optimizer (Adam with weighted decay - reduces chance of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto the training loop, we'll train for a couple of epochs (change `epochs` to modify this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odict_keys(['loss', 'prediction_logits', 'seq_relationship_logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import *\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graph(training_data, validation_data , label ):\n",
    "\n",
    "    font_size = 10\n",
    "    x_labels = [ i for i in range(len(training_data)) ]\n",
    "\n",
    "    plt.ylabel(' F1 ',fontsize=font_size)\n",
    "    plt.plot(x_labels, training_data , 'r') \n",
    "    plt.plot(x_labels, validation_data , 'b') \n",
    "    plt.xlabel(\"Epoch\", fontsize=font_size)\n",
    "    plt.title(label,fontsize=font_size)\n",
    "    plt.legend(['Training', 'Validation'], loc='upper left') \n",
    "    \n",
    "    plt.savefig('./../../results/'+EXPERIMENT_NAME+label+'.pdf')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                          | 0/8001 [00:00<?, ?it/s]/tmp/ipykernel_565051/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|| 8001/8001 [3:35:33<00:00,  1.62s/it, loss=0.0452]\n",
      "100%|| 4000/4000 [38:11<00:00,  1.75it/s]\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: acc  0.6976553880238061    v_masked_token_ F1:  0.6870909681121723  V SEQ F1:  0.9547731433180079 v_seq_accuracy 0.95472028960292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 0/8001 [00:00<?, ?it/s]/tmp/ipykernel_565051/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 1: 100%|| 8001/8001 [3:38:14<00:00,  1.64s/it, loss=0.0216]\n",
      "100%|| 4000/4000 [39:08<00:00,  1.70it/s]\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: acc  0.7209369633021727    v_masked_token_ F1:  0.7120064268190476  V SEQ F1:  0.958167173579503 v_seq_accuracy 0.9582046132260007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 0/8001 [00:00<?, ?it/s]/tmp/ipykernel_565051/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 2: 100%|| 8001/8001 [3:39:56<00:00,  1.65s/it, loss=0.00366]\n",
      "100%|| 4000/4000 [38:14<00:00,  1.74it/s]\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: acc  0.7208679133849591    v_masked_token_ F1:  0.7156897911653717  V SEQ F1:  0.9581763117368074 v_seq_accuracy 0.9581777785891712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 0/8001 [00:00<?, ?it/s]/tmp/ipykernel_565051/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 3: 100%|| 8001/8001 [3:39:35<00:00,  1.65s/it, loss=0.00109]\n",
      "100%|| 4000/4000 [39:07<00:00,  1.70it/s]\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: acc  0.716766054473039    v_masked_token_ F1:  0.7131848113184338  V SEQ F1:  0.9575428328341831 v_seq_accuracy 0.9575632214145681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 0/8001 [00:00<?, ?it/s]/tmp/ipykernel_565051/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 4: 100%|| 8001/8001 [3:38:57<00:00,  1.64s/it, loss=0.000348]\n",
      "100%|| 4000/4000 [38:14<00:00,  1.74it/s]\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: acc  0.7183439185387275    v_masked_token_ F1:  0.715585033897676  V SEQ F1:  0.9577807906435278 v_seq_accuracy 0.9578082524426668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 0/8001 [00:00<?, ?it/s]/tmp/ipykernel_565051/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 5: 100%|| 8001/8001 [3:36:46<00:00,  1.63s/it, loss=0.00052]\n",
      "100%|| 4000/4000 [39:03<00:00,  1.71it/s]\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: acc  0.7197131637056006    v_masked_token_ F1:  0.7169555050089442  V SEQ F1:  0.9579989016767015 v_seq_accuracy 0.9580134714276005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 0/8001 [00:00<?, ?it/s]/tmp/ipykernel_565051/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 6: 100%|| 8001/8001 [3:38:19<00:00,  1.64s/it, loss=0.0015]\n",
      "100%|| 4000/4000 [38:16<00:00,  1.74it/s]\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: acc  0.7217288336293665    v_masked_token_ F1:  0.7184181018956346  V SEQ F1:  0.9582615708613443 v_seq_accuracy 0.9583134914655956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 0/8001 [00:00<?, ?it/s]/tmp/ipykernel_565051/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 7: 100%|| 8001/8001 [3:37:09<00:00,  1.63s/it, loss=0.00113]\n",
      "100%|| 4000/4000 [38:16<00:00,  1.74it/s]\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: acc  0.721007482366561    v_masked_token_ F1:  0.7178360084454513  V SEQ F1:  0.9581384299034635 v_seq_accuracy 0.9581940553361006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 0/8001 [00:00<?, ?it/s]/tmp/ipykernel_565051/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 8: 100%|| 8001/8001 [3:38:39<00:00,  1.64s/it, loss=0.000491]\n",
      "100%|| 4000/4000 [38:23<00:00,  1.74it/s]\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: acc  0.7243380390117341    v_masked_token_ F1:  0.7210037607941403  V SEQ F1:  0.9586364948895147 v_seq_accuracy 0.9586843373483376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 0/8001 [00:00<?, ?it/s]/tmp/ipykernel_565051/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 9: 100%|| 8001/8001 [3:40:13<00:00,  1.65s/it, loss=0.000228]\n",
      "100%|| 4000/4000 [38:18<00:00,  1.74it/s]\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: acc  0.7263522397883253    v_masked_token_ F1:  0.7229189437460388  V SEQ F1:  0.95891767291052 v_seq_accuracy 0.9589861170346494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "from itertools import chain\n",
    "\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "epochs =10\n",
    "counter = 0\n",
    "\n",
    "global_instruction_metrices = []\n",
    "global_masked_token_metrices = []\n",
    "\n",
    "v_global_instruction_metrices = []\n",
    "v_global_masked_token_metrices = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    train_loop = tqdm(train_loader, leave=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    masked_token_predictions_all, masked_token_ground_truths_all = None, None\n",
    "    seq_predictions_all, seq_ground_truths_all = None, None\n",
    "    \n",
    "    # activate training mode\n",
    "    model.train()\n",
    "    for N,batch in enumerate(train_loop):\n",
    "\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        # token_type_ids = batch['token_type_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        batch_mask_arr = batch ['mask_arr']\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids ,attention_mask = attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        token_prediction = torch.argmax(logits, axis=-1)\n",
    "\n",
    "        \n",
    "        batch_masks =   [ torch.flatten(bm.nonzero()).tolist()  for bm in batch_mask_arr]    # torch.flatten(batch ['mask_arr'].nonzero()).tolist()\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        masked_token_prediction = [ token[batch_masks[t]].tolist() for t,token in enumerate(token_prediction) ]\n",
    "        masked_token_prediction = list(chain.from_iterable(masked_token_prediction))\n",
    "        \n",
    "        masked_token_ground_truth   = [ token[batch_masks[t]].tolist() for t,token in enumerate(labels) ]\n",
    "        masked_token_ground_truth = list(chain.from_iterable(masked_token_ground_truth))\n",
    "        \n",
    "\n",
    "\n",
    "        seq_predictions   = token_prediction.detach().cpu().numpy().flatten()\n",
    "        seq_ground_truths = labels.detach().cpu().numpy().flatten()\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        if N==0:\n",
    "\n",
    "            \n",
    "            masked_token_predictions_all         = masked_token_prediction\n",
    "            masked_token_ground_truths_all       = masked_token_ground_truth  \n",
    "\n",
    "\n",
    "            seq_predictions_all = seq_predictions\n",
    "            seq_ground_truths_all = seq_ground_truths\n",
    "            \n",
    "        else:\n",
    "\n",
    "            masked_token_predictions_all   = np.concatenate((masked_token_predictions_all, masked_token_prediction))\n",
    "            masked_token_ground_truths_all = np.concatenate((masked_token_ground_truths_all, masked_token_ground_truth))\n",
    "\n",
    "            seq_predictions_all = np.concatenate((seq_predictions_all, seq_predictions))\n",
    "            seq_ground_truths_all = np.concatenate((seq_ground_truths_all, seq_ground_truths))\n",
    "\n",
    "\n",
    "                # Compute loss\n",
    "        logits = logits.view(-1, logits.size(-1))  # [batch_size * seq_length, vocab_size]\n",
    "        labels = labels.view(-1)  # [batch_size * seq_length]\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        # Now, you can update the model's weights using the optimizer\n",
    "        optim.step()\n",
    "        # Zero gradients after updating the model's weights\n",
    "        optim.zero_grad()\n",
    "\n",
    "        train_loop.set_description(f'Epoch {epoch}')\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    ###########################################\n",
    "    ###############  EVAL Validation  #########\n",
    "    ###########################################\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        v_masked_token_predictions_all, v_masked_token_ground_truths_all = None, None\n",
    "        v_seq_predictions_all, v_seq_ground_truths_all = None, None\n",
    "    \n",
    "    \n",
    "        validation_loop = tqdm(validation_loader, leave=True)\n",
    "        for N,v_batch in enumerate(validation_loop):\n",
    "            \n",
    "            \n",
    "            \n",
    "            v_input_ids = v_batch['input_ids'].to(device)\n",
    "            v_attention_mask = v_batch['attention_mask'].to(device)\n",
    "            v_mask_arr = v_batch ['mask_arr']\n",
    "            v_labels = v_batch['labels'].to(device)\n",
    "            # process\n",
    "            v_outputs = model(v_input_ids, attention_mask=v_attention_mask)\n",
    "\n",
    "\n",
    "            v_logits = v_outputs.logits\n",
    "\n",
    "        \n",
    "            v_token_prediction = torch.argmax(v_logits, axis=-1)\n",
    "\n",
    "                    \n",
    "\n",
    "            v_batch_masks =   [ torch.flatten(bm.nonzero()).tolist()  for bm in v_mask_arr]\n",
    "            \n",
    "            v_masked_token_prediction = [ token[v_batch_masks[t]].tolist() for t,token in enumerate(v_token_prediction) ]\n",
    "            v_masked_token_prediction = list(chain.from_iterable(v_masked_token_prediction))\n",
    "            \n",
    "            v_masked_token_ground_truth   = [ token[v_batch_masks[t]].tolist() for t,token in enumerate(v_labels) ]\n",
    "            v_masked_token_ground_truth = list(chain.from_iterable(v_masked_token_ground_truth))\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            #discard the padding tokens and just keep the non padding tokens for evaluation\n",
    "            v_seq_prediction = v_token_prediction.detach().cpu().numpy().flatten()\n",
    "            v_seq_ground_truth = v_labels.detach().cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "            filteredTokenPredictionWithoutPadding =[] \n",
    "            filteredTokenGTWithoutPadding = []\n",
    "            \n",
    "            for gi,g in enumerate(v_seq_ground_truth):\n",
    "                if g!=pad_token_id:\n",
    "                    filteredTokenGTWithoutPadding.append(v_seq_ground_truth [gi])\n",
    "                    filteredTokenPredictionWithoutPadding.append(v_seq_prediction[gi])\n",
    "\n",
    "            v_seq_ground_truth = filteredTokenGTWithoutPadding\n",
    "            v_seq_prediction = filteredTokenPredictionWithoutPadding\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            if N==0:\n",
    "\n",
    "                v_masked_token_predictions_all   = v_masked_token_prediction\n",
    "                v_masked_token_ground_truths_all = v_masked_token_ground_truth\n",
    "                \n",
    "                v_seq_predictions_all= v_seq_prediction\n",
    "                v_seq_ground_truths_all = v_seq_ground_truth\n",
    "\n",
    "        \n",
    "\n",
    "            else:\n",
    "\n",
    "                v_masked_token_predictions_all   = np.concatenate((v_masked_token_predictions_all, v_masked_token_prediction ))\n",
    "                v_masked_token_ground_truths_all = np.concatenate((v_masked_token_ground_truths_all, v_masked_token_ground_truth ))\n",
    "                \n",
    "                v_seq_predictions_all =np.concatenate((v_seq_predictions_all, v_seq_prediction ))\n",
    "                v_seq_ground_truths_all =np.concatenate((v_seq_ground_truths_all, v_seq_ground_truth ))\n",
    "                \n",
    "\n",
    "            \n",
    " \n",
    "\n",
    "        v_masked_token_accuracy = (accuracy_score(v_masked_token_ground_truths_all, v_masked_token_predictions_all))\n",
    "        v_masked_token_precision, v_masked_token_recall, v_masked_token_f1, _ = precision_recall_fscore_support(v_masked_token_ground_truths_all,v_masked_token_predictions_all,average='weighted')\n",
    "\n",
    "\n",
    "        v_seq_accuracy = (accuracy_score(v_seq_predictions_all, v_seq_ground_truths_all))\n",
    "        v_seq_precision, v_seq_recall, v_seq_f1, _ = precision_recall_fscore_support(v_seq_ground_truths_all,v_seq_predictions_all,average='weighted')\n",
    "\n",
    "        print(\"Validation: acc \",v_masked_token_accuracy,  \"   v_masked_token_ F1: \",v_masked_token_f1 ,\" V SEQ F1: \", v_seq_f1 , 'v_seq_accuracy' ,v_seq_accuracy)\n",
    "        \n",
    "\n",
    "        v_global_masked_token_metrices.append(v_masked_token_f1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./../../models/\"+EXPERIMENT_NAME+\"lama_model.ckpt\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ENDBR64\\nPUSH R15\\nLEA RDI,[8270]\\nPUSH R14\\nPUSH R13\\nPUSH R12\\nPUSH RBP\\nPUSH RBX\\nSUB RSP,8\\nCALL addr0\\nMOVSXD RCX,dword ptr [addr8]\\nCMP ECX,1\\nJLE addr6\\nXOR R13D,R13D\\nLEA RBX,[16576]\\nLEA R14,[16480]\\nLEA RBP,[8297]\\nLEA R12,[16640]\\nNOP dword ptr [RAX]\\nXOR EAX,EAX\\nMOV R15D,4294967295\\nMOV EDX,987654321\\nNOP dword ptr [RAX]\\nMOV ESI,dword ptr [RBX + RAX*4]\\nTEST ESI,ESI\\nJNZ addr5\\nMOV ESI,dword ptr [R12 + RAX*4]\\nCMP EDX,ESI\\nCMOVG R15D,EAX\\nCMOVG EDX,ESI\\nADD RAX,1\\nCMP RCX,RAX\\nJNZ addr4\\nMOVSXD RAX,R15D\\nADD dword ptr [addr7],EDX\\nMOV RSI,RBP\\nMOV EDX,R15D\\nMOV ECX,dword ptr [R14 + RAX*8]\\nMOV R8D,dword ptr [R14 + RAX*8 + 4]\\nMOV dword ptr [RBX + RAX*4],1\\nXOR EAX,EAX\\nMOV EDI,2\\nADD R13D,1\\nCALL addr1\\nMOV EDI,R15D\\nCALL addr2\\nMOVSXD RCX,dword ptr [addr8]\\nLEA EAX,[RCX + -1]\\nCMP R13D,EAX\\nJL addr3\\nMOV EDX,dword ptr [addr7]\\nADD RSP,8\\nLEA RSI,[8321]\\nXOR EAX,EAX\\nPOP RBX\\nMOV EDI,2\\nPOP RBP\\nPOP R12\\nPOP R13\\nPOP R14\\nPOP R15\\nJMP addr1\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[-1] #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'atte' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] , \u001b[43matte\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'atte' is not defined"
     ]
    }
   ],
   "source": [
    "inputs['labels'] [-1] , atte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = text[-1]\n",
    "input = tokenizer(input_text,return_tensors='pt',max_length=MAX_TOKEN_LEN, truncation=True, padding=True)\n",
    "\n",
    "print(input_text)\n",
    "print(tokenizer.decode(input.input_ids[0]))\n",
    "\n",
    "print(input.input_ids)\n",
    "print(input.keys())\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(input.input_ids.to(device), attention_mask=input.attention_mask.to(device))\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = text[-1]\n",
    "\n",
    "print(\"REAL TEXT:\\n\\n\\n\",input_text)\n",
    "print(\"Tokenized TEXT:\\n\\n\\n\",tokenizer.decode(inputs.input_ids[-1]))\n",
    "\n",
    "print(\"Model Input Token IDs:\\n\\n\\n\" ,inputs.input_ids[-1])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(inputs.input_ids[-1:].to(device), attention_mask=inputs.attention_mask[-1:].to(device))\n",
    "\n",
    "    prediction = torch.argmax(output.logits, axis=-1)\n",
    "    print(\"Predicted Token IDs:\\n\\n\\n\", prediction)\n",
    "    print(\"Predicted Code:\\n\\n\\n\",tokenizer.decode(prediction[0] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
