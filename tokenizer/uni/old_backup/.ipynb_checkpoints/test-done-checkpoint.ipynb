{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "#!/usr/bin/env python3\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import sys,os\n",
    "from elftools.elf.elffile import ELFFile\n",
    "from elftools.elf.segments import Segment\n",
    "from capstone import *\n",
    "from capstone.x86 import *\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments,LlamaModel,LlamaForSequenceClassification,LlamaTokenizerFast\n",
    "import os\n",
    "import json \n",
    "import torch, os,re\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from num2words import num2words\n",
    "\n",
    "from trl import SFTTrainer\n",
    "# import torch\n",
    "\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "# from torch.distributed.fsdp.wrap import auto_wrap\n",
    "\n",
    "\n",
    "login(token = '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')\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import sys,os\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MAX_TOKEN_LEN = 512\n",
    "BATCH_SIZE = 2\n",
    "EXPERIMENT_NAME = 'cusTokenizer_UNI_25k_ASIS'\n",
    "\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction,BertForPreTraining\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"./../../models/\" + EXPERIMENT_NAME)\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained('meta-llama/Llama-3.2-1B') #(\"./../../models/\" + EXPERIMENT_NAME)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# print(tokenizer.pad_token) \n",
    "# model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "# model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "\n",
    " \n",
    "# LlamaModel LlamaForSequenceClassification LlamaForCausalLM\n",
    "\n",
    "# Load the model\n",
    "model = LlamaForCausalLM.from_pretrained('meta-llama/Llama-3.2-1B')  # For regression task\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "Functions Count:  2001 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "DATA_PATH = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions'\n",
    "\n",
    "TRAIN_DATA_PATH  ='/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/train/'\n",
    "\n",
    "TEST_DATA_PATH   = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/test/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_json_files = [os.path.join(TRAIN_DATA_PATH, f) for f in os.listdir(TRAIN_DATA_PATH) ]\n",
    "\n",
    "test_json_files = [os.path.join(TEST_DATA_PATH, f) for f in os.listdir(TEST_DATA_PATH) ]\n",
    "\n",
    "# disassembly_decimal disassembly_all_number_to_words disassembly_decimal\n",
    "data_key = \"disassembly_decimal\"\n",
    "\n",
    "print(len(train_json_files))\n",
    "def read_corpus(json_files):\n",
    "\n",
    "    all = []\n",
    "\n",
    "    for k, j_file in enumerate(json_files):\n",
    "        if k>2000:\n",
    "            break\n",
    "        try:\n",
    "\n",
    "            with open(j_file, 'r') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                funct = data[data_key]['input']\n",
    "                \n",
    "                all.append(funct)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "    return all\n",
    "    \n",
    "\n",
    "\n",
    "train_text = read_corpus(train_json_files)\n",
    "test_text  = read_corpus(test_json_files)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Functions Count: \",len(train_text), '\\n')\n",
    "example = train_text[10]\n",
    "text = train_text + test_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDBR64\n",
      "SUB RSP,24\n",
      "LEA RSI,[8196]\n",
      "MOV EDI,2\n",
      "MOV RAX,qword ptr FS:[40]\n",
      "MOV qword ptr [RSP + 8],RAX\n",
      "XOR EAX,EAX\n",
      "CALL 4240\n",
      "XOR EAX,EAX\n",
      "LEA RDX,[RSP + 4]\n",
      "MOV RSI,RSP\n",
      "LEA RDI,[8223]\n",
      "CALL 4256\n",
      "TEST EAX,EAX\n",
      "JLE 4461\n",
      "MOV ECX,dword ptr [RSP + 4]\n",
      "MOV EDI,dword ptr [RSP]\n",
      "TEST ECX,ECX\n",
      "JZ 4452\n",
      "MOV EDX,ECX\n",
      "MOV EAX,EDI\n",
      "NOP word ptr [RAX + RAX*1]\n",
      "MOV ESI,EDX\n",
      "CDQ\n",
      "IDIV ESI\n",
      "MOV EAX,ESI\n",
      "TEST EDX,EDX\n",
      "JNZ 4376\n",
      "MOV EAX,EDI\n",
      "CDQ\n",
      "IDIV ESI\n",
      "MOV EDI,EAX\n",
      "MOV dword ptr [RSP],EAX\n",
      "MOV EAX,ECX\n",
      "CDQ\n",
      "IDIV ESI\n",
      "MOV EDX,EDI\n",
      "LEA RSI,[8232]\n",
      "MOV EDI,2\n",
      "MOV dword ptr [RSP + 4],EAX\n",
      "MOV ECX,EAX\n",
      "XOR EAX,EAX\n",
      "CALL 4240\n",
      "MOV RAX,qword ptr [RSP + 8]\n",
      "SUB RAX,qword ptr FS:[40]\n",
      "JNZ 4456\n",
      "XOR EAX,EAX\n",
      "ADD RSP,24\n",
      "RET\n",
      "MOV ESI,EDI\n",
      "JMP 4387\n",
      "CALL 4224\n",
      "MOV EDI,1\n",
      "CALL 4272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text[51].split(delim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assign a 50% probability of using the genuine next sentence, and 50% probability of using another random sentence.\n",
    "\n",
    "To make this simpler, we'll create a *'bag'* of individual sentences to pull from when selecting a random sentence B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212804 4002\n"
     ]
    }
   ],
   "source": [
    "delim = '\\n'\n",
    "bag = [instruction for instruction_cluster in text for instruction in instruction_cluster.split(delim)  if instruction!= '']\n",
    "bag_size = len(bag)\n",
    "print(bag_size , len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we create our 50/50 NIP training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4002\n",
      "['ENDBR64', 'PUSH R12', 'LEA RSI,[4608]', 'MOV EDI,2', 'LEA R12,[8253]', 'PUSH RBP', 'LEA RBP,[8262]', 'PUSH RBX', 'XOR EBX,EBX', 'CALL 4224', 'JMP 4323', 'MOV RDI,RBP', 'ADD EBX,1', 'CALL 4208', 'CMP EBX,10000', 'JZ 4352', 'CMP EBX,100', 'JNZ 4304', 'MOV RDI,R12', 'MOV EBX,101', 'CALL 4208', 'MOV RDI,RBP', 'CALL 4208', 'JMP 4304', 'POP RBX', 'XOR EAX,EAX', 'POP RBP', 'POP R12', 'RET']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "history = []\n",
    "next_instruction = []\n",
    "label = []\n",
    "\n",
    "\n",
    "instruction_pages = []\n",
    "for instruction_cluster in text:\n",
    "    instructions = [\n",
    "        instruction for instruction in instruction_cluster.split(delim) if instruction != ''\n",
    "    ]\n",
    "\n",
    "    instruction_pages.append(instructions)\n",
    "    # if len(instructions)>page_len:\n",
    "        \n",
    "    #     for i in range(0,len(instructions),page_len):\n",
    "    #         instruction_pages.append(instructions[i:i+page_len])\n",
    "        \n",
    "print(len(instruction_pages))\n",
    "print(instruction_pages[0])\n",
    "\n",
    "for instruction_page in instruction_pages:\n",
    "    \n",
    "#     instructions = [\n",
    "#         instruction for instruction in instruction_page.split(';') if instruction != ''\n",
    "#     ]\n",
    "    \n",
    "    \n",
    "#     num_instructions = len(instruction_page)\n",
    "    \n",
    "    \n",
    "\n",
    "#     start = random.randint(0, num_instructions-2)\n",
    "    # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "    if random.random() >= 0.5:\n",
    "        # this is IsNextSentence\n",
    "        history.append(delim.join(instruction_page[:-1]))\n",
    "        next_instruction.append(instruction_page[-1])\n",
    "        label.append(0)\n",
    "    else:\n",
    "        index = random.randint(0, bag_size-1)\n",
    "        # this is NotNextSentence\n",
    "        history.append(delim.join(instruction_page[:-1]))\n",
    "        next_instruction.append(bag[index])\n",
    "        label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4002\n",
      "0\n",
      "-> ENDBR64\n",
      "PUSH R12\n",
      "LEA RSI,[4608]\n",
      "MOV EDI,2\n",
      "LEA R12,[8253]\n",
      "PUSH RBP\n",
      "LEA RBP,[8262]\n",
      "PUSH RBX\n",
      "XOR EBX,EBX\n",
      "CALL 4224\n",
      "JMP 4323\n",
      "MOV RDI,RBP\n",
      "ADD EBX,1\n",
      "CALL 4208\n",
      "CMP EBX,10000\n",
      "JZ 4352\n",
      "CMP EBX,100\n",
      "JNZ 4304\n",
      "MOV RDI,R12\n",
      "MOV EBX,101\n",
      "CALL 4208\n",
      "MOV RDI,RBP\n",
      "CALL 4208\n",
      "JMP 4304\n",
      "POP RBX\n",
      "XOR EAX,EAX\n",
      "POP RBP\n",
      "POP R12 \n",
      "\n",
      "#  RET \n",
      "\n",
      "0\n",
      "-> ENDBR64\n",
      "PUSH RBX\n",
      "LEA RSI,[8204]\n",
      "LEA RBX,[8227]\n",
      "MOV EDI,2\n",
      "SUB RSP,48\n",
      "MOV RAX,qword ptr FS:[40]\n",
      "MOV qword ptr [RSP + 40],RAX\n",
      "XOR EAX,EAX\n",
      "CALL 4224\n",
      "LEA RSI,[RSP + 28]\n",
      "MOV RDI,RBX\n",
      "XOR EAX,EAX\n",
      "CALL 4240\n",
      "LEA RSI,[8230]\n",
      "MOV EDI,2\n",
      "XOR EAX,EAX\n",
      "CALL 4224\n",
      "LEA RSI,[RSP + 32]\n",
      "MOV RDI,RBX\n",
      "XOR EAX,EAX\n",
      "CALL 4240\n",
      "LEA RSI,[8252]\n",
      "MOV EDI,2\n",
      "XOR EAX,EAX\n",
      "CALL 4224\n",
      "LEA RSI,[RSP + 36]\n",
      "MOV RDI,RBX\n",
      "XOR EAX,EAX\n",
      "CALL 4240\n",
      "MOVSS XMM2,dword ptr [RSP + 28]\n",
      "MOVSS XMM0,dword ptr [RSP + 32]\n",
      "LEA RSI,[8280]\n",
      "MOV EDI,2\n",
      "MOV EAX,1\n",
      "MOVAPS XMM3,XMM2\n",
      "SUBSS XMM3,dword ptr [RSP + 36]\n",
      "MOVAPS XMM1,XMM0\n",
      "DIVSS XMM1,dword ptr [8196]\n",
      "MULSS XMM1,XMM2\n",
      "PXOR XMM0,XMM0\n",
      "DIVSS XMM1,dword ptr [8200]\n",
      "ADDSS XMM1,XMM3\n",
      "CVTSS2SD XMM0,XMM1\n",
      "MOVSS dword ptr [RSP + 12],XMM1\n",
      "CALL 4224\n",
      "MOVSS XMM1,dword ptr [RSP + 12]\n",
      "MOVSS XMM0,dword ptr [RSP + 32]\n",
      "DIVSS XMM0,dword ptr [8196]\n",
      "LEA RSI,[8328]\n",
      "MOV EDI,2\n",
      "MOV EAX,1\n",
      "MULSS XMM0,XMM1\n",
      "MOVAPS XMM3,XMM1\n",
      "SUBSS XMM3,dword ptr [RSP + 36]\n",
      "MOVSS dword ptr [RSP + 28],XMM1\n",
      "DIVSS XMM0,dword ptr [8200]\n",
      "ADDSS XMM3,XMM0\n",
      "PXOR XMM0,XMM0\n",
      "CVTSS2SD XMM0,XMM3\n",
      "MOVSS dword ptr [RSP + 12],XMM3\n",
      "CALL 4224\n",
      "MOVSS XMM3,dword ptr [RSP + 12]\n",
      "MOVSS XMM2,dword ptr [RSP + 32]\n",
      "DIVSS XMM2,dword ptr [8196]\n",
      "MOVAPS XMM1,XMM2\n",
      "MOV EDI,2\n",
      "MOV EAX,1\n",
      "MOVAPS XMM0,XMM3\n",
      "SUBSS XMM0,dword ptr [RSP + 36]\n",
      "LEA RSI,[8376]\n",
      "MOVSS dword ptr [RSP + 28],XMM3\n",
      "MULSS XMM1,XMM3\n",
      "DIVSS XMM1,dword ptr [8200]\n",
      "ADDSS XMM0,XMM1\n",
      "CVTSS2SD XMM0,XMM0\n",
      "CALL 4224\n",
      "MOV RAX,qword ptr [RSP + 40]\n",
      "SUB RAX,qword ptr FS:[40]\n",
      "JNZ 4663\n",
      "ADD RSP,48\n",
      "XOR EAX,EAX\n",
      "POP RBX\n",
      "RET \n",
      "\n",
      "#  CALL 4208 \n",
      "\n",
      "0\n",
      "-> ENDBR64\n",
      "PUSH R12\n",
      "LEA RSI,[8200]\n",
      "MOV EDI,2\n",
      "PUSH RBP\n",
      "PUSH RBX\n",
      "SUB RSP,32\n",
      "MOV RAX,qword ptr FS:[40]\n",
      "MOV qword ptr [RSP + 24],RAX\n",
      "XOR EAX,EAX\n",
      "CALL 4224\n",
      "LEA RCX,[RSP + 16]\n",
      "LEA RDX,[RSP + 12]\n",
      "XOR EAX,EAX\n",
      "LEA RSI,[RSP + 8]\n",
      "LEA R8,[RSP + 20]\n",
      "LEA RDI,[8246]\n",
      "CALL 4240\n",
      "MOV EBX,dword ptr [RSP + 20]\n",
      "MOV R12D,dword ptr [RSP + 12]\n",
      "LEA RSI,[8258]\n",
      "MOV EBP,dword ptr [RSP + 8]\n",
      "MOV EAX,dword ptr [RSP + 16]\n",
      "MOV EDI,2\n",
      "IMUL EBP,EBX\n",
      "IMUL EAX,R12D\n",
      "IMUL EBX,R12D\n",
      "ADD EBP,EAX\n",
      "XOR EAX,EAX\n",
      "CALL 4224\n",
      "XOR EAX,EAX\n",
      "MOV ECX,EBX\n",
      "MOV EDX,EBP\n",
      "LEA RSI,[8252]\n",
      "MOV EDI,2\n",
      "CALL 4224\n",
      "MOV RAX,qword ptr [RSP + 24]\n",
      "SUB RAX,qword ptr FS:[40]\n",
      "JNZ 4434\n",
      "ADD RSP,32\n",
      "XOR EAX,EAX\n",
      "POP RBX\n",
      "POP RBP\n",
      "POP R12\n",
      "RET \n",
      "\n",
      "#  CALL 4208 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(label))\n",
    "for i in range(3):\n",
    "    print(label[i])\n",
    "    print('->',history[i] , '\\n')\n",
    "    print('# ',next_instruction[i] , '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready for tokenization, this time we truncate/pad each token to the same length of *512* tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# inputs = tokenizer(history, next_instruction, return_tensors='pt', \n",
    "#                    max_length=MAX_TOKEN_LEN, truncation=True, padding=True)\n",
    "\n",
    "inputs = tokenizer(history,return_tensors='pt',max_length=MAX_TOKEN_LEN, truncation=True, padding=True)\n",
    "ground_truth = inputs.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the *token_type_ids* tensors have been built correctly (eg **1** indicating sentence B tokens) by checking the first instance of *token_type_ids*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "        128001, 128001, 128001, 128001, 128001, 128001, 128000,    965,   3590,\n",
      "            49,   1227,    198,     47,  20088,    432,    717,    198,    877,\n",
      "            32,    432,  14137,  17706,  16551,     23,    933,  67122,  16421,\n",
      "            40,     11,     17,    198,    877,     32,    432,    717,  17706,\n",
      "         22091,     18,    933,     47,  20088,    432,  27187,    198,    877,\n",
      "            32,    432,  27187,  17706,  23038,     17,    933,     47,  20088,\n",
      "         29074,     55,    198,     55,    878,  50242,     55,     11,   8428,\n",
      "            55,    198,  26502,    220,  16460,     19,    198,     41,   5901,\n",
      "           220,  16739,     18,    198,  67122,    432,  18091,  24412,  27187,\n",
      "           198,  16040,  50242,     55,     11,     16,    198,  26502,    220,\n",
      "         12819,     23,    198,  94258,  50242,     55,     11,   1041,    410,\n",
      "           198,     41,     57,    220,  19305,     17,    198,  94258,  50242,\n",
      "            55,     11,   1041,    198,     41,  71030,    220,  14245,     19,\n",
      "           198,  67122,    432,  18091,  24412,    717,    198,  67122,  50242,\n",
      "            55,     11,   4645,    198,  26502,    220,  12819,     23,    198,\n",
      "         67122,    432,  18091,  24412,  27187,    198,  26502,    220,  12819,\n",
      "            23,    198,     41,   5901,    220,  14245,     19,    198,  48362,\n",
      "         29074,     55,    198,     55,    878,    469,   3027,  43225,   3027,\n",
      "           198,  48362,    432,  27187,    198,  48362,    432,    717])\n",
      "<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>ENDBR64\n",
      "PUSH R12\n",
      "LEA RSI,[4608]\n",
      "MOV EDI,2\n",
      "LEA R12,[8253]\n",
      "PUSH RBP\n",
      "LEA RBP,[8262]\n",
      "PUSH RBX\n",
      "XOR EBX,EBX\n",
      "CALL 4224\n",
      "JMP 4323\n",
      "MOV RDI,RBP\n",
      "ADD EBX,1\n",
      "CALL 4208\n",
      "CMP EBX,10000\n",
      "JZ 4352\n",
      "CMP EBX,100\n",
      "JNZ 4304\n",
      "MOV RDI,R12\n",
      "MOV EBX,101\n",
      "CALL 4208\n",
      "MOV RDI,RBP\n",
      "CALL 4208\n",
      "JMP 4304\n",
      "POP RBX\n",
      "XOR EAX,EAX\n",
      "POP RBP\n",
      "POP R12\n",
      "\n",
      "-->>>>\n",
      " ENDBR64\n",
      "PUSH R12\n",
      "LEA RSI,[4608]\n",
      "MOV EDI,2\n",
      "LEA R12,[8253]\n",
      "PUSH RBP\n",
      "LEA RBP,[8262]\n",
      "PUSH RBX\n",
      "XOR EBX,EBX\n",
      "CALL 4224\n",
      "JMP 4323\n",
      "MOV RDI,RBP\n",
      "ADD EBX,1\n",
      "CALL 4208\n",
      "CMP EBX,10000\n",
      "JZ 4352\n",
      "CMP EBX,100\n",
      "JNZ 4304\n",
      "MOV RDI,R12\n",
      "MOV EBX,101\n",
      "CALL 4208\n",
      "MOV RDI,RBP\n",
      "CALL 4208\n",
      "JMP 4304\n",
      "POP RBX\n",
      "XOR EAX,EAX\n",
      "POP RBP\n",
      "POP R12\n"
     ]
    }
   ],
   "source": [
    "print(inputs.input_ids[0])\n",
    "print(tokenizer.decode(inputs.input_ids[0]))\n",
    "print('\\n-->>>>\\n',history[0])\n",
    "# inputs.token_type_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **0** tokens following our sentence B tokens correspond to *PAD* tokens.\n",
    "\n",
    "Alongside this, we need to create a *labels* tensor too - which corresponds to the values contained within our `label` variable. Our *labels* tensor must be a *LongTensor*, and we will need to transpose the tensor so that it matches our other tensors' dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['next_sentence_label'] = torch.LongTensor([label]).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the labels tensor is simply a clone of the input_ids tensor before masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs['labels'] = inputs.input_ids.copy()\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128000,\n",
       "           965,   3590,     49,   1227,    198,  30138,    432,   4695,     11,\n",
       "          1187,    198,    877,     32,    432,  14137,  17706,  18831,     21,\n",
       "           933,  67122,  16421,     40,     11,     17,    198,  67122,    432,\n",
       "          3027,  36280,   1178,  10295,  25275,   7407,   1272,    933,  67122,\n",
       "          2874,   1178,  10295,    510,     49,   4695,    489,    220,     23,\n",
       "          1145,     49,   3027,    198,     55,    878,    469,   3027,  43225,\n",
       "          3027,    198,  26502,    220,  18517,     15,    198,     55,    878,\n",
       "           469,   3027,  43225,   3027,    198,    877,     32,    432,  17001,\n",
       "         17706,     49,   4695,    489,    220,     19,    933,  67122,    432,\n",
       "         14137,  24412,   4695,    198,    877,     32,    432,  18091,  17706,\n",
       "         23105,     18,    933,  26502,    220,  17837,     21,    198,  10238,\n",
       "           469,   3027,  43225,   3027,    198,     41,    877,    220,  20385,\n",
       "            16,    198,  67122,  21283,     55,  12260,   1178,  10295,    510,\n",
       "            49,   4695,    489,    220,     19,    933,  67122,  16421,     40,\n",
       "         12260,   1178,  10295,    510,     49,   4695,    933,  10238,  21283,\n",
       "            55,     11,   7650,     55,    198,     41,     57,    220,  19697,\n",
       "            17,    198,  67122,  16421,     55,     11,   7650,     55,    198,\n",
       "         67122,    469,   3027,     11,  23988,    198,  58656,   3492,  10295,\n",
       "           510,     49,   3027,    489,    432,   3027,      9,     16,    933,\n",
       "         67122,    469,  14137,     11,   1507,     55,    198,   6620,     48,\n",
       "           198,    926,   3166,    469,  14137,    198,  67122,    469,   3027,\n",
       "            11,   1600,     40,    198,  10238,  16421,     55,     11,   1507,\n",
       "            55,    198,     41,  71030,    220,  18318,     21,    198,  67122,\n",
       "           469,   3027,     11,  23988,    198,   6620,     48,    198,    926,\n",
       "          3166,    469,  14137,    198,  67122,  16421,     40,  43225,   3027,\n",
       "           198,  67122,  75850,  10295,    510,     49,   4695,   1145,     36,\n",
       "          3027,    198,  67122,    469,   3027,     11,   7650,     55,    198,\n",
       "          6620,     48,    198,    926,   3166,    469,  14137,    198,  67122,\n",
       "         16421,     55,     11,  23988,    198,    877,     32,    432,  14137,\n",
       "         17706,  23848,     17,    933,  67122,  16421,     40,     11,     17,\n",
       "           198,  67122,  75850,  10295,    510,     49,   4695,    489,    220,\n",
       "            19,   1145,     36,   3027,    198,  67122,  21283,     55,  43225,\n",
       "          3027,    198,     55,    878,    469,   3027,  43225,   3027,    198,\n",
       "         26502,    220,  18517,     15,    198,  67122,    432,   3027,  36280,\n",
       "          1178,  10295,    510,     49,   4695,    489,    220,     23,    933,\n",
       "         30138,    432,   3027,  36280,   1178,  10295,  25275,   7407,   1272,\n",
       "           933,     41,  71030,    220,  19697,     21,    198,     55,    878,\n",
       "           469,   3027,  43225,   3027,    198,  16040,    432,   4695,     11,\n",
       "          1187,    198,  39164,    198,  67122,    469,  14137,     11,  23988,\n",
       "           198,     41,   5901,    220,  20596,     22,    198,  26502,    220,\n",
       "         16460,     19,    198,  67122,  16421,     40,     11,     16])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we mask tokens in the input_ids tensor using the 15% probability for MLM - ensuring we don't mask CLS, SEP, or PAD tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "# create mask array\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4002, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr.shape\n",
    "# inputs.input_ids.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now take the indices of each True value within each vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4002,\n",
       " 4002,\n",
       " [[4,\n",
       "   6,\n",
       "   13,\n",
       "   16,\n",
       "   28,\n",
       "   33,\n",
       "   40,\n",
       "   44,\n",
       "   49,\n",
       "   51,\n",
       "   68,\n",
       "   75,\n",
       "   78,\n",
       "   79,\n",
       "   88,\n",
       "   92,\n",
       "   96,\n",
       "   97,\n",
       "   102,\n",
       "   110,\n",
       "   154,\n",
       "   159,\n",
       "   162,\n",
       "   170,\n",
       "   175,\n",
       "   179,\n",
       "   185,\n",
       "   193,\n",
       "   198,\n",
       "   208,\n",
       "   219,\n",
       "   235,\n",
       "   248,\n",
       "   254,\n",
       "   255,\n",
       "   257,\n",
       "   264,\n",
       "   267,\n",
       "   275,\n",
       "   276,\n",
       "   287,\n",
       "   289,\n",
       "   314,\n",
       "   320,\n",
       "   321,\n",
       "   334,\n",
       "   335,\n",
       "   349,\n",
       "   353,\n",
       "   356,\n",
       "   367,\n",
       "   387,\n",
       "   390,\n",
       "   393,\n",
       "   396,\n",
       "   397,\n",
       "   406,\n",
       "   413,\n",
       "   418,\n",
       "   421,\n",
       "   433,\n",
       "   434,\n",
       "   435,\n",
       "   438,\n",
       "   450,\n",
       "   458,\n",
       "   465,\n",
       "   475,\n",
       "   483,\n",
       "   489,\n",
       "   490,\n",
       "   497,\n",
       "   511],\n",
       "  [0,\n",
       "   10,\n",
       "   17,\n",
       "   19,\n",
       "   21,\n",
       "   29,\n",
       "   40,\n",
       "   54,\n",
       "   56,\n",
       "   60,\n",
       "   69,\n",
       "   73,\n",
       "   84,\n",
       "   89,\n",
       "   103,\n",
       "   120,\n",
       "   121,\n",
       "   155,\n",
       "   159,\n",
       "   164,\n",
       "   185,\n",
       "   187,\n",
       "   190,\n",
       "   199,\n",
       "   201,\n",
       "   207,\n",
       "   226,\n",
       "   230,\n",
       "   232,\n",
       "   233,\n",
       "   234,\n",
       "   253,\n",
       "   254,\n",
       "   264,\n",
       "   266,\n",
       "   270,\n",
       "   285,\n",
       "   297,\n",
       "   300,\n",
       "   309,\n",
       "   313,\n",
       "   318,\n",
       "   322,\n",
       "   330,\n",
       "   335,\n",
       "   341,\n",
       "   345,\n",
       "   346,\n",
       "   351,\n",
       "   358,\n",
       "   360,\n",
       "   361,\n",
       "   363,\n",
       "   380,\n",
       "   384,\n",
       "   385,\n",
       "   391,\n",
       "   396,\n",
       "   403,\n",
       "   438,\n",
       "   443,\n",
       "   445,\n",
       "   447,\n",
       "   448,\n",
       "   449,\n",
       "   450,\n",
       "   453,\n",
       "   455,\n",
       "   468,\n",
       "   478,\n",
       "   488,\n",
       "   496,\n",
       "   507,\n",
       "   508],\n",
       "  [4,\n",
       "   5,\n",
       "   20,\n",
       "   23,\n",
       "   26,\n",
       "   39,\n",
       "   40,\n",
       "   45,\n",
       "   46,\n",
       "   48,\n",
       "   52,\n",
       "   61,\n",
       "   67,\n",
       "   81,\n",
       "   96,\n",
       "   103,\n",
       "   106,\n",
       "   109,\n",
       "   113,\n",
       "   118,\n",
       "   120,\n",
       "   134,\n",
       "   150,\n",
       "   169,\n",
       "   171,\n",
       "   172,\n",
       "   193,\n",
       "   199,\n",
       "   200,\n",
       "   213,\n",
       "   216,\n",
       "   220,\n",
       "   221,\n",
       "   223,\n",
       "   224,\n",
       "   229,\n",
       "   231,\n",
       "   233,\n",
       "   244,\n",
       "   245,\n",
       "   252,\n",
       "   268,\n",
       "   269,\n",
       "   278,\n",
       "   286,\n",
       "   288,\n",
       "   290,\n",
       "   298,\n",
       "   346,\n",
       "   355,\n",
       "   359,\n",
       "   363,\n",
       "   366,\n",
       "   369,\n",
       "   376,\n",
       "   386,\n",
       "   408,\n",
       "   409,\n",
       "   432,\n",
       "   434,\n",
       "   437,\n",
       "   445,\n",
       "   449,\n",
       "   454,\n",
       "   460,\n",
       "   461,\n",
       "   463,\n",
       "   468,\n",
       "   470,\n",
       "   490,\n",
       "   498,\n",
       "   504,\n",
       "   510,\n",
       "   511]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (selection) , len(inputs.input_ids), selection[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then apply these indices to each row in input_ids, assigning each value at these indices a value of 103."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "        128001, 128001,    965,    198,    432,  67122,     32,  17706,    933,\n",
       "         29074,     55,    198,   5901,  67122,  24412,     23,    198,  94258,\n",
       "            11,     55,     19,    198,     23,  26502,   5901,    220,    198,\n",
       "           717])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[0][selection[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_labels = []\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    masked_labels.append(inputs.input_ids[i, selection[i]])\n",
    "    inputs.input_ids[i, selection[i]] = 103\n",
    "# masked_labels[0]\n",
    "inputs[\"mask_arr\"] = mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'next_sentence_label', 'labels', 'mask_arr'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inputs` tensors are now ready, and we can begin building the model input pipeline for training. We first create a PyTorch dataset from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeditationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our data using the `MeditationDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MeditationsDataset(inputs)\n",
    "# print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4002\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 2001) range(2001, 4002)\n"
     ]
    }
   ],
   "source": [
    "print( range(len(train_text)), range(len(train_text) , len(dataset)  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001, 2001)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data_portion =  len(train_text)/(len(train_text) + len( test_text) )\n",
    "# print(train_data_portion ,(len(train_text) + len( test_text) ))\n",
    "\n",
    "# train_size = int(train_data_portion * len(dataset))\n",
    "# validation_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset  = torch.utils.data.Subset(dataset, range(len(train_text)))\n",
    "validation_dataset = torch.utils.data.Subset(dataset, range(len(train_text) , len(dataset)))\n",
    "\n",
    "len(train_dataset) , len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And initialize the dataloader, which we'll be using to load our data into the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader      = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto setting up the training loop. First we setup GPU/CPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the training mode of our model, and initialize our optimizer (Adam with weighted decay - reduces chance of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto the training loop, we'll train for a couple of epochs (change `epochs` to modify this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odict_keys(['loss', 'prediction_logits', 'seq_relationship_logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import *\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graph(training_data, validation_data , label ):\n",
    "\n",
    "    font_size = 10\n",
    "    x_labels = [ i for i in range(len(training_data)) ]\n",
    "\n",
    "    plt.ylabel(' F1 ',fontsize=font_size)\n",
    "    plt.plot(x_labels, training_data , 'r') \n",
    "    plt.plot(x_labels, validation_data , 'b') \n",
    "    plt.xlabel(\"Epoch\", fontsize=font_size)\n",
    "    plt.title(label,fontsize=font_size)\n",
    "    plt.legend(['Training', 'Validation'], loc='upper left') \n",
    "    \n",
    "    plt.savefig('./../../results/'+EXPERIMENT_NAME+label+'.pdf')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1001 [00:00<?, ?it/s]/tmp/ipykernel_660385/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|███████████████████| 1001/1001 [07:03<00:00,  2.36it/s, loss=0.12]\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    Masked Token f1 0.712684415091251     SEQ F1 0.9517644403910472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "from itertools import chain\n",
    "\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "epochs =1\n",
    "counter = 0\n",
    "\n",
    "global_instruction_metrices = []\n",
    "global_masked_token_metrices = []\n",
    "\n",
    "v_global_instruction_metrices = []\n",
    "v_global_masked_token_metrices = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    train_loop = tqdm(train_loader, leave=True)\n",
    "    \n",
    "    \n",
    "    instruction_predictions_all, instruction_ground_truths_all = None, None\n",
    "    masked_token_predictions_all, masked_token_ground_truths_all = None, None\n",
    "    seq_predictions_all, seq_ground_truths_all = None, None\n",
    "    \n",
    "    # activate training mode\n",
    "    model.train()\n",
    "    for N,batch in enumerate(train_loop):\n",
    "\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        # token_type_ids = batch['token_type_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        batch_mask_arr = batch ['mask_arr']\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        token_prediction = torch.argmax(logits, axis=-1)\n",
    "\n",
    "        # token_prediction = token_prediction.reshape( BATCH_SIZE ,MAX_TOKEN_LEN)\n",
    "        # token_prediction.to(device)\n",
    "        # print(token_prediction.shape ,batch_mask_arr.shape ,batch_mask_arr , labels.shape)\n",
    "        # print(token_prediction.shape , logits.shape , token_prediction.reshape(2, 512))\n",
    "\n",
    "        # batch_masks = selection [BATCH_SIZE*N : (BATCH_SIZE*(N+1))]\n",
    "        # print('batch_masks old: ',batch_masks)\n",
    "        # print(batch ['mask_arr'].shape) #torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        \n",
    "        batch_masks =   [ torch.flatten(bm.nonzero()).tolist()  for bm in batch_mask_arr]    # torch.flatten(batch ['mask_arr'].nonzero()).tolist()\n",
    "        # print('batch_masks new: ',batch_masks)\n",
    "        \n",
    "        # print(\"BATCH_SIZE*N : (BATCH_SIZE*(N+1)): \",BATCH_SIZE*N , (BATCH_SIZE*(N+1)) )\n",
    "        # print(\"batch_masks:\",batch_masks)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        masked_token_prediction = [ token[batch_masks[t]].tolist() for t,token in enumerate(token_prediction) ]\n",
    "        masked_token_prediction = list(chain.from_iterable(masked_token_prediction))\n",
    "        \n",
    "        masked_token_ground_truth   = [ token[batch_masks[t]].tolist() for t,token in enumerate(labels) ]\n",
    "        masked_token_ground_truth = list(chain.from_iterable(masked_token_ground_truth))\n",
    "        \n",
    "\n",
    "        # print(token_prediction , token_ground_truth)\n",
    "\n",
    "        # token_prediction = token_prediction.detach().cpu().numpy().flatten()\n",
    "        # token_ground_truth = labels.detach().cpu().numpy().flatten()\n",
    "\n",
    "        # print(\"token_prediction  : \", token_prediction)\n",
    "        # print(\"token_ground_truth: \", token_ground_truth)\n",
    "\n",
    "\n",
    "        seq_predictions   = token_prediction.detach().cpu().numpy().flatten()\n",
    "        seq_ground_truths = labels.detach().cpu().numpy().flatten()\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        if N==0:\n",
    "\n",
    "            \n",
    "            masked_token_predictions_all         = masked_token_prediction\n",
    "            masked_token_ground_truths_all       = masked_token_ground_truth  \n",
    "\n",
    "\n",
    "            seq_predictions_all = seq_predictions\n",
    "            seq_ground_truths_all = seq_ground_truths\n",
    "            \n",
    "        else:\n",
    "\n",
    "            masked_token_predictions_all   = np.concatenate((masked_token_predictions_all, masked_token_prediction))\n",
    "            masked_token_ground_truths_all = np.concatenate((masked_token_ground_truths_all, masked_token_ground_truth))\n",
    "\n",
    "            seq_predictions_all = np.concatenate((seq_predictions_all, seq_predictions))\n",
    "            seq_ground_truths_all = np.concatenate((seq_ground_truths_all, seq_ground_truths))\n",
    "\n",
    "\n",
    "                # Compute loss\n",
    "        logits = logits.view(-1, logits.size(-1))  # [batch_size * seq_length, vocab_size]\n",
    "        labels = labels.view(-1)  # [batch_size * seq_length]\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        # Now, you can update the model's weights using the optimizer\n",
    "        optim.step()\n",
    "        # Zero gradients after updating the model's weights\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # extract loss\n",
    "        # loss = outputs.loss\n",
    "        # # calculate loss for every parameter that needs grad update\n",
    "        # loss.backward()\n",
    "        # # update parameters\n",
    "        # optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        train_loop.set_description(f'Epoch {epoch}')\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    \n",
    "\n",
    "    masked_token_accuracy = (accuracy_score(masked_token_ground_truths_all, masked_token_predictions_all))\n",
    "    masked_token_precision, masked_token_recall, masked_token_f1, _ = precision_recall_fscore_support(masked_token_ground_truths_all,masked_token_predictions_all,average='weighted')\n",
    "\n",
    "    seq_precision, seq_recall, seq_f1, _ = precision_recall_fscore_support(seq_ground_truths_all,seq_predictions_all,average='weighted')\n",
    "    \n",
    "    print(\"Training: \",    '  Masked Token f1',masked_token_f1 , \"    SEQ F1\",seq_f1)\n",
    "    global_masked_token_metrices.append( masked_token_f1) \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[0,1,2,3,4,5]\n",
    "a[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128256/2/512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "loss = CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "\n",
    "print(input.shape,target.shape )\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "print(input.shape,target.shape )\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Example logits: [batch_size, seq_length, vocab_size]\n",
    "logits = torch.randn(2, 512, 32000)  # Raw model output\n",
    "labels = torch.randint(0, 32000, (2, 512))  # Ground truth labels\n",
    "\n",
    "print(logits.shape , labels.shape )\n",
    "# Reshape logits and labels for CrossEntropyLoss\n",
    "logits = logits.view(-1, logits.size(-1))  # [batch_size * seq_length, vocab_size]\n",
    "labels = labels.view(-1)  # [batch_size * seq_length]\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = CrossEntropyLoss()\n",
    "print(logits.shape , labels.shape )\n",
    "loss = loss_fn(logits, labels)\n",
    "\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
