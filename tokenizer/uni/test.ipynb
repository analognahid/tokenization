{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "#!/usr/bin/env python3\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import sys,os\n",
    "from elftools.elf.elffile import ELFFile\n",
    "from elftools.elf.segments import Segment\n",
    "from capstone import *\n",
    "from capstone.x86 import *\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments,LlamaModel,LlamaForSequenceClassification,LlamaTokenizerFast\n",
    "import os\n",
    "import json \n",
    "import torch, os,re\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from num2words import num2words\n",
    "\n",
    "from trl import SFTTrainer\n",
    "# import torch\n",
    "\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "# from torch.distributed.fsdp.wrap import auto_wrap\n",
    "\n",
    "\n",
    "login(token = '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')\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import sys,os\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MAX_TOKEN_LEN = 1024\n",
    "BATCH_SIZE =4\n",
    "EXPERIMENT_NAME = 'cusTokenizer_UNI_25k_ASIS'\n",
    "\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction,BertForPreTraining\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# tokenizer = LlamaTokenizerFast.from_pretrained('meta-llama/Llama-3.2-1B') #(\"./../../models/\" + EXPERIMENT_NAME)\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"./../../models/\" + EXPERIMENT_NAME)\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"./../../models/\" + EXPERIMENT_NAME)\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'}) #todo remove for llama loader\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# print(tokenizer.pad_token) \n",
    "# model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "# model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "\n",
    " \n",
    "# LlamaModel LlamaForSequenceClassification LlamaForCausalLM\n",
    "\n",
    "# Load the model\n",
    "model = LlamaForCausalLM.from_pretrained('meta-llama/Llama-3.2-1B')  # For regression task\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "Functions Count:  80000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "DATA_PATH = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions'\n",
    "\n",
    "TRAIN_DATA_PATH  ='/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/train/'\n",
    "\n",
    "TEST_DATA_PATH   = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/test/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_json_files = [os.path.join(TRAIN_DATA_PATH, f) for f in os.listdir(TRAIN_DATA_PATH) ]\n",
    "\n",
    "test_json_files = [os.path.join(TEST_DATA_PATH, f) for f in os.listdir(TEST_DATA_PATH) ]\n",
    "\n",
    "# disassembly_decimal disassembly_all_number_to_words disassembly_decimal\n",
    "data_key = \"disassembly_decimal\"\n",
    "\n",
    "print(len(train_json_files))\n",
    "def read_corpus(json_files):\n",
    "\n",
    "    all = []\n",
    "\n",
    "    for k, j_file in enumerate(json_files):\n",
    "        # if k>32:\n",
    "        #     break\n",
    "        try:\n",
    "\n",
    "            with open(j_file, 'r') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                funct = data[data_key]['input']\n",
    "                \n",
    "                all.append(funct)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "    return all\n",
    "    \n",
    "\n",
    "\n",
    "train_text = read_corpus(train_json_files)\n",
    "test_text  = read_corpus(test_json_files)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Functions Count: \",len(train_text), '\\n')\n",
    "example = train_text[10]\n",
    "text = train_text + test_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDBR64\n",
      "PUSH R15\n",
      "LEA RDI,[8270]\n",
      "PUSH R14\n",
      "PUSH R13\n",
      "PUSH R12\n",
      "PUSH RBP\n",
      "PUSH RBX\n",
      "SUB RSP,8\n",
      "CALL 4256\n",
      "MOVSXD RCX,dword ptr [17568]\n",
      "CMP ECX,1\n",
      "JLE 5555\n",
      "XOR R13D,R13D\n",
      "LEA RBX,[16576]\n",
      "LEA R14,[16480]\n",
      "LEA RBP,[8297]\n",
      "LEA R12,[16640]\n",
      "NOP dword ptr [RAX]\n",
      "XOR EAX,EAX\n",
      "MOV R15D,4294967295\n",
      "MOV EDX,987654321\n",
      "NOP dword ptr [RAX]\n",
      "MOV ESI,dword ptr [RBX + RAX*4]\n",
      "TEST ESI,ESI\n",
      "JNZ 5476\n",
      "MOV ESI,dword ptr [R12 + RAX*4]\n",
      "CMP EDX,ESI\n",
      "CMOVG R15D,EAX\n",
      "CMOVG EDX,ESI\n",
      "ADD RAX,1\n",
      "CMP RCX,RAX\n",
      "JNZ 5456\n",
      "MOVSXD RAX,R15D\n",
      "ADD dword ptr [16560],EDX\n",
      "MOV RSI,RBP\n",
      "MOV EDX,R15D\n",
      "MOV ECX,dword ptr [R14 + RAX*8]\n",
      "MOV R8D,dword ptr [R14 + RAX*8 + 4]\n",
      "MOV dword ptr [RBX + RAX*4],1\n",
      "XOR EAX,EAX\n",
      "MOV EDI,2\n",
      "ADD R13D,1\n",
      "CALL 4288\n",
      "MOV EDI,R15D\n",
      "CALL 5104\n",
      "MOVSXD RCX,dword ptr [17568]\n",
      "LEA EAX,[RCX + -1]\n",
      "CMP R13D,EAX\n",
      "JL 5440\n",
      "MOV EDX,dword ptr [16560]\n",
      "ADD RSP,8\n",
      "LEA RSI,[8321]\n",
      "XOR EAX,EAX\n",
      "POP RBX\n",
      "MOV EDI,2\n",
      "POP RBP\n",
      "POP R12\n",
      "POP R13\n",
      "POP R14\n",
      "POP R15\n",
      "JMP 4288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text[51].split(delim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assign a 50% probability of using the genuine next sentence, and 50% probability of using another random sentence.\n",
    "\n",
    "To make this simpler, we'll create a *'bag'* of individual sentences to pull from when selecting a random sentence B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5312741 100000\n"
     ]
    }
   ],
   "source": [
    "delim = '\\n'\n",
    "bag = [instruction for instruction_cluster in text for instruction in instruction_cluster.split(delim)  if instruction!= '']\n",
    "bag_size = len(bag)\n",
    "print(bag_size , len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we create our 50/50 NIP training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "['ENDBR64', 'PUSH R12', 'LEA RSI,[4608]', 'MOV EDI,2', 'LEA R12,[8253]', 'PUSH RBP', 'LEA RBP,[8262]', 'PUSH RBX', 'XOR EBX,EBX', 'CALL 4224', 'JMP 4323', 'MOV RDI,RBP', 'ADD EBX,1', 'CALL 4208', 'CMP EBX,10000', 'JZ 4352', 'CMP EBX,100', 'JNZ 4304', 'MOV RDI,R12', 'MOV EBX,101', 'CALL 4208', 'MOV RDI,RBP', 'CALL 4208', 'JMP 4304', 'POP RBX', 'XOR EAX,EAX', 'POP RBP', 'POP R12', 'RET']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "history = []\n",
    "next_instruction = []\n",
    "label = []\n",
    "\n",
    "\n",
    "instruction_pages = []\n",
    "for instruction_cluster in text:\n",
    "    instructions = [\n",
    "        instruction for instruction in instruction_cluster.split(delim) if instruction != ''\n",
    "    ]\n",
    "\n",
    "    instruction_pages.append(instructions)\n",
    "    # if len(instructions)>page_len:\n",
    "        \n",
    "    #     for i in range(0,len(instructions),page_len):\n",
    "    #         instruction_pages.append(instructions[i:i+page_len])\n",
    "        \n",
    "print(len(instruction_pages))\n",
    "print(instruction_pages[0])\n",
    "\n",
    "for instruction_page in instruction_pages:\n",
    "    \n",
    "#     instructions = [\n",
    "#         instruction for instruction in instruction_page.split(';') if instruction != ''\n",
    "#     ]\n",
    "    \n",
    "    \n",
    "#     num_instructions = len(instruction_page)\n",
    "    \n",
    "    \n",
    "\n",
    "#     start = random.randint(0, num_instructions-2)\n",
    "    # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "    if random.random() >= 0.5:\n",
    "        # this is IsNextSentence\n",
    "        history.append(delim.join(instruction_page[:-1]))\n",
    "        next_instruction.append(instruction_page[-1])\n",
    "        label.append(0)\n",
    "    else:\n",
    "        index = random.randint(0, bag_size-1)\n",
    "        # this is NotNextSentence\n",
    "        history.append(delim.join(instruction_page[:-1]))\n",
    "        next_instruction.append(bag[index])\n",
    "        label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "1\n",
      "-> ENDBR64\n",
      "PUSH R12\n",
      "LEA RSI,[4608]\n",
      "MOV EDI,2\n",
      "LEA R12,[8253]\n",
      "PUSH RBP\n",
      "LEA RBP,[8262]\n",
      "PUSH RBX\n",
      "XOR EBX,EBX\n",
      "CALL 4224\n",
      "JMP 4323\n",
      "MOV RDI,RBP\n",
      "ADD EBX,1\n",
      "CALL 4208\n",
      "CMP EBX,10000\n",
      "JZ 4352\n",
      "CMP EBX,100\n",
      "JNZ 4304\n",
      "MOV RDI,R12\n",
      "MOV EBX,101\n",
      "CALL 4208\n",
      "MOV RDI,RBP\n",
      "CALL 4208\n",
      "JMP 4304\n",
      "POP RBX\n",
      "XOR EAX,EAX\n",
      "POP RBP\n",
      "POP R12 \n",
      "\n",
      "#  LEA RSI,[8196] \n",
      "\n",
      "1\n",
      "-> ENDBR64\n",
      "PUSH RBX\n",
      "LEA RSI,[8204]\n",
      "LEA RBX,[8227]\n",
      "MOV EDI,2\n",
      "SUB RSP,48\n",
      "MOV RAX,qword ptr FS:[40]\n",
      "MOV qword ptr [RSP + 40],RAX\n",
      "XOR EAX,EAX\n",
      "CALL 4224\n",
      "LEA RSI,[RSP + 28]\n",
      "MOV RDI,RBX\n",
      "XOR EAX,EAX\n",
      "CALL 4240\n",
      "LEA RSI,[8230]\n",
      "MOV EDI,2\n",
      "XOR EAX,EAX\n",
      "CALL 4224\n",
      "LEA RSI,[RSP + 32]\n",
      "MOV RDI,RBX\n",
      "XOR EAX,EAX\n",
      "CALL 4240\n",
      "LEA RSI,[8252]\n",
      "MOV EDI,2\n",
      "XOR EAX,EAX\n",
      "CALL 4224\n",
      "LEA RSI,[RSP + 36]\n",
      "MOV RDI,RBX\n",
      "XOR EAX,EAX\n",
      "CALL 4240\n",
      "MOVSS XMM2,dword ptr [RSP + 28]\n",
      "MOVSS XMM0,dword ptr [RSP + 32]\n",
      "LEA RSI,[8280]\n",
      "MOV EDI,2\n",
      "MOV EAX,1\n",
      "MOVAPS XMM3,XMM2\n",
      "SUBSS XMM3,dword ptr [RSP + 36]\n",
      "MOVAPS XMM1,XMM0\n",
      "DIVSS XMM1,dword ptr [8196]\n",
      "MULSS XMM1,XMM2\n",
      "PXOR XMM0,XMM0\n",
      "DIVSS XMM1,dword ptr [8200]\n",
      "ADDSS XMM1,XMM3\n",
      "CVTSS2SD XMM0,XMM1\n",
      "MOVSS dword ptr [RSP + 12],XMM1\n",
      "CALL 4224\n",
      "MOVSS XMM1,dword ptr [RSP + 12]\n",
      "MOVSS XMM0,dword ptr [RSP + 32]\n",
      "DIVSS XMM0,dword ptr [8196]\n",
      "LEA RSI,[8328]\n",
      "MOV EDI,2\n",
      "MOV EAX,1\n",
      "MULSS XMM0,XMM1\n",
      "MOVAPS XMM3,XMM1\n",
      "SUBSS XMM3,dword ptr [RSP + 36]\n",
      "MOVSS dword ptr [RSP + 28],XMM1\n",
      "DIVSS XMM0,dword ptr [8200]\n",
      "ADDSS XMM3,XMM0\n",
      "PXOR XMM0,XMM0\n",
      "CVTSS2SD XMM0,XMM3\n",
      "MOVSS dword ptr [RSP + 12],XMM3\n",
      "CALL 4224\n",
      "MOVSS XMM3,dword ptr [RSP + 12]\n",
      "MOVSS XMM2,dword ptr [RSP + 32]\n",
      "DIVSS XMM2,dword ptr [8196]\n",
      "MOVAPS XMM1,XMM2\n",
      "MOV EDI,2\n",
      "MOV EAX,1\n",
      "MOVAPS XMM0,XMM3\n",
      "SUBSS XMM0,dword ptr [RSP + 36]\n",
      "LEA RSI,[8376]\n",
      "MOVSS dword ptr [RSP + 28],XMM3\n",
      "MULSS XMM1,XMM3\n",
      "DIVSS XMM1,dword ptr [8200]\n",
      "ADDSS XMM0,XMM1\n",
      "CVTSS2SD XMM0,XMM0\n",
      "CALL 4224\n",
      "MOV RAX,qword ptr [RSP + 40]\n",
      "SUB RAX,qword ptr FS:[40]\n",
      "JNZ 4663\n",
      "ADD RSP,48\n",
      "XOR EAX,EAX\n",
      "POP RBX\n",
      "RET \n",
      "\n",
      "#  MOVZX EDX,byte ptr [16448] \n",
      "\n",
      "1\n",
      "-> ENDBR64\n",
      "PUSH R12\n",
      "LEA RSI,[8200]\n",
      "MOV EDI,2\n",
      "PUSH RBP\n",
      "PUSH RBX\n",
      "SUB RSP,32\n",
      "MOV RAX,qword ptr FS:[40]\n",
      "MOV qword ptr [RSP + 24],RAX\n",
      "XOR EAX,EAX\n",
      "CALL 4224\n",
      "LEA RCX,[RSP + 16]\n",
      "LEA RDX,[RSP + 12]\n",
      "XOR EAX,EAX\n",
      "LEA RSI,[RSP + 8]\n",
      "LEA R8,[RSP + 20]\n",
      "LEA RDI,[8246]\n",
      "CALL 4240\n",
      "MOV EBX,dword ptr [RSP + 20]\n",
      "MOV R12D,dword ptr [RSP + 12]\n",
      "LEA RSI,[8258]\n",
      "MOV EBP,dword ptr [RSP + 8]\n",
      "MOV EAX,dword ptr [RSP + 16]\n",
      "MOV EDI,2\n",
      "IMUL EBP,EBX\n",
      "IMUL EAX,R12D\n",
      "IMUL EBX,R12D\n",
      "ADD EBP,EAX\n",
      "XOR EAX,EAX\n",
      "CALL 4224\n",
      "XOR EAX,EAX\n",
      "MOV ECX,EBX\n",
      "MOV EDX,EBP\n",
      "LEA RSI,[8252]\n",
      "MOV EDI,2\n",
      "CALL 4224\n",
      "MOV RAX,qword ptr [RSP + 24]\n",
      "SUB RAX,qword ptr FS:[40]\n",
      "JNZ 4434\n",
      "ADD RSP,32\n",
      "XOR EAX,EAX\n",
      "POP RBX\n",
      "POP RBP\n",
      "POP R12\n",
      "RET \n",
      "\n",
      "#  POP R13 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(label))\n",
    "for i in range(3):\n",
    "    print(label[i])\n",
    "    print('->',history[i] , '\\n')\n",
    "    print('# ',next_instruction[i] , '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready for tokenization, this time we truncate/pad each token to the same length of *512* tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# inputs = tokenizer(history, next_instruction, return_tensors='pt', \n",
    "#                    max_length=MAX_TOKEN_LEN, truncation=True, padding=True)\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "inputs = tokenizer(history,return_tensors='pt',max_length=MAX_TOKEN_LEN, truncation=True, padding=True)\n",
    "ground_truth = inputs.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the *token_type_ids* tensors have been built correctly (eg **1** indicating sentence B tokens) by checking the first instance of *token_type_ids*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25001, 25001, 25001,  ...,    40,    30,   174])\n",
      "</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> <s> ▁ENDBR64\n",
      " PUSH ▁R12\n",
      " LEA ▁RSI,[4608]\n",
      " MOV ▁EDI,2\n",
      " LEA ▁R12,[8253]\n",
      " PUSH ▁RBP\n",
      " LEA ▁RBP,[8262]\n",
      " PUSH ▁ RBX\n",
      "XOR ▁EBX,EBX\n",
      "C A LL ▁4224\n",
      "J MP ▁4323\n",
      " MOV ▁RDI,RBP\n",
      "ADD ▁EBX,1\n",
      "C A LL ▁4208\n",
      " CMP ▁EBX,10000\n",
      "J Z ▁4352\n",
      " CMP ▁EBX,100\n",
      "J N Z ▁4304\n",
      "MOV ▁RDI,R12\n",
      "MOV ▁EBX,101\n",
      " C A LL ▁4208\n",
      "MOV ▁RDI,RBP\n",
      "C A LL ▁4208\n",
      " J MP ▁4304\n",
      " POP ▁ RBX\n",
      "XOR ▁EAX,EAX \n",
      "POP ▁RBP\n",
      " POP ▁R12\n",
      "\n",
      "-->>>>\n",
      " ENDBR64\n",
      "PUSH R12\n",
      "LEA RSI,[4608]\n",
      "MOV EDI,2\n",
      "LEA R12,[8253]\n",
      "PUSH RBP\n",
      "LEA RBP,[8262]\n",
      "PUSH RBX\n",
      "XOR EBX,EBX\n",
      "CALL 4224\n",
      "JMP 4323\n",
      "MOV RDI,RBP\n",
      "ADD EBX,1\n",
      "CALL 4208\n",
      "CMP EBX,10000\n",
      "JZ 4352\n",
      "CMP EBX,100\n",
      "JNZ 4304\n",
      "MOV RDI,R12\n",
      "MOV EBX,101\n",
      "CALL 4208\n",
      "MOV RDI,RBP\n",
      "CALL 4208\n",
      "JMP 4304\n",
      "POP RBX\n",
      "XOR EAX,EAX\n",
      "POP RBP\n",
      "POP R12\n"
     ]
    }
   ],
   "source": [
    "print(inputs.input_ids[0])\n",
    "print(tokenizer.decode(inputs.input_ids[0]))\n",
    "print('\\n-->>>>\\n',history[0])\n",
    "# inputs.token_type_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **0** tokens following our sentence B tokens correspond to *PAD* tokens.\n",
    "\n",
    "Alongside this, we need to create a *labels* tensor too - which corresponds to the values contained within our `label` variable. Our *labels* tensor must be a *LongTensor*, and we will need to transpose the tensor so that it matches our other tensors' dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['next_sentence_label'] = torch.LongTensor([label]).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the labels tensor is simply a clone of the input_ids tensor before masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs['labels'] = inputs.input_ids.copy()\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25001, 25001, 25001,  ...,   173,    30,   420])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we mask tokens in the input_ids tensor using the 15% probability for MLM - ensuring we don't mask CLS, SEP, or PAD tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "# create mask array\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 1024])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr.shape\n",
    "# inputs.input_ids.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now take the indices of each True value within each vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,\n",
       " 100000,\n",
       " [[6,\n",
       "   9,\n",
       "   22,\n",
       "   29,\n",
       "   39,\n",
       "   49,\n",
       "   52,\n",
       "   53,\n",
       "   62,\n",
       "   73,\n",
       "   76,\n",
       "   77,\n",
       "   84,\n",
       "   90,\n",
       "   95,\n",
       "   100,\n",
       "   102,\n",
       "   109,\n",
       "   128,\n",
       "   133,\n",
       "   140,\n",
       "   145,\n",
       "   150,\n",
       "   157,\n",
       "   163,\n",
       "   190,\n",
       "   198,\n",
       "   202,\n",
       "   204,\n",
       "   211,\n",
       "   215,\n",
       "   220,\n",
       "   244,\n",
       "   253,\n",
       "   260,\n",
       "   269,\n",
       "   284,\n",
       "   288,\n",
       "   291,\n",
       "   297,\n",
       "   303,\n",
       "   310,\n",
       "   311,\n",
       "   319,\n",
       "   323,\n",
       "   324,\n",
       "   325,\n",
       "   328,\n",
       "   334,\n",
       "   341,\n",
       "   343,\n",
       "   344,\n",
       "   345,\n",
       "   355,\n",
       "   358,\n",
       "   362,\n",
       "   365,\n",
       "   370,\n",
       "   380,\n",
       "   381,\n",
       "   388,\n",
       "   391,\n",
       "   393,\n",
       "   397,\n",
       "   407,\n",
       "   412,\n",
       "   415,\n",
       "   417,\n",
       "   420,\n",
       "   432,\n",
       "   449,\n",
       "   462,\n",
       "   465,\n",
       "   467,\n",
       "   471,\n",
       "   477,\n",
       "   479,\n",
       "   483,\n",
       "   485,\n",
       "   486,\n",
       "   495,\n",
       "   497,\n",
       "   501,\n",
       "   506,\n",
       "   514,\n",
       "   523,\n",
       "   530,\n",
       "   544,\n",
       "   549,\n",
       "   557,\n",
       "   564,\n",
       "   566,\n",
       "   570,\n",
       "   586,\n",
       "   599,\n",
       "   604,\n",
       "   608,\n",
       "   610,\n",
       "   614,\n",
       "   616,\n",
       "   617,\n",
       "   618,\n",
       "   619,\n",
       "   630,\n",
       "   635,\n",
       "   636,\n",
       "   640,\n",
       "   651,\n",
       "   654,\n",
       "   658,\n",
       "   660,\n",
       "   667,\n",
       "   670,\n",
       "   676,\n",
       "   708,\n",
       "   718,\n",
       "   738,\n",
       "   759,\n",
       "   761,\n",
       "   764,\n",
       "   771,\n",
       "   773,\n",
       "   787,\n",
       "   792,\n",
       "   797,\n",
       "   808,\n",
       "   809,\n",
       "   847,\n",
       "   851,\n",
       "   857,\n",
       "   862,\n",
       "   867,\n",
       "   868,\n",
       "   876,\n",
       "   890,\n",
       "   895,\n",
       "   907,\n",
       "   910,\n",
       "   911,\n",
       "   918,\n",
       "   923,\n",
       "   929,\n",
       "   935,\n",
       "   937,\n",
       "   941,\n",
       "   943,\n",
       "   953,\n",
       "   954,\n",
       "   956,\n",
       "   970,\n",
       "   1000,\n",
       "   1003,\n",
       "   1004,\n",
       "   1012,\n",
       "   1013,\n",
       "   1014],\n",
       "  [36,\n",
       "   46,\n",
       "   61,\n",
       "   69,\n",
       "   85,\n",
       "   97,\n",
       "   99,\n",
       "   105,\n",
       "   107,\n",
       "   108,\n",
       "   114,\n",
       "   116,\n",
       "   121,\n",
       "   133,\n",
       "   140,\n",
       "   143,\n",
       "   146,\n",
       "   155,\n",
       "   175,\n",
       "   177,\n",
       "   178,\n",
       "   194,\n",
       "   200,\n",
       "   202,\n",
       "   206,\n",
       "   223,\n",
       "   237,\n",
       "   239,\n",
       "   245,\n",
       "   251,\n",
       "   262,\n",
       "   271,\n",
       "   273,\n",
       "   280,\n",
       "   281,\n",
       "   286,\n",
       "   289,\n",
       "   292,\n",
       "   313,\n",
       "   319,\n",
       "   338,\n",
       "   339,\n",
       "   346,\n",
       "   354,\n",
       "   357,\n",
       "   359,\n",
       "   368,\n",
       "   369,\n",
       "   372,\n",
       "   373,\n",
       "   378,\n",
       "   394,\n",
       "   402,\n",
       "   404,\n",
       "   408,\n",
       "   410,\n",
       "   415,\n",
       "   419,\n",
       "   420,\n",
       "   427,\n",
       "   440,\n",
       "   442,\n",
       "   449,\n",
       "   465,\n",
       "   468,\n",
       "   473,\n",
       "   486,\n",
       "   502,\n",
       "   514,\n",
       "   518,\n",
       "   523,\n",
       "   526,\n",
       "   540,\n",
       "   543,\n",
       "   553,\n",
       "   558,\n",
       "   567,\n",
       "   574,\n",
       "   578,\n",
       "   583,\n",
       "   595,\n",
       "   607,\n",
       "   616,\n",
       "   624,\n",
       "   627,\n",
       "   635,\n",
       "   639,\n",
       "   647,\n",
       "   650,\n",
       "   672,\n",
       "   673,\n",
       "   676,\n",
       "   695,\n",
       "   701,\n",
       "   709,\n",
       "   714,\n",
       "   719,\n",
       "   722,\n",
       "   723,\n",
       "   733,\n",
       "   734,\n",
       "   752,\n",
       "   757,\n",
       "   761,\n",
       "   766,\n",
       "   768,\n",
       "   774,\n",
       "   775,\n",
       "   781,\n",
       "   789,\n",
       "   792,\n",
       "   794,\n",
       "   799,\n",
       "   801,\n",
       "   806,\n",
       "   807,\n",
       "   828,\n",
       "   835,\n",
       "   836,\n",
       "   844,\n",
       "   846,\n",
       "   848,\n",
       "   850,\n",
       "   854,\n",
       "   861,\n",
       "   863,\n",
       "   867,\n",
       "   885,\n",
       "   886,\n",
       "   892,\n",
       "   895,\n",
       "   896,\n",
       "   898,\n",
       "   914,\n",
       "   922,\n",
       "   936,\n",
       "   937,\n",
       "   938,\n",
       "   945,\n",
       "   947,\n",
       "   948,\n",
       "   949,\n",
       "   959,\n",
       "   965,\n",
       "   969,\n",
       "   970,\n",
       "   973,\n",
       "   982,\n",
       "   985,\n",
       "   994,\n",
       "   997,\n",
       "   1003],\n",
       "  [6,\n",
       "   44,\n",
       "   53,\n",
       "   66,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   75,\n",
       "   76,\n",
       "   79,\n",
       "   85,\n",
       "   90,\n",
       "   95,\n",
       "   97,\n",
       "   100,\n",
       "   101,\n",
       "   109,\n",
       "   115,\n",
       "   119,\n",
       "   122,\n",
       "   127,\n",
       "   134,\n",
       "   139,\n",
       "   142,\n",
       "   147,\n",
       "   153,\n",
       "   159,\n",
       "   170,\n",
       "   186,\n",
       "   189,\n",
       "   191,\n",
       "   194,\n",
       "   203,\n",
       "   217,\n",
       "   219,\n",
       "   223,\n",
       "   226,\n",
       "   239,\n",
       "   258,\n",
       "   259,\n",
       "   261,\n",
       "   271,\n",
       "   272,\n",
       "   281,\n",
       "   283,\n",
       "   291,\n",
       "   295,\n",
       "   310,\n",
       "   316,\n",
       "   320,\n",
       "   324,\n",
       "   329,\n",
       "   340,\n",
       "   352,\n",
       "   354,\n",
       "   357,\n",
       "   366,\n",
       "   367,\n",
       "   368,\n",
       "   373,\n",
       "   393,\n",
       "   397,\n",
       "   403,\n",
       "   418,\n",
       "   458,\n",
       "   470,\n",
       "   484,\n",
       "   485,\n",
       "   489,\n",
       "   491,\n",
       "   504,\n",
       "   506,\n",
       "   510,\n",
       "   511,\n",
       "   533,\n",
       "   560,\n",
       "   565,\n",
       "   575,\n",
       "   579,\n",
       "   596,\n",
       "   610,\n",
       "   613,\n",
       "   641,\n",
       "   644,\n",
       "   665,\n",
       "   666,\n",
       "   669,\n",
       "   672,\n",
       "   674,\n",
       "   685,\n",
       "   689,\n",
       "   690,\n",
       "   699,\n",
       "   716,\n",
       "   723,\n",
       "   731,\n",
       "   732,\n",
       "   736,\n",
       "   739,\n",
       "   747,\n",
       "   749,\n",
       "   754,\n",
       "   762,\n",
       "   768,\n",
       "   780,\n",
       "   781,\n",
       "   788,\n",
       "   807,\n",
       "   815,\n",
       "   821,\n",
       "   864,\n",
       "   867,\n",
       "   873,\n",
       "   874,\n",
       "   877,\n",
       "   891,\n",
       "   892,\n",
       "   893,\n",
       "   896,\n",
       "   902,\n",
       "   907,\n",
       "   909,\n",
       "   913,\n",
       "   916,\n",
       "   924,\n",
       "   928,\n",
       "   933,\n",
       "   936,\n",
       "   952,\n",
       "   961,\n",
       "   964,\n",
       "   965,\n",
       "   968,\n",
       "   992,\n",
       "   993,\n",
       "   996,\n",
       "   1011]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (selection) , len(inputs.input_ids), selection[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then apply these indices to each row in input_ids, assigning each value at these indices a value of 103."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001,\n",
       "        25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001,\n",
       "        25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001,\n",
       "        25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001,\n",
       "        25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001,\n",
       "        25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001,\n",
       "        25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001,\n",
       "        25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001,\n",
       "        25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001,\n",
       "        25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001,\n",
       "        25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001,\n",
       "        25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001,\n",
       "        25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001,\n",
       "        25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001,\n",
       "        25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 25001, 16705,\n",
       "           43,   886, 19096,    63,    62,    28])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[0][selection[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_labels = []\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    masked_labels.append(inputs.input_ids[i, selection[i]])\n",
    "    inputs.input_ids[i, selection[i]] = 103\n",
    "# masked_labels[0]\n",
    "inputs[\"mask_arr\"] = mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'next_sentence_label', 'labels', 'mask_arr'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inputs` tensors are now ready, and we can begin building the model input pipeline for training. We first create a PyTorch dataset from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeditationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our data using the `MeditationDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MeditationsDataset(inputs)\n",
    "# print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 80000) range(80000, 100000)\n"
     ]
    }
   ],
   "source": [
    "print( range(len(train_text)), range(len(train_text) , len(dataset)  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 20000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data_portion =  len(train_text)/(len(train_text) + len( test_text) )\n",
    "# print(train_data_portion ,(len(train_text) + len( test_text) ))\n",
    "\n",
    "# train_size = int(train_data_portion * len(dataset))\n",
    "# validation_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset  = torch.utils.data.Subset(dataset, range(len(train_text)))\n",
    "validation_dataset = torch.utils.data.Subset(dataset, range(len(train_text) , len(dataset)))\n",
    "\n",
    "len(train_dataset) , len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And initialize the dataloader, which we'll be using to load our data into the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader      = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto setting up the training loop. First we setup GPU/CPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the training mode of our model, and initialize our optimizer (Adam with weighted decay - reduces chance of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto the training loop, we'll train for a couple of epochs (change `epochs` to modify this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odict_keys(['loss', 'prediction_logits', 'seq_relationship_logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import *\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graph(training_data, validation_data , label ):\n",
    "\n",
    "    font_size = 10\n",
    "    x_labels = [ i for i in range(len(training_data)) ]\n",
    "\n",
    "    plt.ylabel(' F1 ',fontsize=font_size)\n",
    "    plt.plot(x_labels, training_data , 'r') \n",
    "    plt.plot(x_labels, validation_data , 'b') \n",
    "    plt.xlabel(\"Epoch\", fontsize=font_size)\n",
    "    plt.title(label,fontsize=font_size)\n",
    "    plt.legend(['Training', 'Validation'], loc='upper left') \n",
    "    \n",
    "    plt.savefig('./../../results/'+EXPERIMENT_NAME+label+'.pdf')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                 | 0/20000 [00:00<?, ?it/s]/tmp/ipykernel_735857/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|█████████████| 20000/20000 [8:35:29<00:00,  1.55s/it, loss=0.0867]\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    Masked Token f1 0.9295308155917333     SEQ F1 0.9902399645080532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/5000 [00:00<?, ?it/s]/tmp/ipykernel_735857/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|███████████████████████████████████████| 5000/5000 [45:44<00:00,  1.82it/s]\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:     v_masked_token_ F1:  0.9383772951742592  V SEQ F1:  0.9915701571875017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAAHECAYAAAAtRr6vAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABBqElEQVR4nO3deVyVZeL///fhsB0QcCEWVyxHwMwQMVxyqxRMHbWasMzUynJcyTYdxzLLqMxyppRS0zQrzdI+1lhmpj1UdFCKJGWwRcMMhjQHXBIQru8f/ji/jtwiGovo6/l4nMeDc93XfV8L0nl339d9H5sxxggAAAAu3Gq7AwAAABcjQhIAAIAFQhIAAIAFQhIAAIAFQhIAAIAFQhIAAIAFQhIAAIAFQhIAAIAFQhIAAIAFQhJwCbLZbPrggw+q9JhhYWGaM2dOlR6zJo59MXjjjTdUv3595/vp06crKirqDx2zKo4BoGKEJKAGjBgxQjabTaNHjy63bcyYMbLZbBoxYkTNd6yKhIWFyWaznfXVs2fP2u5iOdOnT3f2z263q1mzZrrvvvv0yy+/VHvbDz/8sDZs2FDp+lah93yPcaG++uor9e/fX0FBQfL29lZYWJgSEhJ06NCham8bqG2EJKCGNGvWTMuXL9dvv/3mLDt58qTeeecdNW/evBZ79sft2LFDOTk5ysnJ0fvvvy9JysrKcpatWrWqlnto7eqrr1ZOTo6ys7OVnJysDz/8UHfffbdl3ZKSEpWWllZJu/Xq1VOjRo1q/RjnkpeXp5tuukmBgYFat26dMjMztWjRIoWGhurEiRPV1m5xcXG1HRs4H4QkoIZER0erefPmLoFh1apVatasmdq3b+9S95NPPtH111+v+vXrq1GjRurfv7++//575/aioiKNGzdOoaGhzv+7T0pKOmvbM2bMUHBwsNLT0yVJKSkp6t69uxwOh5o1a6YJEybo+PHjzvp5eXkaMGCAHA6HWrZsqbfeeqvCsV1xxRUKCQlRSEiIGjZsKEkKCgpylm3cuFFXX321vLy8FBYWptmzZ1d4vMWLFysgIEDr16+XJO3Zs0c333yz6tWrp+DgYA0bNszlTEbPnj01YcIEPfroo2rYsKFCQkI0ffr0CtuQJHd3d4WEhKhJkybq37+/JkyYoE8//VS//fab8xLZRx99pDZt2sjLy0s//vijioqK9Oijj6pJkyby9fVVbGysNm3a5HLcN954Q82bN5ePj48GDx6sw4cPu2y3ulS2aNEi5xyFhoZq3Lhxkk6fpZOkwYMHy2azOd+feYzS0lLNmDFDTZs2lZeXl6KiovTJJ584t+/fv182m02rVq1Sr1695OPjo2uvvVbbtm076/ykpKSooKBACxcuVPv27dWyZUvdcMMNmjNnjkuw3717t/r16yd/f3/5+fmpW7duzn+vle3Xu+++q549e8rb21vLli2TdPrfQWRkpLy9vRUREaF58+Y59zvfvwHgQhCSgBo0cuRILV682Pl+0aJFuueee8rVO378uCZNmqQdO3Zow4YNcnNz0+DBg51nMv75z39qzZo1evfdd5WVlaVly5Y5Pzx/zxijiRMn6vXXX9eWLVsUFRWljIwMxcXF6ZZbbtGuXbu0YsUKbdmyxfmhLJ2+PLh//359/vnneu+99zRv3jzl5eVd0JjT0tJ0++23a8iQIcrIyND06dM1bdo0vfHGG5b1X3jhBT388MNat26devfurZycHPXo0UNRUVHauXOnPvnkE/33v//V7bff7rLfkiVL5Ovrq3//+996/vnnNWPGDGfIqiyHw6HS0lKdOnVKknTixAklJSVp4cKF2r17t4KCgjRy5Eht3bpVy5cv165du/SXv/xF8fHx+vbbbyVJ//73v3XPPfdozJgxSk9PV69evfT0009X2G5ycrLGjh2r+++/XxkZGVqzZo1atWol6fRZOul0YMjJyXG+P9M//vEPzZ49Wy+88IJ27dqluLg4/fnPf3b2q8zUqVP18MMPKz09Xa1bt9Ydd9zhHO+ZQkJCdOrUKa1evVrGGMs6Bw8eVPfu3eXt7a3PP/9caWlpuueee5zHrGy/HnvsMU2YMEGZmZmKi4vTggULNHXqVM2cOVOZmZl65plnNG3aNC1ZskRS5f8GgD/EAKh2w4cPNwMHDjS//PKL8fLyMvv27TP79+833t7e5pdffjEDBw40w4cPP+v+eXl5RpLJyMgwxhgzfvx4c8MNN5jS0lLL+pLMypUrzV133WUiIiLMgQMHnNuGDRtm7r//fpf6mzdvNm5ubua3334zWVlZRpLZvn27c3tmZqaRZF566aVzjnXjxo1Gkjly5Igxxpg777zT9O7d26XOI488Ytq0aeN836JFC/PSSy+ZyZMnm9DQULNr1y7ntmnTppk+ffq47H/gwAEjyWRlZRljjOnRo4e5/vrrXep07NjRPPbYY2ft5xNPPGGuvfZalzG2atXKXHfddcYYYxYvXmwkmfT0dGed7777zthsNnPw4EGXY914441mypQpxhhj7rjjDhMfH++yPSEhwQQEBJy17caNG5upU6eeta+SzOrVqyvsf+PGjc3MmTNd6nTs2NGMGTPGGGPMvn37jCSzcOFC5/bdu3cbSSYzM/Osbf/tb38z7u7upmHDhiY+Pt48//zzJjc317l9ypQppmXLlqaoqMhy/8r2a86cOS51mjVrZt5++22Xsqeeesp07tzZGHPuvwGgKnAmCahBgYGB6tevn5YsWaLFixerX79+CgwMLFfv+++/15133qkrr7xS/v7+atmypSQpOztb0ukzPenp6QoPD3deIjrTgw8+qG3btmnz5s1q2rSpszwtLU1vvPGG6tWr53zFxcWptLRU+/btU2Zmptzd3RUTE+PcJyIiwuXurPORmZmprl27upR17dpV3377rUpKSpxls2fP1muvvaYtW7bommuucenvxo0bXfobERHhnKcy7dq1c2kjNDT0nGe/MjIyVK9ePTkcDrVp00bNmjVzubTo6enpctwvv/xSxhi1bt3apT9ffPGFsy+ZmZnq3LmzSztnvv+9vLw8/fzzz7rxxhsr7GtFCgoK9PPPP1vOc2ZmpkvZ78cTGhrq7MPZzJw5U7m5uXr11VfVpk0bvfrqq4qIiFBGRoYkKT09Xd26dZOHh8cf6tfv/7398ssvOnDggO69916XeX766aed81yZvwHgj3Kv7Q4Al5t77rnHeWlr7ty5lnUGDBigZs2aacGCBWrcuLFKS0vVtm1bFRUVSTq9vmnfvn36+OOP9dlnn+n222/XTTfdpPfee895jN69e+udd97RunXrNHToUGd5aWmpHnjgAU2YMKFcu82bN1dWVpak03dUVQVjTLljGYtLN926ddO//vUvvfvuu5o8ebJLfwcMGKDnnnuu3D5lH/KSyn1I22y2cy60Dg8P15o1a2S329W4cWN5eXm5bHc4HC59Ly0tld1uV1pamux2u0vdevXqnXVsFXE4HOdVvyJW83xm2e/nqWzbueapUaNG+stf/qK//OUvSkpKUvv27fXCCy9oyZIllep/Zfrl6+vr/LmsPwsWLFBsbKxLvbJ5r8zfAPBHEZKAGhYfH+8MO3FxceW2Hz58WJmZmXrttdfUrVs3SdKWLVvK1fP391dCQoISEhJ02223KT4+Xr/++qtz4fSf//xnDRgwQHfeeafsdruGDBki6fSHy+7du51rXs4UGRmpU6dOaefOnbruuusknb5T7X//+98FjbdNmzbl+p+SkqLWrVu7BI3rrrtO48ePV1xcnOx2ux555BFnf99//32FhYXJ3b1q/5Pl6el51nmw0r59e5WUlCgvL8/5uzlTmzZttH37dpeyM9//np+fn8LCwrRhwwb16tXLso6Hh4fLWbcz+fv7q3HjxtqyZYu6d+/uLE9JSXH+DquKp6enrrrqKudC/3bt2mnJkiUqLi4uF1QvtF/BwcFq0qSJfvjhB5eAf6Zz/Q0AfxQhCahhdrvdeanhzLMRktSgQQM1atRI8+fPV2hoqLKzs13OrEjSSy+9pNDQUEVFRcnNzU0rV65USEhIuUtigwcP1ptvvqlhw4bJ3d1dt912mx577DF16tRJY8eO1ahRo+Tr66vMzEytX79eL7/8ssLDwxUfH69Ro0Zp/vz5cnd3V2Ji4gWf8XjooYfUsWNHPfXUU0pISNC2bdv0yiuvuNypVKZz5876+OOPFR8fL3d3dz344IMaO3asFixYoDvuuEOPPPKIAgMD9d1332n58uVasGCB5RxWl9atW2vo0KG6++67NXv2bLVv316HDh3S559/rmuuuUY333yzJkyYoC5duuj555/XoEGD9Omnn7rczWVl+vTpGj16tIKCgtS3b18dPXpUW7du1fjx4yXJGaK6du0qLy8vNWjQoNwxHnnkET3xxBO66qqrFBUVpcWLFys9Pf2cdyZW5KOPPtLy5cs1ZMgQtW7dWsYYffjhh1q7dq3zBoRx48bp5Zdf1pAhQzRlyhQFBARo+/btuu666xQeHn7B/Zo+fbomTJggf39/9e3bV4WFhdq5c6eOHDmiSZMmVfpvAPhDanE9FHDZKFu4fTZnLtxev369iYyMNF5eXqZdu3Zm06ZNLot358+fb6Kiooyvr6/x9/c3N954o/nyyy+d++uMhb4rVqww3t7e5v333zfGGJOammp69+5t6tWrZ3x9fU27du1cFtfm5OSYfv36GS8vL9O8eXOzdOlS5+Lqczlz4bYxxrz33numTZs2xsPDwzRv3tzMmjXLZZ8zj/3FF18YX19f849//MMYY8zevXvN4MGDTf369Y3D4TAREREmMTHRuWi3R48eZuLEiRXO6ZnOXPh8psWLF7ssti5TVFRkHn/8cRMWFmY8PDxMSEiIGTx4sMti89dff900bdrUOBwOM2DAAPPCCy9UuHDbGGNeffVVEx4ebjw8PExoaKgZP368c9uaNWtMq1atjLu7u2nRooXlMUpKSsyTTz5pmjRpYjw8PMy1115rPv74Y+f2sgXSX331lbPsyJEjRpLZuHGj5Rx8//33ZtSoUaZ169bG4XCY+vXrm44dO5rFixe71Pv6669Nnz59jI+Pj/Hz8zPdunUz33///QX3q8xbb71loqKijKenp2nQoIHp3r27WbVqlTHm3H8DQFWwGXOeF9ABAAAuA9zdBgAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGHSV6g0tJS/fzzz/Lz86uyr28AAADVyxijo0ePqnHjxnJzq/hcESHpAv38889q1qxZbXcDAABcgAMHDrh8+bcVQtIF8vPzk3R6kv39/Wu5NwAAoDIKCgrUrFkz5+d4RQhJF6jsEpu/vz8hCQCAOqYyS2VYuA0AAGCBkAQAAGCBkAQAAGCBNUnVrKSkRMXFxbXdDVQBDw8P2e322u4GAKCGEJKqiTFGubm5+t///lfbXUEVql+/vkJCQng2FgBcBghJ1aQsIAUFBcnHx4cP1TrOGKMTJ04oLy9PkhQaGlrLPQIAVDdCUjUoKSlxBqRGjRrVdndQRRwOhyQpLy9PQUFBXHoDgEscC7erQdkaJB8fn1ruCapa2e+UdWYAcOkjJFUjLrFdevidAsDlg5AEAABggZCEatezZ08lJiZWuv7+/ftls9mUnp5ebX0CAOBcWLgNp3NdSho+fLjeeOON8z7uqlWr5OHhUen6zZo1U05OjgIDA8+7LQAAqgohCU45OTnOn1esWKHHH39cWVlZzrKyu7vKFBcXVyr8NGzY8Lz6YbfbFRIScl77AABQ1bjcBqeQkBDnKyAgQDabzfn+5MmTql+/vt5991317NlT3t7eWrZsmQ4fPqw77rhDTZs2lY+Pj6655hq98847Lsc983JbWFiYnnnmGd1zzz3y8/NT8+bNNX/+fOf2My+3bdq0STabTRs2bFBMTIx8fHzUpUsXlwAnSU8//bSCgoLk5+en++67T5MnT1ZUVFR1TRcA4BJHSKopxkjHj9f8y5gqHcZjjz2mCRMmKDMzU3FxcTp58qQ6dOigjz76SN98843uv/9+DRs2TP/+978rPM7s2bMVExOjr776SmPGjNFf//pX/ec//6lwn6lTp2r27NnauXOn3N3ddc899zi3vfXWW5o5c6aee+45paWlqXnz5kpOTq6SMQMALk9cbqspJ05I9erVfLvHjkm+vlV2uMTERN1yyy0uZQ8//LDz5/Hjx+uTTz7RypUrFRsbe9bj3HzzzRozZoyk08HrpZde0qZNmxQREXHWfWbOnKkePXpIkiZPnqx+/frp5MmT8vb21ssvv6x7771XI0eOlCQ9/vjj+vTTT3Xs2LELHisA4PLGmSScl5iYGJf3JSUlmjlzptq1a6dGjRqpXr16+vTTT5WdnV3hcdq1a+f8ueyyXtlXflRmn7KvBSnbJysrS9ddd51L/TPfAwBwPjiTVFN8fE6f1amNdquQ7xlnpWbPnq2XXnpJc+bM0TXXXCNfX18lJiaqqKiowuOcueDbZrOptLS00vuU3Yn3+33OvDvPVPGlRgDA5YWQVFNstiq97HWx2Lx5swYOHKi77rpL0unQ8u233yoyMrJG+xEeHq7U1FQNGzbMWbZz584a7QMA4NLC5Tb8Ia1atdL69euVkpKizMxMPfDAA8rNza3xfowfP16vv/66lixZom+//VZPP/20du3axdeIAAAuGGeS8IdMmzZN+/btU1xcnHx8fHT//fdr0KBBys/Pr9F+DB06VD/88IMefvhhnTx5UrfffrtGjBih1NTUGu0HAODSYTMs3LggBQUFCggIUH5+vvz9/V22nTx5Uvv27VPLli3l7e1dSz1E7969FRISojfffLPKjsnvFgDqtoo+v8/EmSRcEk6cOKFXX31VcXFxstvteuedd/TZZ59p/fr1td01AEAdRUjCJcFms2nt2rV6+umnVVhYqPDwcL3//vu66aabartrAIA6qtYXbs+bN8956aJDhw7avHlzhfXnzp2ryMhIORwOhYeHa+nSpS7bV61apZiYGNWvX1++vr6Kiooqd7nl1KlT+vvf/66WLVvK4XDoyiuv1IwZM855CzouXg6HQ5999pl+/fVXHT9+XF9++WW5h14CAHA+avVM0ooVK5SYmKh58+apa9eueu2119S3b1/t2bNHzZs3L1c/OTlZU6ZM0YIFC9SxY0elpqZq1KhRatCggQYMGCDp9JepTp06VREREfL09NRHH32kkSNHKigoSHFxcZKk5557Tq+++qqWLFmiq6++Wjt37tTIkSMVEBCgiRMn1ugcAACAi1OtLtyOjY1VdHS0y3dsRUZGatCgQUpKSipXv0uXLuratatmzZrlLEtMTNTOnTu1ZcuWs7YTHR2tfv366amnnpIk9e/fX8HBwXr99deddW699Vb5+PhUepEvC7cvT/xuAaBuO5+F27V2ua2oqEhpaWnq06ePS3mfPn2UkpJiuU9hYWG5DyaHw6HU1FQVFxeXq2+M0YYNG5SVlaXu3bs7y6+//npt2LBBe/fulSR9/fXX2rJli26++eaz9rewsFAFBQUuLwAAcOmqtctthw4dUklJiYKDg13Kg4ODz/owwri4OC1cuFCDBg1SdHS00tLStGjRIhUXF+vQoUPO7/PKz89XkyZNVFhYKLvdrnnz5ql3797O4zz22GPKz89XRESE7Ha78/vH7rjjjrP2NykpSU8++WQVjBwAANQFtX53m9X3bZ3tKcnTpk1Tbm6uOnXqJGOMgoODNWLECD3//POy2+3Oen5+fkpPT9exY8e0YcMGTZo0SVdeeaV69uwp6fRaqGXLluntt9/W1VdfrfT0dCUmJqpx48YaPny4ZdtTpkzRpEmTnO8LCgrUrFmzPzh6AABwsaq1kBQYGCi73V7urFFeXl65s0tlHA6HFi1apNdee03//e9/FRoaqvnz58vPz0+BgYHOem5ubmrVqpUkKSoqSpmZmUpKSnKGpEceeUSTJ0/WkCFDJEnXXHONfvzxRyUlJZ01JHl5ecnLy+uPDhsAANQRtbYmydPTUx06dCj3sL/169erS5cuFe7r4eGhpk2bym63a/ny5erfv7/c3M4+FGOMCgsLne9PnDhRrr7dbucRAFWgZ8+eSkxMdL4PCwvTnDlzKtzHZrPpgw8++MNtV9VxAACQavly26RJkzRs2DDFxMSoc+fOmj9/vrKzszV69GhJpy9xHTx40PkspL179yo1NVWxsbE6cuSIXnzxRX3zzTdasmSJ85hJSUmKiYnRVVddpaKiIq1du1ZLly51uYNuwIABmjlzppo3b66rr75aX331lV588UXdc889NTsBF5kBAwbot99+02effVZu27Zt29SlSxelpaUpOjq60sfcsWOHfH19q7Kbmj59uj744AOlp6e7lOfk5KhBgwZV2hYA4PJVqyEpISFBhw8f1owZM5STk6O2bdtq7dq1atGihaTTH3rZ2dnO+iUlJZo9e7aysrLk4eGhXr16KSUlRWFhYc46x48f15gxY/TTTz/J4XAoIiJCy5YtU0JCgrPOyy+/rGnTpmnMmDHKy8tT48aN9cADD+jxxx+vsbFfjO69917dcsst+vHHH52/gzKLFi1SVFTUeQUkSbriiiuqsosVCgkJqbG2AACXAYMLkp+fbySZ/Pz8ctt+++03s2fPHvPbb7/VQs8uXHFxsQkODjbTp093KT9+/Ljx8/Mz06dPN0OGDDFNmjQxDofDtG3b1rz99tsudXv06GEmTpzofN+iRQvz0ksvOd/v3bvXdOvWzXh5eZnIyEjz6aefGklm9erVzjqPPvqo+dOf/mQcDodp2bKl+fvf/26KioqMMcYsXrzYSHJ5LV682Bhjyh1n165dplevXsbb29s0bNjQjBo1yhw9etS5ffjw4WbgwIFm1qxZJiQkxDRs2NCMGTPG2ZaVuvq7BQCcVtHn95lq/e62y4Ux0okTNd+uj490lpsFy3F3d9fdd9+tN954Q48//rjzLsOVK1eqqKhI9913n9555x099thj8vf317/+9S8NGzZMV155pWJjY895/NLSUt1yyy0KDAzU9u3bVVBQ4LJ+qYyfn5/eeOMNNW7cWBkZGRo1apT8/Pz06KOPKiEhQd98840++eQT52XBgICAcsc4ceKE4uPj1alTJ+3YsUN5eXm67777NG7cOL3xxhvOehs3blRoaKg2btyo7777TgkJCYqKitKoUaMqN2kAgEsWIamGnDgh1atX8+0eOyadz5Kge+65R7NmzdKmTZvUq1cvSacvtd1yyy1q0qSJHn74YWfd8ePH65NPPtHKlSsrFZI+++wzZWZmav/+/WratKkk6ZlnnlHfvn1d6v397393/hwWFqaHHnpIK1as0KOPPiqHw6F69erJ3d29wstrb731ln777TctXbrUuSbqlVde0YABA/Tcc88576Bs0KCBXnnlFdntdkVERKhfv37asGEDIQkAQEiCq4iICHXp0kWLFi1Sr1699P3332vz5s369NNPVVJSomeffVYrVqzQwYMHVVhYqMLCwkovzM7MzFTz5s2dAUmSOnfuXK7ee++9pzlz5ui7777TsWPHdOrUqXM+Ot6qrWuvvdalb127dlVpaamysrKcIenqq692ecZWaGioMjIyzqstAMCliZBUQ3x8Tp/VqY12z9e9996rcePGae7cuVq8eLFatGihG2+8UbNmzdJLL72kOXPm6JprrpGvr68SExNVVFRUqeMai68JPPPBodu3b9eQIUP05JNPKi4uTgEBAVq+fLlmz559XmMwFTyU9PflHh4e5bbxKAgAgERIqjE22/ld9qpNt99+uyZOnKi3335bS5Ys0ahRo2Sz2bR582YNHDhQd911l6TTa4y+/fZbRUZGVuq4bdq0UXZ2tn7++Wc1btxY0ulHC/ze1q1b1aJFC02dOtVZ9uOPP7rU8fT0VElJyTnbWrJkiY4fP+48m7R161a5ubmpdevWleovAODyVmsPk8TFq169ekpISNDf/vY3/fzzzxoxYoQkqVWrVlq/fr1SUlKUmZmpBx544Kzfs2flpptuUnh4uO6++259/fXX2rx5s0sYKmsjOztby5cv1/fff69//vOfWr16tUudsLAw7du3T+np6Tp06JDLg0LLDB06VN7e3ho+fLi++eYbbdy4UePHj9ewYcPO+kR3AAB+j5AES/fee6+OHDmim266Sc2bN5d0+rvzoqOjFRcXp549eyokJESDBg2q9DHd3Ny0evVqFRYW6rrrrtN9992nmTNnutQZOHCgHnzwQY0bN05RUVFKSUnRtGnTXOrceuutio+PV69evXTFFVfonXfeKdeWj4+P1q1bp19//VUdO3bUbbfdphtvvFGvvPLK+U8GAOCyZDNWC0VwTgUFBQoICFB+fn65RcUnT57Uvn371LJlS3l7e9dSD1Ed+N0CQN1W0ef3mTiTBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQVI1YE3/p4XcKAJcPQlI1KHuK84na+EZbVKuy3+mZT+oGAFx6eOJ2NbDb7apfv77y8vIknX5mz9m+IgN1gzFGJ06cUF5enurXr+/yfW8AgEsTIamalH1DfVlQwqWhfv36zt8tAODSRkiqJjabTaGhoQoKClJxcXFtdwdVwMPDgzNIAHAZISRVM7vdzgcrAAB1EAu3AQAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALBCSAAAALNR6SJo3b55atmwpb29vdejQQZs3b66w/ty5cxUZGSmHw6Hw8HAtXbrUZfuqVasUExOj+vXry9fXV1FRUXrzzTfLHefgwYO666671KhRI/n4+CgqKkppaWlVOjYAAFB3uddm4ytWrFBiYqLmzZunrl276rXXXlPfvn21Z88eNW/evFz95ORkTZkyRQsWLFDHjh2VmpqqUaNGqUGDBhowYIAkqWHDhpo6daoiIiLk6empjz76SCNHjlRQUJDi4uIkSUeOHFHXrl3Vq1cvffzxxwoKCtL333+v+vXr1+TwAQDARcxmjDG11XhsbKyio6OVnJzsLIuMjNSgQYOUlJRUrn6XLl3UtWtXzZo1y1mWmJionTt3asuWLWdtJzo6Wv369dNTTz0lSZo8ebK2bt16zrNWFSkoKFBAQIDy8/Pl7+9/wccBAAA153w+v2vtcltRUZHS0tLUp08fl/I+ffooJSXFcp/CwkJ5e3u7lDkcDqWmpqq4uLhcfWOMNmzYoKysLHXv3t1ZvmbNGsXExOgvf/mLgoKC1L59ey1YsKDC/hYWFqqgoMDlBQAALl21FpIOHTqkkpISBQcHu5QHBwcrNzfXcp+4uDgtXLhQaWlpMsZo586dWrRokYqLi3Xo0CFnvfz8fNWrV0+enp7q16+fXn75ZfXu3du5/YcfflBycrL+9Kc/ad26dRo9erQmTJhQbn3T7yUlJSkgIMD5atas2R+cAQAAcDGr1TVJkmSz2VzeG2PKlZWZNm2acnNz1alTJxljFBwcrBEjRuj555+X3W531vPz81N6erqOHTumDRs2aNKkSbryyivVs2dPSVJpaaliYmL0zDPPSJLat2+v3bt3Kzk5WXfffbdl21OmTNGkSZOc7wsKCghKAABcwmrtTFJgYKDsdnu5s0Z5eXnlzi6VcTgcWrRokU6cOKH9+/crOztbYWFh8vPzU2BgoLOem5ubWrVqpaioKD300EO67bbbXNY4hYaGqk2bNi7HjoyMVHZ29ln76+XlJX9/f5cXAAC4dNVaSPL09FSHDh20fv16l/L169erS5cuFe7r4eGhpk2bym63a/ny5erfv7/c3M4+FGOMCgsLne+7du2qrKwslzp79+5VixYtLmAkAADgUlSrl9smTZqkYcOGKSYmRp07d9b8+fOVnZ2t0aNHSzp9ievgwYPOtUJ79+5VamqqYmNjdeTIEb344ov65ptvtGTJEucxk5KSFBMTo6uuukpFRUVau3atli5d6nIH3YMPPqguXbromWee0e23367U1FTNnz9f8+fPr9kJAAAAF61aDUkJCQk6fPiwZsyYoZycHLVt21Zr1651ntHJyclxuQRWUlKi2bNnKysrSx4eHurVq5dSUlIUFhbmrHP8+HGNGTNGP/30kxwOhyIiIrRs2TIlJCQ463Ts2FGrV6/WlClTNGPGDLVs2VJz5szR0KFDa2zsAADg4larz0mqy3hOEgAAdU+deE4SAADAxYyQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYIGQBAAAYKHKQtLXX38tu91+3vvNmzdPLVu2lLe3tzp06KDNmzdXWH/u3LmKjIyUw+FQeHi4li5d6rJ91apViomJUf369eXr66uoqCi9+eabZz1eUlKSbDabEhMTz7vvAADg0uVelQczxpxX/RUrVigxMVHz5s1T165d9dprr6lv377as2ePmjdvXq5+cnKypkyZogULFqhjx45KTU3VqFGj1KBBAw0YMECS1LBhQ02dOlURERHy9PTURx99pJEjRyooKEhxcXEux9uxY4fmz5+vdu3aXfigAQDAJclmKplsbrnllgq35+fna9OmTSopKal047GxsYqOjlZycrKzLDIyUoMGDVJSUlK5+l26dFHXrl01a9YsZ1liYqJ27typLVu2nLWd6Oho9evXT0899ZSz7NixY4qOjta8efP09NNPKyoqSnPmzKl03wsKChQQEKD8/Hz5+/tXej8AAFB7zufzu9KX2z788EOdPHlSAQEBlq969eqdVyeLioqUlpamPn36uJT36dNHKSkplvsUFhbK29vbpczhcCg1NVXFxcXl6htjtGHDBmVlZal79+4u28aOHat+/frppptuqlR/CwsLVVBQ4PICAACXrkpfbouMjNStt96qe++913J7enq6Pvroo0o3fOjQIZWUlCg4ONilPDg4WLm5uZb7xMXFaeHChRo0aJCio6OVlpamRYsWqbi4WIcOHVJoaKik02e1mjRposLCQtntds2bN0+9e/d2Hmf58uX68ssvtWPHjkr3NykpSU8++WSl6wMAgLqt0meSOnTooC+//PKs2728vCzXEZ2LzWZzeW+MKVdWZtq0aerbt686deokDw8PDRw4UCNGjJAkl0Xjfn5+Sk9P144dOzRz5kxNmjRJmzZtkiQdOHBAEydO1LJly8qdlarIlClTlJ+f73wdOHDg/AYKAADqlEqvSSosLFRJSYl8fHyqpOGioiL5+Pho5cqVGjx4sLN84sSJSk9P1xdffHHWfYuLi/Xf//5XoaGhmj9/vh577DH973//k5ubdea77777dODAAa1bt04ffPCBBg8e7BKqSkpKZLPZ5Obm5jz7dC6sSQIAoO6pljVJXl5eVRaQJMnT01MdOnTQ+vXrXcrXr1+vLl26VLivh4eHmjZtKrvdruXLl6t///5nDUjS6bNThYWFkqQbb7xRGRkZSk9Pd75iYmI0dOhQpaenX9BjDAAAwKWn0muSHn/8cU2ePNkZlI4cOaIGDRr8ocYnTZqkYcOGKSYmRp07d9b8+fOVnZ2t0aNHSzp9ievgwYPOZyHt3btXqampio2N1ZEjR/Tiiy/qm2++0ZIlS5zHTEpKUkxMjK666ioVFRVp7dq1Wrp0qfMOOj8/P7Vt29alH76+vmrUqFG5cgAAcPmqdEiaOXOmxo0b5wxJLVq0UHp6uq688soLbjwhIUGHDx/WjBkzlJOTo7Zt22rt2rVq0aKFJCknJ0fZ2dnO+iUlJZo9e7aysrLk4eGhXr16KSUlRWFhYc46x48f15gxY/TTTz/J4XAoIiJCy5YtU0JCwgX3EwAAXH4qvSbJzc1Nubm5CgoKknT6jMzXX3/9h0JSXcaaJAAA6p5qWZMEAABwOan05TabzaajR4/K29vbeZv+sWPHyj1UkbMqAADgUlDpkGSMUevWrV3et2/f3uW9zWY7r68lAQAAuFhVOiRt3LixOvsBAABwUal0SOrRo0d19gMAAOCiwsJtAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC4QkAAAAC7UekubNm6eWLVvK29tbHTp00ObNmyusP3fuXEVGRsrhcCg8PFxLly512b5q1SrFxMSofv368vX1VVRUlN58802XOklJSerYsaP8/PwUFBSkQYMGKSsrq8rHBgAA6q5aDUkrVqxQYmKipk6dqq+++krdunVT3759lZ2dbVk/OTlZU6ZM0fTp07V79249+eSTGjt2rD788ENnnYYNG2rq1Knatm2bdu3apZEjR2rkyJFat26ds84XX3yhsWPHavv27Vq/fr1OnTqlPn366Pjx49U+ZgAAUDfYjDGmthqPjY1VdHS0kpOTnWWRkZEaNGiQkpKSytXv0qWLunbtqlmzZjnLEhMTtXPnTm3ZsuWs7URHR6tfv3566qmnLLf/8ssvCgoK0hdffKHu3btXqu8FBQUKCAhQfn6+/P39K7UPAACoXefz+V1rZ5KKioqUlpamPn36uJT36dNHKSkplvsUFhbK29vbpczhcCg1NVXFxcXl6htjtGHDBmVlZVUYfvLz8yWdPgt1NoWFhSooKHB5AQCAS1ethaRDhw6ppKREwcHBLuXBwcHKzc213CcuLk4LFy5UWlqajDHauXOnFi1apOLiYh06dMhZLz8/X/Xq1ZOnp6f69eunl19+Wb1797Y8pjFGkyZN0vXXX6+2bduetb9JSUkKCAhwvpo1a3YBowYAAHVFrS/cttlsLu+NMeXKykybNk19+/ZVp06d5OHhoYEDB2rEiBGSJLvd7qzn5+en9PR07dixQzNnztSkSZO0adMmy2OOGzdOu3bt0jvvvFNhP6dMmaL8/Hzn68CBA5UfJAAAqHNqLSQFBgbKbreXO2uUl5dX7uxSGYfDoUWLFunEiRPav3+/srOzFRYWJj8/PwUGBjrrubm5qVWrVoqKitJDDz2k2267zXKN0/jx47VmzRpt3LhRTZs2rbC/Xl5e8vf3d3kBAIBLV62FJE9PT3Xo0EHr1693KV+/fr26dOlS4b4eHh5q2rSp7Ha7li9frv79+8vN7exDMcaosLDQ5f24ceO0atUqff7552rZsuUfGwwAALjkuNdm45MmTdKwYcMUExOjzp07a/78+crOztbo0aMlnb7EdfDgQeezkPbu3avU1FTFxsbqyJEjevHFF/XNN99oyZIlzmMmJSUpJiZGV111lYqKirR27VotXbrU5Q66sWPH6u2339b//d//yc/Pz3k2KyAgQA6HowZnAAAAXKxqNSQlJCTo8OHDmjFjhnJyctS2bVutXbtWLVq0kCTl5OS4PDOppKREs2fPVlZWljw8PNSrVy+lpKQoLCzMWef48eMaM2aMfvrpJzkcDkVERGjZsmVKSEhw1ikLTD179nTpz+LFi51rnAAAwOWtVp+TVJfxnCQAAOqeOvGcJAAAgIsZIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMACIQkAAMBCrYekefPmqWXLlvL29laHDh20efPmCuvPnTtXkZGRcjgcCg8P19KlS122r1q1SjExMapfv758fX0VFRWlN9988w+3CwAALi+1GpJWrFihxMRETZ06VV999ZW6deumvn37Kjs727J+cnKypkyZounTp2v37t168sknNXbsWH344YfOOg0bNtTUqVO1bds27dq1SyNHjtTIkSO1bt26C24XAABcfmzGGFNbjcfGxio6OlrJycnOssjISA0aNEhJSUnl6nfp0kVdu3bVrFmznGWJiYnauXOntmzZctZ2oqOj1a9fPz311FMX1K6VgoICBQQEKD8/X/7+/pXaBwAA1K7z+fyutTNJRUVFSktLU58+fVzK+/Tpo5SUFMt9CgsL5e3t7VLmcDiUmpqq4uLicvWNMdqwYYOysrLUvXv3C263rO2CggKXFwAAuHTVWkg6dOiQSkpKFBwc7FIeHBys3Nxcy33i4uK0cOFCpaWlyRijnTt3atGiRSouLtahQ4ec9fLz81WvXj15enqqX79+evnll9W7d+8LbleSkpKSFBAQ4Hw1a9bsQocOAADqgFpfuG2z2VzeG2PKlZWZNm2a+vbtq06dOsnDw0MDBw7UiBEjJEl2u91Zz8/PT+np6dqxY4dmzpypSZMmadOmTRfcriRNmTJF+fn5zteBAwfOY5QAAKCuqbWQFBgYKLvdXu7sTV5eXrmzPGUcDocWLVqkEydOaP/+/crOzlZYWJj8/PwUGBjorOfm5qZWrVopKipKDz30kG677TbnWqMLaVeSvLy85O/v7/ICAACXrloLSZ6enurQoYPWr1/vUr5+/Xp16dKlwn09PDzUtGlT2e12LV++XP3795eb29mHYoxRYWHhH24XAABcPtxrs/FJkyZp2LBhiomJUefOnTV//nxlZ2dr9OjRkk5f4jp48KDzWUh79+5VamqqYmNjdeTIEb344ov65ptvtGTJEucxk5KSFBMTo6uuukpFRUVau3atli5d6nIn27naBQAAqNWQlJCQoMOHD2vGjBnKyclR27ZttXbtWrVo0UKSlJOT4/LsopKSEs2ePVtZWVny8PBQr169lJKSorCwMGed48ePa8yYMfrpp5/kcDgUERGhZcuWKSEhodLtAgAA1OpzkuoynpMEAEDdUyeekwQAAHAxIyQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYICQBAABYcK/tDtRVxhhJUkFBQS33BAAAVFbZ53bZ53hFCEkX6OjRo5KkZs2a1XJPAADA+Tp69KgCAgIqrGMzlYlSKKe0tFQ///yz/Pz8ZLPZars7ta6goEDNmjXTgQMH5O/vX9vduWQxzzWDea4ZzHPNYa7/f8YYHT16VI0bN5abW8WrjjiTdIHc3NzUtGnT2u7GRcff3/+y/wOsCcxzzWCeawbzXHOY69POdQapDAu3AQAALBCSAAAALBCSUCW8vLz0xBNPyMvLq7a7ckljnmsG81wzmOeaw1xfGBZuAwAAWOBMEgAAgAVCEgAAgAVCEgAAgAVCEgAAgAVCEirlyJEjGjZsmAICAhQQEKBhw4bpf//7X4X7GGM0ffp0NW7cWA6HQz179tTu3bvPWrdv376y2Wz64IMPqn4AdUR1zPOvv/6q8ePHKzw8XD4+PmrevLkmTJig/Pz8ah7NxWXevHlq2bKlvL291aFDB23evLnC+l988YU6dOggb29vXXnllXr11VfL1Xn//ffVpk0beXl5qU2bNlq9enV1db/OqOp5XrBggbp166YGDRqoQYMGuummm5SamlqdQ6gTquPfc5nly5fLZrNp0KBBVdzrOsgAlRAfH2/atm1rUlJSTEpKimnbtq3p379/hfs8++yzxs/Pz7z//vsmIyPDJCQkmNDQUFNQUFCu7osvvmj69u1rJJnVq1dX0yguftUxzxkZGeaWW24xa9asMd99953ZsGGD+dOf/mRuvfXWmhjSRWH58uXGw8PDLFiwwOzZs8dMnDjR+Pr6mh9//NGy/g8//GB8fHzMxIkTzZ49e8yCBQuMh4eHee+995x1UlJSjN1uN88884zJzMw0zzzzjHF3dzfbt2+vqWFddKpjnu+8804zd+5c89VXX5nMzEwzcuRIExAQYH766aeaGtZFpzrmucz+/ftNkyZNTLdu3czAgQOreSQXP0ISzmnPnj1Gkst//Ldt22Ykmf/85z+W+5SWlpqQkBDz7LPPOstOnjxpAgICzKuvvupSNz093TRt2tTk5ORc1iGpuuf59959913j6elpiouLq24AF7HrrrvOjB492qUsIiLCTJ482bL+o48+aiIiIlzKHnjgAdOpUyfn+9tvv93Ex8e71ImLizNDhgypol7XPdUxz2c6deqU8fPzM0uWLPnjHa6jqmueT506Zbp27WoWLlxohg8fTkgyxnC5Dee0bds2BQQEKDY21lnWqVMnBQQEKCUlxXKfffv2KTc3V3369HGWeXl5qUePHi77nDhxQnfccYdeeeUVhYSEVN8g6oDqnOcz5efny9/fX+7ul/7XNxYVFSktLc1ljiSpT58+Z52jbdu2lasfFxennTt3qri4uMI6Fc37pay65vlMJ06cUHFxsRo2bFg1Ha9jqnOeZ8yYoSuuuEL33ntv1Xe8jiIk4Zxyc3MVFBRUrjwoKEi5ubln3UeSgoODXcqDg4Nd9nnwwQfVpUsXDRw4sAp7XDdV5zz/3uHDh/XUU0/pgQce+IM9rhsOHTqkkpKS85qj3Nxcy/qnTp3SoUOHKqxztmNe6qprns80efJkNWnSRDfddFPVdLyOqa553rp1q15//XUtWLCgejpeRxGSLmPTp0+XzWar8LVz505Jks1mK7e/Mcay/PfO3P77fdasWaPPP/9cc+bMqZoBXaRqe55/r6CgQP369VObNm30xBNP/IFR1T2VnaOK6p9Zfr7HvBxUxzyXef755/XOO+9o1apV8vb2roLe1l1VOc9Hjx7VXXfdpQULFigwMLDqO1uHXfrn2nFW48aN05AhQyqsExYWpl27dum///1vuW2//PJLuf87KVN26Sw3N1ehoaHO8ry8POc+n3/+ub7//nvVr1/fZd9bb71V3bp106ZNm85jNBev2p7nMkePHlV8fLzq1aun1atXy8PD43yHUicFBgbKbreX+79sqzkqExISYlnf3d1djRo1qrDO2Y55qauueS7zwgsv6JlnntFnn32mdu3aVW3n65DqmOfdu3dr//79GjBggHN7aWmpJMnd3V1ZWVm66qqrqngkdUQtrYVCHVK2oPjf//63s2z79u2VWlD83HPPOcsKCwtdFhTn5OSYjIwMl5ck849//MP88MMP1Tuoi1B1zbMxxuTn55tOnTqZHj16mOPHj1ffIC5S1113nfnrX//qUhYZGVnhQtfIyEiXstGjR5dbuN23b1+XOvHx8Zf9wu2qnmdjjHn++eeNv7+/2bZtW9V2uI6q6nn+7bffyv23eODAgeaGG24wGRkZprCwsHoGUgcQklAp8fHxpl27dmbbtm1m27Zt5pprril3a3p4eLhZtWqV8/2zzz5rAgICzKpVq0xGRoa54447zvoIgDK6jO9uM6Z65rmgoMDExsaaa665xnz33XcmJyfH+Tp16lSNjq+2lN0y/frrr5s9e/aYxMRE4+vra/bv32+MMWby5Mlm2LBhzvplt0w/+OCDZs+ePeb1118vd8v01q1bjd1uN88++6zJzMw0zz77LI8AqIZ5fu6554ynp6d57733XP7tHj16tMbHd7Gojnk+E3e3nUZIQqUcPnzYDB061Pj5+Rk/Pz8zdOhQc+TIEZc6kszixYud70tLS80TTzxhQkJCjJeXl+nevbvJyMiosJ3LPSRVxzxv3LjRSLJ87du3r2YGdhGYO3euadGihfH09DTR0dHmiy++cG4bPny46dGjh0v9TZs2mfbt2xtPT08TFhZmkpOTyx1z5cqVJjw83Hh4eJiIiAjz/vvvV/cwLnpVPc8tWrSw/Lf7xBNP1MBoLl7V8e/59whJp9mM+f9WbwEAAMCJu9sAAAAsEJIAAAAsEJIAAAAsEJIAAAAsEJIAAAAsEJIAAAAsEJIAAAAsEJIAoIrYbDZ98MEHtd0NAFWEkATgkjBixAjZbLZyr/j4+NruGoA6yr22OwAAVSU+Pl6LFy92KfPy8qql3gCo6ziTBOCS4eXlpZCQEJdXgwYNJJ2+FJacnKy+ffvK4XCoZcuWWrlypcv+GRkZuuGGG+RwONSoUSPdf//9OnbsmEudRYsW6eqrr5aXl5dCQ0M1btw4l+2HDh3S4MGD5ePjoz/96U9as2ZN9Q4aQLUhJAG4bEybNk233nqrvv76a91111264447lJmZKUk6ceKE4uPj1aBBA+3YsUMrV67UZ5995hKCkpOTNXbsWN1///3KyMjQmjVr1KpVK5c2nnzySd1+++3atWuXbr75Zg0dOlS//vprjY4TQBWp7W/YBYCqMHz4cGO3242vr6/La8aMGcYYYySZ0aNHu+wTGxtr/vrXvxpjjJk/f75p0KCBOXbsmHP7v/71L+Pm5mZyc3ONMcY0btzYTJ069ax9kGT+/ve/O98fO3bM2Gw28/HHH1fZOAHUHNYkAbhk9OrVS8nJyS5lDRs2dP7cuXNnl22dO3dWenq6JCkzM1PXXnutfH19ndu7du2q0tJSZWVlyWaz6eeff9aNN95YYR/atWvn/NnX11d+fn7Ky8u70CEBqEWEJACXDF9f33KXv87FZrNJkowxzp+t6jgcjkodz8PDo9y+paWl59UnABcH1iQBuGxs37693PuIiAhJUps2bZSenq7jx487t2/dulVubm5q3bq1/Pz8FBYWpg0bNtRonwHUHs4kAbhkFBYWKjc316XM3d1dgYGBkqSVK1cqJiZG119/vd566y2lpqbq9ddflyQNHTpUTzzxhIYPH67p06frl19+0fjx4zVs2DAFBwdLkqZPn67Ro0crKChIffv21dGjR7V161aNHz++ZgcKoEYQkgBcMj755BOFhoa6lIWHh+s///mPpNN3ni1fvlxjxoxRSEiI3nrrLbVp00aS5OPjo3Xr1mnixInq2LGjfHx8dOutt+rFF190Hmv48OE6efKkXnrpJT388MMKDAzUbbfdVnMDBFCjbMYYU9udAIDqZrPZtHr1ag0aNKi2uwKgjmBNEgAAgAVCEgAAgAXWJAG4LLCyAMD54kwSAACABUISAACABUISAACABUISAACABUISAACABUISAACABUISAACABUISAACABUISAACAhf8HtDCXlc+UIEcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-17 14:25:28,732] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/raisul/anaconda3/envs/pytorch/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "  0%|                                                 | 0/20000 [00:00<?, ?it/s]/tmp/ipykernel_735857/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 1:  65%|███████▏   | 13086/20000 [5:33:51<2:56:23,  1.53s/it, loss=0.0572]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Now, you can update the model's weights using the optimizer\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Zero gradients after updating the model's weights\u001b[39;00m\n\u001b[1;32m    102\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/optimization.py:647\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[1;32m    646\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[0;32m--> 647\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    650\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "from itertools import chain\n",
    "\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "epochs =100\n",
    "counter = 0\n",
    "\n",
    "global_instruction_metrices = []\n",
    "global_masked_token_metrices = []\n",
    "\n",
    "v_global_instruction_metrices = []\n",
    "v_global_masked_token_metrices = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    train_loop = tqdm(train_loader, leave=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    masked_token_predictions_all, masked_token_ground_truths_all = None, None\n",
    "    seq_predictions_all, seq_ground_truths_all = None, None\n",
    "    \n",
    "    # activate training mode\n",
    "    model.train()\n",
    "    for N,batch in enumerate(train_loop):\n",
    "\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        # token_type_ids = batch['token_type_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        batch_mask_arr = batch ['mask_arr']\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids ,attention_mask = attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        token_prediction = torch.argmax(logits, axis=-1)\n",
    "\n",
    "        \n",
    "        batch_masks =   [ torch.flatten(bm.nonzero()).tolist()  for bm in batch_mask_arr]    # torch.flatten(batch ['mask_arr'].nonzero()).tolist()\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        masked_token_prediction = [ token[batch_masks[t]].tolist() for t,token in enumerate(token_prediction) ]\n",
    "        masked_token_prediction = list(chain.from_iterable(masked_token_prediction))\n",
    "        \n",
    "        masked_token_ground_truth   = [ token[batch_masks[t]].tolist() for t,token in enumerate(labels) ]\n",
    "        masked_token_ground_truth = list(chain.from_iterable(masked_token_ground_truth))\n",
    "        \n",
    "\n",
    "\n",
    "        seq_predictions   = token_prediction.detach().cpu().numpy().flatten()\n",
    "        seq_ground_truths = labels.detach().cpu().numpy().flatten()\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        if N==0:\n",
    "\n",
    "            \n",
    "            masked_token_predictions_all         = masked_token_prediction\n",
    "            masked_token_ground_truths_all       = masked_token_ground_truth  \n",
    "\n",
    "\n",
    "            seq_predictions_all = seq_predictions\n",
    "            seq_ground_truths_all = seq_ground_truths\n",
    "            \n",
    "        else:\n",
    "\n",
    "            masked_token_predictions_all   = np.concatenate((masked_token_predictions_all, masked_token_prediction))\n",
    "            masked_token_ground_truths_all = np.concatenate((masked_token_ground_truths_all, masked_token_ground_truth))\n",
    "\n",
    "            seq_predictions_all = np.concatenate((seq_predictions_all, seq_predictions))\n",
    "            seq_ground_truths_all = np.concatenate((seq_ground_truths_all, seq_ground_truths))\n",
    "\n",
    "\n",
    "                # Compute loss\n",
    "        logits = logits.view(-1, logits.size(-1))  # [batch_size * seq_length, vocab_size]\n",
    "        labels = labels.view(-1)  # [batch_size * seq_length]\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        # Now, you can update the model's weights using the optimizer\n",
    "        optim.step()\n",
    "        # Zero gradients after updating the model's weights\n",
    "        optim.zero_grad()\n",
    "\n",
    "        train_loop.set_description(f'Epoch {epoch}')\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    \n",
    "\n",
    "    masked_token_accuracy = (accuracy_score(masked_token_ground_truths_all, masked_token_predictions_all))\n",
    "    masked_token_precision, masked_token_recall, masked_token_f1, _ = precision_recall_fscore_support(masked_token_ground_truths_all,masked_token_predictions_all,average='weighted')\n",
    "\n",
    "    seq_precision, seq_recall, seq_f1, _ = precision_recall_fscore_support(seq_ground_truths_all,seq_predictions_all,average='weighted')\n",
    "    \n",
    "    print(\"Training: \",    '  Masked Token f1',masked_token_f1 , \"    SEQ F1\",seq_f1)\n",
    "    global_masked_token_metrices.append( masked_token_f1) \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ###########################################\n",
    "    ###############  EVAL Validation  #########\n",
    "    ###########################################\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        v_masked_token_predictions_all, v_masked_token_ground_truths_all = None, None\n",
    "        v_seq_predictions_all, v_seq_ground_truths_all = None, None\n",
    "    \n",
    "    \n",
    "        validation_loop = tqdm(validation_loader, leave=True)\n",
    "        for N,v_batch in enumerate(validation_loop):\n",
    "            \n",
    "            \n",
    "            \n",
    "            v_input_ids = v_batch['input_ids'].to(device)\n",
    "            # v_token_type_ids = v_batch['token_type_ids'].to(device)\n",
    "            v_attention_mask = v_batch['attention_mask'].to(device)\n",
    "            # v_next_sentence_label = v_batch['next_sentence_label'].to(device)\n",
    "            v_mask_arr = v_batch ['mask_arr']\n",
    "            v_labels = v_batch['labels'].to(device)\n",
    "            # process\n",
    "            v_outputs = model(v_input_ids, attention_mask=v_attention_mask)\n",
    "\n",
    "\n",
    "            v_logits = v_outputs.logits\n",
    "\n",
    "        \n",
    "            v_token_prediction = torch.argmax(v_logits, axis=-1)\n",
    "\n",
    "                    \n",
    "\n",
    "            v_batch_masks =   [ torch.flatten(bm.nonzero()).tolist()  for bm in v_mask_arr]\n",
    "            \n",
    "            v_masked_token_prediction = [ token[v_batch_masks[t]].tolist() for t,token in enumerate(v_token_prediction) ]\n",
    "            v_masked_token_prediction = list(chain.from_iterable(v_masked_token_prediction))\n",
    "            \n",
    "            v_masked_token_ground_truth   = [ token[v_batch_masks[t]].tolist() for t,token in enumerate(v_labels) ]\n",
    "            v_masked_token_ground_truth = list(chain.from_iterable(v_masked_token_ground_truth))\n",
    "    \n",
    "            \n",
    "            \n",
    "\n",
    "            v_seq_prediction = v_token_prediction.detach().cpu().numpy().flatten()\n",
    "            v_seq_ground_truth = v_labels.detach().cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            # token_prediction = [ token[batch_masks[t]].tolist() for t,token in enumerate(token_prediction) ]\n",
    "            # token_prediction = list(chain.from_iterable(token_prediction))\n",
    "            \n",
    "            # token_ground_truth   = [ token[batch_masks[t]].tolist() for t,token in enumerate(labels) ]\n",
    "            # token_ground_truth = list(chain.from_iterable(token_ground_truth))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            if N==0:\n",
    "\n",
    "                v_masked_token_predictions_all   = v_masked_token_prediction\n",
    "                v_masked_token_ground_truths_all = v_masked_token_ground_truth\n",
    "                \n",
    "                v_seq_predictions_all= v_seq_prediction\n",
    "                v_seq_ground_truths_all = v_seq_ground_truth\n",
    "\n",
    "        \n",
    "\n",
    "            else:\n",
    "\n",
    "                v_masked_token_predictions_all   = np.concatenate((v_masked_token_predictions_all, v_masked_token_prediction ))\n",
    "                v_masked_token_ground_truths_all = np.concatenate((v_masked_token_ground_truths_all, v_masked_token_ground_truth ))\n",
    "                \n",
    "                v_seq_predictions_all =np.concatenate((v_seq_predictions_all, v_seq_prediction ))\n",
    "                v_seq_ground_truths_all =np.concatenate((v_seq_ground_truths_all, v_seq_ground_truth ))\n",
    "                \n",
    "\n",
    "            \n",
    " \n",
    "\n",
    "        v_masked_token_accuracy = (accuracy_score(v_masked_token_ground_truths_all, v_masked_token_predictions_all))\n",
    "        v_masked_token_precision, v_masked_token_recall, v_masked_token_f1, _ = precision_recall_fscore_support(v_masked_token_ground_truths_all,v_masked_token_predictions_all,average='weighted')\n",
    "\n",
    "\n",
    "        v_seq_accuracy = (accuracy_score(v_seq_predictions_all, v_seq_ground_truths_all))\n",
    "        v_seq_precision, v_seq_recall, v_seq_f1, _ = precision_recall_fscore_support(v_seq_ground_truths_all,v_seq_predictions_all,average='weighted')\n",
    "\n",
    "        print(\"Validation: \",  \"   v_masked_token_ F1: \",v_masked_token_f1 ,\" V SEQ F1: \", v_seq_f1)\n",
    "        \n",
    "\n",
    "        v_global_masked_token_metrices.append(v_masked_token_f1) \n",
    "\n",
    "    \n",
    "    plot_graph(global_instruction_metrices, v_global_instruction_metrices, 'Next Sentence Prediction Scores')\n",
    "    plot_graph(global_masked_token_metrices, v_global_masked_token_metrices, 'Masked Token Prediction Scores')\n",
    "    model.save_pretrained(\"./../../models/\"+EXPERIMENT_NAME+\"lama_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[0,1,2,3,4,5]\n",
    "a[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128256/2/512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
