{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a59060-e593-41b3-a4ce-4bb340ef07b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os,re\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from num2words import num2words\n",
    "\n",
    "from trl import SFTTrainer\n",
    "# import torch\n",
    "\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "# from torch.distributed.fsdp.wrap import auto_wrap\n",
    "\n",
    "\n",
    "login(token = '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')\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "779c65c3-daa0-4075-9c9a-c0eb7ea73af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions Count:  241 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "DATA_PATH = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions'\n",
    "\n",
    "TRAIN_DATA_PATH  ='/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/train/'\n",
    "\n",
    "TEST_DATA_PATH   = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/test/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_json_files = [os.path.join(TRAIN_DATA_PATH, f) for f in os.listdir(TRAIN_DATA_PATH) ]\n",
    "\n",
    "test_json_files = [os.path.join(TEST_DATA_PATH, f) for f in os.listdir(TEST_DATA_PATH) ]\n",
    "\n",
    "# disassembly_decimal disassembly_all_number_to_words disassembly_decimal\n",
    "data_key = \"disassembly_all_number_to_words\"\n",
    "\n",
    "def read_corpus(json_files):\n",
    "\n",
    "    all = []\n",
    "\n",
    "    for k, j_file in enumerate(json_files):\n",
    "        # if k>20:\n",
    "        #     break\n",
    "        try:\n",
    "\n",
    "            with open(j_file, 'r') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                funct = data[data_key]['input']\n",
    "                \n",
    "                all.append(funct)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "    return all\n",
    "    \n",
    "\n",
    "\n",
    "train_text = read_corpus(train_json_files)\n",
    "test_text  = read_corpus(test_json_files)\n",
    "\n",
    "\n",
    "        \n",
    "# text = text[0:5000]\n",
    "print(\"Functions Count: \",len(train_text), len(test_text) '\\n')\n",
    "example = train_text[10]\n",
    "text = train_text + test_text\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(text), 1000):\n",
    "        yield text[i : i + 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41fe2949-b651-43ac-b90e-5b10ccae8e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDBR64\n",
      "PUSH R12\n",
      "PUSH RBP\n",
      "MOV EBP,EDI\n",
      "NEG EBP\n",
      "PUSH RBX\n",
      "CMOVS EBP,EDI\n",
      "CMP EBP,zero\n",
      "JLE five\n",
      "MOV EBX,zero\n",
      "LEA R12,[six]\n",
      "JMP four\n",
      "ADD EBX,zero\n",
      "CMP EBP,EBX\n",
      "JZ five\n",
      "MOV EAX,EBP\n",
      "CDQ\n",
      "IDIV EBX\n",
      "TEST EDX,EDX\n",
      "JNZ three\n",
      "MOV EDX,EBX\n",
      "MOV RSI,R12\n",
      "MOV EDI,one\n",
      "XOR EAX,EAX\n",
      "CALL two\n",
      "ADD EBX,zero\n",
      "CMP EBP,EBX\n",
      "JNZ four\n",
      "POP RBX\n",
      "POP RBP\n",
      "POP R12\n",
      "RET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c0932c-2cde-4148-85b7-0feb9504b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.Unigram())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "625a52cb-1c25-40e1-9600-d2f1db2ac56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7986e18-4308-462b-ae63-a6f3aea135d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Regex\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"``\", '\"'),\n",
    "        normalizers.Replace(\"''\", '\"'),\n",
    "        normalizers.NFKD(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "    ]\n",
    ")\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ac41cd-b3ab-4b8c-a000-53885499335d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDBR64\n",
      "PUSH R12\n",
      "PUSH RBP\n",
      "MOV EBP,EDI\n",
      "NEG EBP\n",
      "PUSH RBX\n",
      "CMOVS EBP,EDI\n",
      "CMP EBP,zero\n",
      "JLE five\n",
      "MOV EBX,zero\n",
      "LEA R12,[six]\n",
      "JMP four\n",
      "ADD EBX,zero\n",
      "CMP EBP,EBX\n",
      "JZ five\n",
      "MOV EAX,EBP\n",
      "CDQ\n",
      "IDIV EBX\n",
      "TEST EDX,EDX\n",
      "JNZ three\n",
      "MOV EDX,EBX\n",
      "MOV RSI,R12\n",
      "MOV EDI,one\n",
      "XOR EAX,EAX\n",
      "CALL two\n",
      "ADD EBX,zero\n",
      "CMP EBP,EBX\n",
      "JNZ four\n",
      "POP RBX\n",
      "POP RBP\n",
      "POP R12\n",
      "RET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.normalizer.normalize_str(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "144134ac-02c0-49ae-9ea0-d5ae5b40ef1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁ENDBR64\\nPUSH', (0, 12)),\n",
       " ('▁R12\\nPUSH', (12, 21)),\n",
       " ('▁RBP\\nMOV', (21, 29)),\n",
       " ('▁EBP,EDI\\nNEG', (29, 41)),\n",
       " ('▁EBP\\nPUSH', (41, 50)),\n",
       " ('▁RBX\\nCMOVS', (50, 60)),\n",
       " ('▁EBP,EDI\\nCMP', (60, 72)),\n",
       " ('▁EBP,zero\\nJLE', (72, 85)),\n",
       " ('▁five\\nMOV', (85, 94)),\n",
       " ('▁EBX,zero\\nLEA', (94, 107)),\n",
       " ('▁R12,[six]\\nJMP', (107, 121)),\n",
       " ('▁four\\nADD', (121, 130)),\n",
       " ('▁EBX,zero\\nCMP', (130, 143)),\n",
       " ('▁EBP,EBX\\nJZ', (143, 154)),\n",
       " ('▁five\\nMOV', (154, 163)),\n",
       " ('▁EAX,EBP\\nCDQ\\nIDIV', (163, 180)),\n",
       " ('▁EBX\\nTEST', (180, 189)),\n",
       " ('▁EDX,EDX\\nJNZ', (189, 201)),\n",
       " ('▁three\\nMOV', (201, 211)),\n",
       " ('▁EDX,EBX\\nMOV', (211, 223)),\n",
       " ('▁RSI,R12\\nMOV', (223, 235)),\n",
       " ('▁EDI,one\\nXOR', (235, 247)),\n",
       " ('▁EAX,EAX\\nCALL', (247, 260)),\n",
       " ('▁two\\nADD', (260, 268)),\n",
       " ('▁EBX,zero\\nCMP', (268, 281)),\n",
       " ('▁EBP,EBX\\nJNZ', (281, 293)),\n",
       " ('▁four\\nPOP', (293, 302)),\n",
       " ('▁RBX\\nPOP', (302, 310)),\n",
       " ('▁RBP\\nPOP', (310, 318)),\n",
       " ('▁R12\\nRET\\n', (318, 327))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "472f62d4-c2ad-4ad6-859c-a99348df4470",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9da034a8-b62c-4272-a8f0-1189b7e3a00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88a13051-7ac1-4541-b069-cf0855d7a021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁ENDBR64\\n', 'P', 'U', 'SH', '▁R12\\nP', 'U', 'SH', '▁RBP\\nMOV', '▁EBP,EDI\\n', 'N', 'E', 'G', '▁', 'EBP\\nP', 'U', 'SH', '▁RBX\\nC', 'MOV', 'S', '▁EBP,EDI\\n', 'CM', 'P', '▁EBP,zero\\nJ', 'LE', '▁five\\nMOV', '▁EBX,zero\\n', 'LEA', '▁R12,[', 'six', ']\\n', 'J', 'M', 'P', '▁', 'four\\nADD', '▁EBX,zero\\n', 'CM', 'P', '▁EBP,EBX\\nJ', 'Z', '▁five\\nMOV', '▁EAX,EBP\\n', 'CDQ\\nI', 'DIV', '▁', 'EBX\\n', 'T', 'ES', 'T', '▁EDX,EDX\\n', 'JN', 'Z', '▁three\\nMOV', '▁EDX,EBX\\n', 'MOV', '▁RSI,', 'R12\\nMOV', '▁EDI,one\\n', 'X', 'O', 'R', '▁EAX,EAX\\nC', 'AL', 'L', '▁two\\n', 'ADD', '▁EBX,zero\\n', 'CM', 'P', '▁EBP,EBX\\nJ', 'N', 'Z', '▁four\\nP', 'OP', '▁RBX\\n', 'P', 'OP', '▁RBP\\nP', 'OP', '▁R12\\nRET\\n']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(example)\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "808a999b-58e2-4c51-b082-96a7a1925058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=80, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cfaa1ef-67ea-4835-ab95-c0eea6c307b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.special_tokens_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb3f087f-3f61-4f7b-a335-ffe92be7b0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31,\n",
       " 12,\n",
       " 14,\n",
       " 21,\n",
       " 108,\n",
       " 14,\n",
       " 21,\n",
       " 174,\n",
       " 480,\n",
       " 72,\n",
       " 46,\n",
       " 217,\n",
       " 6,\n",
       " 974,\n",
       " 14,\n",
       " 21,\n",
       " 827,\n",
       " 20,\n",
       " 29,\n",
       " 480,\n",
       " 30,\n",
       " 12,\n",
       " 608,\n",
       " 74,\n",
       " 130,\n",
       " 189,\n",
       " 24,\n",
       " 435,\n",
       " 694,\n",
       " 722,\n",
       " 44,\n",
       " 40,\n",
       " 12,\n",
       " 6,\n",
       " 274,\n",
       " 189,\n",
       " 30,\n",
       " 12,\n",
       " 699,\n",
       " 13,\n",
       " 130,\n",
       " 502,\n",
       " 375,\n",
       " 358,\n",
       " 6,\n",
       " 798,\n",
       " 26,\n",
       " 51,\n",
       " 26,\n",
       " 359,\n",
       " 45,\n",
       " 13,\n",
       " 38,\n",
       " 278,\n",
       " 20,\n",
       " 39,\n",
       " 233,\n",
       " 82,\n",
       " 22,\n",
       " 23,\n",
       " 15,\n",
       " 63,\n",
       " 18,\n",
       " 16,\n",
       " 204,\n",
       " 32,\n",
       " 189,\n",
       " 30,\n",
       " 12,\n",
       " 699,\n",
       " 72,\n",
       " 13,\n",
       " 574,\n",
       " 25,\n",
       " 71,\n",
       " 12,\n",
       " 25,\n",
       " 76,\n",
       " 25,\n",
       " 145]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "821e59ee-86bc-40b2-b2e0-5857ba597bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "307aa99f-7d78-4c3a-a1ef-a513370d8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54a48f0c-b963-4208-bdac-1543bedaae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b2328c2-d35a-40f4-808d-234e888c31d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 31, 12, 14, 21, 108, 14, 21, 174, 480, 72, 46, 217, 6, 974, 14, 21, 827, 20, 29, 480, 30, 12, 608, 74, 130, 189, 24, 435, 694, 722, 44, 40, 12, 6, 274, 189, 30, 12, 699, 13, 130, 502, 375, 358, 6, 798, 26, 51, 26, 359, 45, 13, 38, 278, 20, 39, 233, 82, 22, 23, 15, 63, 18, 16, 204, 32, 189, 30, 12, 699, 72, 13, 574, 25, 71, 12, 25, 76, 25, 145, 4], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4d5a481-a7b4-466d-9517-6870f41a7ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ENDBR64\\nPUSH R12\\nPUSH RBP\\nMOV EBP,EDI\\nNEG EBP\\nPUSH RBX\\nCMOVS EBP,EDI\\nCMP EBP,zero\\nJLE five\\nMOV EBX,zero\\nLEA R12,[six]\\nJMP four\\nADD EBX,zero\\nCMP EBP,EBX\\nJZ five\\nMOV EAX,EBP\\nCDQ\\nIDIV EBX\\nTEST EDX,EDX\\nJNZ three\\nMOV EDX,EBX\\nMOV RSI,R12\\nMOV EDI,one\\nXOR EAX,EAX\\nCALL two\\nADD EBX,zero\\nCMP EBP,EBX\\nJNZ four\\nPOP RBX\\nPOP RBP\\nPOP R12\\nRET\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e731187-a02c-4c92-a00f-5b7e7a4ed0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./../models/cusTokenizer_UNI_25k_numToWords/tokenizer_config.json',\n",
       " './../models/cusTokenizer_UNI_25k_numToWords/special_tokens_map.json',\n",
       " './../models/cusTokenizer_UNI_25k_numToWords/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.save_pretrained(\"./../models/cusTokenizer_UNI_25k_numToWords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1a5eedd-7b7a-44c6-aa8a-044acab0adbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
