{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b4579f-cccb-4423-a0d9-f48d35e374bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys,os\n",
    "from elftools.elf.elffile import ELFFile\n",
    "from elftools.elf.segments import Segment\n",
    "from capstone import *\n",
    "from capstone.x86 import *\n",
    "\n",
    "import os\n",
    "import json \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import sys,os\n",
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f9e3ca-865a-481a-ae4c-89a5c21cac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_TOKEN_LEN = 1024\n",
    "BATCH_SIZE = 50\n",
    "epochs = 100\n",
    "\n",
    "EXPERIMENT_NAME = 'cusTokenizer_WP_35k_ATW_signature'\n",
    "new_vocab_size = 35000\n",
    "# disassembly_decimal disassembly_all_number_to_words disassembly_decimal \n",
    "data_key = \"disassembly_all_number_to_words\"\n",
    "\n",
    "\n",
    "from transformers import BartTokenizer,BertTokenizer, BertForNextSentencePrediction,BertForPreTraining,BertConfig,AutoModelForMaskedLM,get_linear_schedule_with_warmup ,BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast ,AutoModelForPreTraining ,BertGenerationEncoder, BartForConditionalGeneration,BertGenerationDecoder,EncoderDecoderModel\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"./../../models/\" + EXPERIMENT_NAME)\n",
    "print(tokenizer.pad_token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd5a1a9-36cf-459c-afe7-40a1b3d2bb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions Count:  80000 \n",
      "\n",
      "['int', '*struct']\n",
      "30 \n",
      " ['int', '*char', '*int', 'short', '*short', '*uint', '*ulong', '*struct', 'ushort', 'long', '*longlong', 'uchar', 'longlong', '*long', 'void', 'ulong', '*bool', 'double', '*ulonglong', 'uint', 'None', '*uchar', '*float', '*ushort', '*void', '*double', 'char', 'bool', 'float', 'ulonglong']\n"
     ]
    }
   ],
   "source": [
    "def generate_param_string(param_list):\n",
    "\n",
    "\n",
    "    if len(param_list)==0:\n",
    "        return 'void'\n",
    "    else:\n",
    "        return \" \".join(param_list)\n",
    "def unique_items(list_of_lists):\n",
    "    # Flatten the list of lists\n",
    "    print(list_of_lists[3].split())\n",
    "    flattened_list = [item for sublist in list_of_lists for item in sublist.split()]\n",
    "    # Convert the flattened list to a set to remove duplicates\n",
    "    unique_elements = set(flattened_list)\n",
    "    return list(unique_elements)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DATA_PATH = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions'\n",
    "\n",
    "TRAIN_DATA_PATH  ='/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/train/'\n",
    "\n",
    "TEST_DATA_PATH   = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/test/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_json_files = [os.path.join(TRAIN_DATA_PATH, f) for f in os.listdir(TRAIN_DATA_PATH) ]\n",
    "\n",
    "test_json_files = [os.path.join(TEST_DATA_PATH, f) for f in os.listdir(TEST_DATA_PATH) ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_corpus(json_files):\n",
    "\n",
    "    all = []\n",
    "    all_params = []\n",
    "    all_returns = []\n",
    "    all_signatures = []\n",
    "\n",
    "    for k, j_file in enumerate(json_files):\n",
    "        # if k>1000:\n",
    "        #     break\n",
    "        try:\n",
    "\n",
    "            with open(j_file, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                funct = data[data_key]['input']\n",
    "\n",
    "                # if len(data['signature']['paramTypes'])<5:\n",
    "                all.append(funct)\n",
    "                param_string = generate_param_string( data['signature']['paramTypes'] )\n",
    "                all_params.append( param_string )\n",
    "                all_returns.append(  data['signature']['returnType']  )\n",
    "                all_signatures.append(param_string + \" [SEP] \"+ data['signature']['returnType'] )\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "    return all , all_params , all_returns , all_signatures\n",
    "    \n",
    "\n",
    "\n",
    "train_text , train_params , train_returns, train_signatures = read_corpus(train_json_files)\n",
    "\n",
    "test_text  , test_params  , test_returns , test_signatures = read_corpus(test_json_files)\n",
    "\n",
    "\n",
    "        \n",
    "# text = text[0:5000]\n",
    "print(\"Functions Count: \",len(train_text), '\\n')\n",
    "example = train_text[10]\n",
    "text = train_text + test_text\n",
    "\n",
    "\n",
    "unique_types = unique_items(train_params)\n",
    "print(len(unique_types), '\\n',unique_types)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33d9aa6-251c-4ace-a92a-7037d7c36ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "train_source_encodings = tokenize(train_text)\n",
    "train_target_encodings = tokenize(train_signatures)\n",
    "\n",
    "\n",
    "test_source_encodings = tokenize(test_text)\n",
    "test_target_encodings = tokenize(test_signatures)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(train_source_encodings['input_ids'], train_source_encodings['attention_mask'], train_target_encodings['input_ids'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "test_dataset = TensorDataset(test_source_encodings['input_ids'], test_source_encodings['attention_mask'], test_target_encodings['input_ids'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37578ca9-ed2d-4861-b0ef-e190a62c9b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8630a7f-854d-49a3-990f-e91f1c12151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"./../../models/\"+EXPERIMENT_NAME+\"_model.ckpt\", forced_bos_token_id=0)\n",
    "# tok = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "# example_english_phrase = \"UN Chief Says There Is No <mask> in Syria\"\n",
    "# batch = tok(example_english_phrase, return_tensors=\"pt\")\n",
    "# generated_ids = model.generate(batch[\"input_ids\"])\n",
    "# assert tok.batch_decode(generated_ids, skip_special_tokens=True) == [\n",
    "#     \"UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2fd5b8-233c-439d-94a7-96902bc2a488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358add79-a5dc-4b80-8ba7-73cbbf3de4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LongformerModel, LongformerTokenizer\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "# model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "# # Example of how to prepare inputs\n",
    "# inputs = tokenizer(\"Example of a long input sequence \" * 50, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "# output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf9ba31-668a-4b0c-ad01-182c3976105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token_id = tokenizer.sep_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "\n",
    "\n",
    "\n",
    "# def align_output(pred, gt):\n",
    "#     params_pred = []\n",
    "#     params_gt = []\n",
    "#     ret_pred = []\n",
    "#     ret_gt = []\n",
    "#     for i,gid in enumerate(gt):\n",
    "#         if gid!= cls_token_id:\n",
    "            \n",
    "            \n",
    "#             aligned_pred.append(pred[i])\n",
    "#             aligned_gt.append(gid)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0e9fb-9576-447e-9229-b1199ec4b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d79dec-3b92-42a4-8fd9-762179256ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(1):  # Number of epochs\n",
    "\n",
    "##########################################\n",
    "    #eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_all_ground_truth = []\n",
    "        test_all_prediction = []\n",
    "        right = 0\n",
    "        wrong = 0\n",
    "        for n,test_batch in enumerate(test_loader):\n",
    "    \n",
    "            test_input_ids, test_attention_mask, test_labels = [x.to(device) for x in test_batch]\n",
    "            # print(input_ids.shape , attention_mask.shape , labels.shape )\n",
    "            test_outputs = model(input_ids=test_input_ids, attention_mask=test_attention_mask)\n",
    "            test_token_predictions = torch.argmax(test_outputs.logits, axis=-1)\n",
    "    \n",
    "            test_labels_list = test_labels.tolist()\n",
    "            test_token_predictions_list = test_token_predictions.tolist()\n",
    "            \n",
    "            for  j,test_label in enumerate(test_labels_list):\n",
    "                if test_label==test_token_predictions_list[j]:\n",
    "                    right = right +1\n",
    "                else:\n",
    "                    wrong = wrong+ 1\n",
    "                for i, lid in enumerate(test_label):\n",
    "                    if lid not in  [cls_token_id , sep_token_id , pad_token_id]:\n",
    "                        test_all_ground_truth.append(lid)\n",
    "                        test_all_prediction.append(test_token_predictions_list[j][i])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        print(\"# # #\" *10)\n",
    "        # test_pred_tokens = tokenizer.convert_ids_to_tokens(test_token_predictions[0].tolist())\n",
    "        # test_gt_tokens = tokenizer.convert_ids_to_tokens(test_labels[0].tolist())\n",
    "        # print('test_pred_tokens' , test_pred_tokens)\n",
    "        # print('test_gt_tokens' ,test_gt_tokens)\n",
    "        print ('epoch: ',epoch , ' n: ', n )\n",
    "        print(len(test_all_ground_truth) , len(test_all_prediction))\n",
    "        print(test_all_ground_truth[-10:])\n",
    "        print(test_all_prediction[-10:])\n",
    "        test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(test_all_ground_truth,test_all_prediction ,average='weighted')\n",
    "        print('TEST precision, recall, f1' ,test_precision, test_recall, test_f1)\n",
    "    \n",
    "        print(test_labels_list[0])\n",
    "        print(test_token_predictions_list[0])\n",
    "        print(\"right\",right, \"  wrong : \", wrong)\n",
    "    \n",
    "        # Save the model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d99f0-476e-4a01-a056-d3cec482ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token_id = tokenizer.sep_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ed88a9-4f8c-444f-99f8-f214df133a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
