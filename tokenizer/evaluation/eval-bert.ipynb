{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2643729/3274186680.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys,os\n",
    "from elftools.elf.elffile import ELFFile\n",
    "from elftools.elf.segments import Segment\n",
    "from capstone import *\n",
    "from capstone.x86 import *\n",
    "\n",
    "import os\n",
    "import json \n",
    "\n",
    "import sys,os\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForPreTraining(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "      (position_embeddings): Embedding(1024, 384)\n",
       "      (token_type_embeddings): Embedding(2, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertPreTrainingHeads(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=384, out_features=30522, bias=True)\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=384, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MAX_TOKEN_LEN = 1024\n",
    "BATCH_SIZE = 100#160 #40\n",
    "epochs = 10\n",
    "\n",
    "EXPERIMENT_NAME = 'defaultTokenizer_Trained_ASIS'#defaultTokenizer_Trained_ASIS_model.ckpt \n",
    "# disassembly_decimal disassembly_all_number_to_words disassembly_decimal  disassembly_addresses_to_words\n",
    "data_key = \"disassembly_decimal\"\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction,BertForPreTraining,BertConfig,AutoModelForMaskedLM,get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast ,AutoModelForPreTraining\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('microsoft/MiniLM-L12-H384-uncased')\n",
    "# print(tokenizer.pad_token) \n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('./../../models/bert_ASIS')\n",
    "print(tokenizer.pad_token) \n",
    "\n",
    "\n",
    "# config = BertConfig.from_pretrained(\"./../../models/\"+EXPERIMENT_NAME+\"_model.ckpt\") #\"./../../models/\"+EXPERIMENT_NAME+\"_model.ckpt\"\n",
    "# # Change max_position_embeddings to 1024 in the config\n",
    "# config.max_position_embeddings = 1024\n",
    "# Load the model with the modified config\n",
    "model = BertForPreTraining.from_pretrained(\"./../../models/\"+EXPERIMENT_NAME+\"_model.ckpt\" )#('./../../models/cusTokenizer_UNI_25k_ATW_albert_model.ckpt')#\n",
    "# Access and modify the positional embeddings\n",
    "# # The model's `bert` attribute holds the BERT layers\n",
    "# model_bert = model.bert  # This is the BERT model itself (the backbone)\n",
    "# # Resize the position embeddings to accommodate the new max length (1024 tokens)\n",
    "# model_bert.embeddings.position_embeddings = torch.nn.Embedding(1024, model_bert.config.hidden_size)\n",
    "# # Optional: Initialize the new embeddings using the original ones for the first 512 positions\n",
    "# with torch.no_grad():\n",
    "#     model_bert.embeddings.position_embeddings.weight[:512, :] = model_bert.embeddings.position_embeddings.weight[:512, :]\n",
    "\n",
    "\n",
    "# model.resize_token_embeddings(new_vocab_size)\n",
    "\n",
    "\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "Functions Count:  80000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "DATA_PATH = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions'\n",
    "\n",
    "TRAIN_DATA_PATH  ='/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/train/'\n",
    "\n",
    "TEST_DATA_PATH   = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/test/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_json_files = [os.path.join(TRAIN_DATA_PATH, f) for f in os.listdir(TRAIN_DATA_PATH) ]\n",
    "\n",
    "test_json_files = [os.path.join(TEST_DATA_PATH, f) for f in os.listdir(TEST_DATA_PATH) ]\n",
    "\n",
    "\n",
    "print(len(train_json_files))\n",
    "def read_corpus(json_files):\n",
    "\n",
    "    all = []\n",
    "\n",
    "    for k, j_file in enumerate(json_files):\n",
    "        # if k>1000:\n",
    "        #     break\n",
    "        try:\n",
    "\n",
    "            with open(j_file, 'r') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                funct = data[data_key]['input']\n",
    "                \n",
    "                all.append(funct)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "    return all\n",
    "    \n",
    "\n",
    "\n",
    "train_text = read_corpus(train_json_files)\n",
    "test_text  = read_corpus(test_json_files)\n",
    "\n",
    "\n",
    "        \n",
    "# text = text[0:5000]\n",
    "print(\"Functions Count: \",len(train_text), '\\n')\n",
    "example = train_text[10]\n",
    "text = train_text + test_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDBR64\n",
      "PUSH R15\n",
      "LEA RDI,[8270]\n",
      "PUSH R14\n",
      "PUSH R13\n",
      "PUSH R12\n",
      "PUSH RBP\n",
      "PUSH RBX\n",
      "SUB RSP,8\n",
      "CALL 4256\n",
      "MOVSXD RCX,dword ptr [17568]\n",
      "CMP ECX,1\n",
      "JLE 5555\n",
      "XOR R13D,R13D\n",
      "LEA RBX,[16576]\n",
      "LEA R14,[16480]\n",
      "LEA RBP,[8297]\n",
      "LEA R12,[16640]\n",
      "NOP dword ptr [RAX]\n",
      "XOR EAX,EAX\n",
      "MOV R15D,4294967295\n",
      "MOV EDX,987654321\n",
      "NOP dword ptr [RAX]\n",
      "MOV ESI,dword ptr [RBX + RAX*4]\n",
      "TEST ESI,ESI\n",
      "JNZ 5476\n",
      "MOV ESI,dword ptr [R12 + RAX*4]\n",
      "CMP EDX,ESI\n",
      "CMOVG R15D,EAX\n",
      "CMOVG EDX,ESI\n",
      "ADD RAX,1\n",
      "CMP RCX,RAX\n",
      "JNZ 5456\n",
      "MOVSXD RAX,R15D\n",
      "ADD dword ptr [16560],EDX\n",
      "MOV RSI,RBP\n",
      "MOV EDX,R15D\n",
      "MOV ECX,dword ptr [R14 + RAX*8]\n",
      "MOV R8D,dword ptr [R14 + RAX*8 + 4]\n",
      "MOV dword ptr [RBX + RAX*4],1\n",
      "XOR EAX,EAX\n",
      "MOV EDI,2\n",
      "ADD R13D,1\n",
      "CALL 4288\n",
      "MOV EDI,R15D\n",
      "CALL 5104\n",
      "MOVSXD RCX,dword ptr [17568]\n",
      "LEA EAX,[RCX + -1]\n",
      "CMP R13D,EAX\n",
      "JL 5440\n",
      "MOV EDX,dword ptr [16560]\n",
      "ADD RSP,8\n",
      "LEA RSI,[8321]\n",
      "XOR EAX,EAX\n",
      "POP RBX\n",
      "MOV EDI,2\n",
      "POP RBP\n",
      "POP R12\n",
      "POP R13\n",
      "POP R14\n",
      "POP R15\n",
      "JMP 4288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text[51].split(delim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assign a 50% probability of using the genuine next sentence, and 50% probability of using another random sentence.\n",
    "\n",
    "To make this simpler, we'll create a *'bag'* of individual sentences to pull from when selecting a random sentence B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5312741 100000\n"
     ]
    }
   ],
   "source": [
    "delim = '\\n'\n",
    "bag = [instruction for instruction_cluster in text for instruction in instruction_cluster.split(delim)  if instruction!= '']\n",
    "bag_size = len(bag)\n",
    "print(bag_size , len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we create our 50/50 NIP training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "['ENDBR64', 'PUSH R12', 'LEA RSI,[4608]', 'MOV EDI,2', 'LEA R12,[8253]', 'PUSH RBP', 'LEA RBP,[8262]', 'PUSH RBX', 'XOR EBX,EBX', 'CALL 4224', 'JMP 4323', 'MOV RDI,RBP', 'ADD EBX,1', 'CALL 4208', 'CMP EBX,10000', 'JZ 4352', 'CMP EBX,100', 'JNZ 4304', 'MOV RDI,R12', 'MOV EBX,101', 'CALL 4208', 'MOV RDI,RBP', 'CALL 4208', 'JMP 4304', 'POP RBX', 'XOR EAX,EAX', 'POP RBP', 'POP R12', 'RET']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "history = []\n",
    "next_instruction = []\n",
    "label = []\n",
    "\n",
    "\n",
    "instruction_pages = []\n",
    "for instruction_cluster in text:\n",
    "    instructions = [\n",
    "        instruction for instruction in instruction_cluster.split(delim) if instruction != ''\n",
    "    ]\n",
    "\n",
    "    instruction_pages.append(instructions)\n",
    "\n",
    "        \n",
    "print(len(instruction_pages))\n",
    "print(instruction_pages[0])\n",
    "\n",
    "for instruction_page in instruction_pages:\n",
    "        # this is IsNextSentence\n",
    "        history.append(delim.join(instruction_page))\n",
    "        next_instruction.append(instruction_page[-1])\n",
    "        label.append(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready for tokenization, this time we truncate/pad each token to the same length of *512* tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(history, next_instruction, return_tensors='pt', \n",
    "                   max_length=MAX_TOKEN_LEN, truncation=True, padding=True)\n",
    "ground_truth = inputs.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the *token_type_ids* tensors have been built correctly (eg **1** indicating sentence B tokens) by checking the first instance of *token_type_ids*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the labels tensor is simply a clone of the input_ids tensor before masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs['next_sentence_label'] = torch.LongTensor([label]).T\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we mask tokens in the input_ids tensor using the 15% probability for MLM - ensuring we don't mask CLS, SEP, or PAD tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "# create mask array\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 905])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr.shape\n",
    "# inputs.input_ids.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now take the indices of each True value within each vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,\n",
       " 100000,\n",
       " [[3, 6, 17, 21, 31, 43, 49, 52, 53, 55, 58, 61, 71, 83],\n",
       "  [20,\n",
       "   25,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   33,\n",
       "   35,\n",
       "   42,\n",
       "   51,\n",
       "   60,\n",
       "   65,\n",
       "   74,\n",
       "   90,\n",
       "   92,\n",
       "   93,\n",
       "   96,\n",
       "   98,\n",
       "   108,\n",
       "   123,\n",
       "   142,\n",
       "   164,\n",
       "   175,\n",
       "   177,\n",
       "   179,\n",
       "   182,\n",
       "   191,\n",
       "   197,\n",
       "   204,\n",
       "   205,\n",
       "   211,\n",
       "   216,\n",
       "   219,\n",
       "   228,\n",
       "   233,\n",
       "   249,\n",
       "   253,\n",
       "   257,\n",
       "   265,\n",
       "   269,\n",
       "   275,\n",
       "   276,\n",
       "   294,\n",
       "   303,\n",
       "   312,\n",
       "   314,\n",
       "   327,\n",
       "   348,\n",
       "   381,\n",
       "   396,\n",
       "   397,\n",
       "   404,\n",
       "   405,\n",
       "   408,\n",
       "   413,\n",
       "   417,\n",
       "   427,\n",
       "   430,\n",
       "   431,\n",
       "   434,\n",
       "   453,\n",
       "   461],\n",
       "  [3,\n",
       "   4,\n",
       "   7,\n",
       "   8,\n",
       "   19,\n",
       "   23,\n",
       "   24,\n",
       "   37,\n",
       "   44,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   69,\n",
       "   73,\n",
       "   78,\n",
       "   82,\n",
       "   87,\n",
       "   88,\n",
       "   100,\n",
       "   102,\n",
       "   108,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   127,\n",
       "   128,\n",
       "   150,\n",
       "   170,\n",
       "   172,\n",
       "   179,\n",
       "   184,\n",
       "   200,\n",
       "   221,\n",
       "   224,\n",
       "   229]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (selection) , len(inputs.input_ids), selection[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then apply these indices to each row in input_ids, assigning each value at these indices a value of 103."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_labels = []\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    masked_labels.append(inputs.input_ids[i, selection[i]])\n",
    "    inputs.input_ids[i, selection[i]] = 103\n",
    "# masked_labels[0]\n",
    "inputs[\"mask_arr\"] = mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'next_sentence_label', 'labels', 'mask_arr'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inputs` tensors are now ready, and we can begin building the model input pipeline for training. We first create a PyTorch dataset from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeditationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our data using the `MeditationDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MeditationsDataset(inputs)\n",
    "# print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 20000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_dataset  = torch.utils.data.Subset(dataset, range(len(train_text)))\n",
    "validation_dataset = torch.utils.data.Subset(dataset, range(len(train_text) , len(dataset)))\n",
    "\n",
    "len(train_dataset) , len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And initialize the dataloader, which we'll be using to load our data into the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader      = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the training mode of our model, and initialize our optimizer (Adam with weighted decay - reduces chance of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto the training loop, we'll train for a couple of epochs (change `epochs` to modify this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odict_keys(['loss', 'prediction_logits', 'seq_relationship_logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import *\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graph(training_data, validation_data , label ):\n",
    "\n",
    "    font_size = 10\n",
    "    x_labels = [ i for i in range(len(training_data)) ]\n",
    "\n",
    "    plt.ylabel(' F1 ',fontsize=font_size)\n",
    "    plt.plot(x_labels, training_data , 'r') \n",
    "    plt.plot(x_labels, validation_data , 'b') \n",
    "    plt.xlabel(\"Epoch\", fontsize=font_size)\n",
    "    plt.title(label,fontsize=font_size)\n",
    "    plt.legend(['Training', 'Validation'], loc='upper left') \n",
    "    \n",
    "    plt.savefig('./../../results/'+EXPERIMENT_NAME+label+'.pdf')\n",
    "    plt.show()\n",
    "    with open('./../../results/'+EXPERIMENT_NAME+label+'.json', 'w') as json_file:\n",
    "        json.dump([training_data, validation_data , label], json_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=3e-4 ,weight_decay=0.0001)\n",
    "\n",
    "# Number of training steps per epoch\n",
    "train_steps = len(train_loader)\n",
    "\n",
    "# Define the number of total training steps and warmup steps\n",
    "total_steps = train_steps * epochs\n",
    "warmup_steps = int(total_steps * 0.1)  # 10% of total steps as warmup\n",
    "\n",
    "# Scheduler setup\n",
    "scheduler = get_linear_schedule_with_warmup(optim,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                           | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_2643729/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "  0%|▏                                  | 1/200 [00:00<01:52,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_prediction\n",
      " tensor([[  2, 161, 133,  ...,   0,   0,   0],\n",
      "        [  2, 161, 121,  ...,   0,   0,   0],\n",
      "        [  2, 161, 121,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  2, 161, 234,  ...,   0,   0,   0],\n",
      "        [  2, 161, 133,  ...,   0,   0,   0],\n",
      "        [  2, 161,  85,  ...,   0,   0,   0]], device='cuda:1') \n",
      " input_ids \n",
      " tensor([[103, 161, 133,  ...,   0,   0,   0],\n",
      "        [  2, 103, 121,  ...,   0,   0,   0],\n",
      "        [  2, 161, 121,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  2, 161, 234,  ...,   0,   0,   0],\n",
      "        [  2, 161, 133,  ...,   0,   0,   0],\n",
      "        [103, 103,  85,  ...,   0,   0,   0]], device='cuda:1')\n",
      "masked_token_prediction\n",
      ":  [2, 98, 105, 22, 21, 7, 402, 91, 113, 22, 96, 106, 91, 22, 135, 88, 7, 142, 119, 161, 85, 194, 5, 98, 157, 22, 22, 88, 96, 96, 130, 7, 98, 133, 145, 153, 105, 7, 5, 112, 96, 5, 22, 11, 119, 153, 96, 21, 153, 7, 126, 11, 169, 91, 22, 106, 21, 18, 121, 133, 191, 140, 13, 105, 111, 22, 114, 96, 105, 7, 140, 21, 150, 7, 5, 14, 7, 1554, 85, 7, 111, 91, 595, 126, 115, 115, 111, 21, 6, 18, 22, 129, 1432, 161, 7, 7, 106, 21, 142, 98, 6, 105, 7, 150, 7, 91, 221, 150, 21, 117, 21, 105, 195, 195, 106, 195, 18, 22, 7, 314, 117, 12, 106, 98, 7, 85, 85, 98, 6, 96, 106, 88, 154, 14, 113, 197, 117, 119, 7, 12, 7, 119, 108, 7, 119, 108, 284, 3, 96, 117, 109, 173, 112, 109, 22, 7, 91, 21, 6, 14, 119, 96, 96, 5, 14, 148, 126, 11, 7, 91, 21, 6, 96, 112, 22, 21, 7, 1432, 112, 6, 128, 85, 88, 91, 7, 18, 22, 115, 146, 121, 21, 133, 98, 7, 98, 7, 113, 85, 203, 21, 6, 22, 130, 7, 105, 96, 6, 113, 88, 96, 7, 155, 142, 859, 98, 108, 115, 115, 114, 115, 621, 121, 85, 85, 98, 18, 7, 113, 21, 21, 106, 96, 7, 85, 88, 91, 260, 17, 6, 11, 22, 21, 215, 85, 85, 7, 7, 215, 85, 106, 98, 18, 91, 6, 7, 91, 18, 85, 91, 7, 438, 106, 18, 85, 85, 145, 22, 22, 179, 111, 22, 117, 85, 6, 18, 7, 91, 7, 21, 7, 115, 170, 154, 222, 7, 21, 21, 88, 7, 7, 179, 263, 153, 96, 22, 179, 91, 22, 7, 237, 7, 153, 263, 91, 105, 106, 22, 88, 115, 7, 21, 121, 157, 85, 21, 6, 88, 7, 106, 22, 153, 18, 21, 200, 7, 91, 85, 88, 21, 22, 244, 88, 91, 281, 6, 244, 7, 85, 7, 91, 6, 21, 6, 5, 91, 119, 91, 119, 128, 7, 108, 154, 161, 133, 334, 106, 6, 98, 6, 7, 297, 155, 20, 142, 22, 96, 7, 22, 91, 98, 21, 22, 96, 91, 21, 98, 191, 85, 297, 85, 7, 22, 112, 21, 113, 22, 108, 128, 22, 88, 153, 88, 112, 7, 798, 98, 6, 22, 133, 96, 88, 7, 105, 11, 766, 3, 114, 22, 907, 6, 85, 106, 98, 88, 7, 85, 111, 2299, 22, 85, 7, 126, 11, 109, 22, 142, 115, 7, 7, 21, 91, 2307, 5203, 2, 85, 111, 157, 140, 133, 21, 109, 21, 22, 317, 85, 117, 91, 469, 145, 88, 6, 18, 169, 114, 169, 106, 91, 169, 141, 7, 21, 141, 1825, 119, 96, 11, 96, 22, 169, 22, 91, 21, 96, 169, 96, 126, 96, 7, 85, 7, 98, 7, 109, 105, 109, 126, 7, 105, 121, 121, 105, 907, 7, 106, 22, 108, 341, 7, 10, 7, 96, 379, 96, 276, 153, 7, 98, 317, 21, 98, 2547, 141, 179, 91, 130, 7, 128, 215, 145, 88, 126, 130, 11, 215, 114, 85, 91, 21, 142, 106, 155, 20, 4723, 8, 85, 115, 18, 22, 4701, 333, 161, 113, 197, 117, 12, 98, 85, 106, 7, 85, 98, 237, 215, 149, 171, 5, 96, 7, 130, 7, 21, 11, 130, 215, 135, 88, 113, 21, 196, 196, 6, 91, 21, 949, 7, 184, 88, 7, 117, 223, 85, 7, 224, 96, 88, 22, 7, 130, 130, 7, 21, 7, 130, 200, 112, 153, 7, 85, 130, 145, 130, 223, 85, 7, 21, 12, 7, 7, 106, 20, 22, 21, 7, 139, 85, 96, 21, 21, 96, 10, 402, 108, 85, 21, 98, 7, 21, 7, 375, 108, 14, 111, 343, 22, 7, 142, 108, 7, 396, 22, 157, 22, 85, 7, 119, 85, 85, 7, 149, 105, 111, 21, 7, 21, 1054, 7, 88, 199, 91, 6, 7, 199, 22, 7, 21, 22, 85, 7, 21, 142, 98, 7, 3, 161, 105, 21, 98, 22, 197, 119, 12, 18, 10, 7, 88, 117, 121, 21, 22, 7, 11, 105, 108, 113, 108, 7, 179, 7, 119, 7, 91, 22, 179, 130, 7, 154, 22, 21, 18, 96, 8, 22, 22, 113, 7, 126, 98, 7, 85, 111, 115, 208, 2, 161, 111, 98, 11, 7, 7, 85, 88, 85, 88, 7, 91, 269, 91, 269, 85, 139, 88, 105, 11, 223, 223, 121, 111, 133, 98, 140, 171, 91, 21, 88, 126, 7, 88, 6, 16, 22, 109, 186, 186, 135, 7, 115, 148, 148, 187, 7, 85, 7, 20, 98, 105, 7, 112, 98, 6, 182, 141, 119, 7, 385, 88, 96, 141, 141, 85, 204, 88, 7, 22, 91, 21, 88, 197, 22, 378, 106, 142, 106, 6, 22, 88, 98, 22, 199, 128, 105, 11, 7, 402, 7, 88, 119, 85, 140, 196, 6, 126, 140, 260, 187, 161, 112, 22, 113, 119, 7, 7, 140, 191, 182, 6, 153, 7, 105, 171, 6, 96, 21, 6, 91, 6, 6, 96, 11, 7, 85, 7, 129, 114, 111, 105, 85, 7, 85, 6, 85, 146, 157, 108, 187, 85, 109, 12, 187, 105, 109, 7, 21, 7, 85, 7, 109, 7, 88, 7, 106, 126, 108, 115, 129, 208, 85, 109, 109, 96, 85, 109, 341, 85, 91, 91, 6, 126, 7, 18, 105, 85, 121, 106, 91, 6, 7, 10, 281, 133, 88, 119, 21, 22, 88, 140, 85, 18, 7, 85, 98, 7, 7, 119, 182, 96, 96, 96, 7, 91, 196, 3, 14, 105, 621, 91, 386, 106, 91, 142, 22, 85, 7, 98, 112, 234, 21, 22, 150, 343, 193, 193, 91, 22, 91, 119, 7, 88, 126, 247, 98, 6, 232, 135, 7, 223, 129, 105, 114, 7, 21, 6, 18, 7, 112, 113, 7, 199, 171, 111, 128, 222, 18, 108, 115, 154, 139, 7, 165, 7, 133, 106, 22, 7, 139, 6, 119, 6, 128, 139, 112, 7, 6, 14, 112, 91, 153, 6, 8, 7, 128, 7, 105, 133, 14, 7, 85, 88, 7, 6, 7, 5, 7, 133, 98, 114, 182, 105, 7, 21, 7, 7, 85, 105, 7, 111, 22, 91, 269, 8, 203, 148, 117, 12, 85, 113, 7, 117, 12, 176, 223, 161, 121, 21, 16, 108, 141, 12, 85, 119, 7, 117, 7, 141, 7, 85, 85, 13, 7, 141, 88, 7, 85, 117, 141, 7, 12, 141, 284, 284, 2, 7, 18, 7, 18, 7, 85, 91, 21, 22, 106, 7, 108, 7, 91, 18, 145, 88, 115, 111, 145, 7, 130, 85, 150, 153, 5, 22, 145, 7, 133, 88, 21, 21, 6, 5, 22, 128, 191, 105, 11, 352, 112, 150, 6, 98, 154, 7, 112, 145, 7, 873, 270, 7, 7, 3417, 270, 112, 7, 109, 7, 106, 22, 85, 1080, 22, 148, 3, 1324, 105, 7, 106, 119, 14, 119, 7, 7, 11, 7, 115, 161, 121, 121, 133, 142, 88, 157, 109, 113, 7, 106, 1221, 22, 7, 21, 7, 88, 21, 6, 351, 111, 85, 6, 149, 85, 21, 6, 98, 4747, 133, 20, 135, 7, 88, 3, 22, 91, 112, 21, 5, 11, 22, 11, 105, 229, 85, 96, 229, 7, 22, 126, 91, 153, 268, 154, 109, 229, 1752, 109, 821, 149, 106, 21, 109, 157, 7, 821, 85, 149, 796, 6, 157, 91, 21, 130, 106, 91, 130, 6, 85, 130, 96, 91, 6, 106, 91, 7, 106, 91, 7, 96, 7, 21, 91, 21, 106, 154, 157, 130, 10, 85, 21, 130, 7, 96, 130, 22, 85, 130, 130, 157, 96, 106, 148, 21, 130, 157, 96, 7, 113, 7, 85, 21, 149, 21, 7, 197, 20, 21, 98, 105, 98, 157, 22, 157, 7, 191, 21, 7, 155, 108, 154, 22, 85, 111, 105, 12, 105, 21, 113, 119, 4648, 284, 85, 7, 114, 85, 119, 88, 85, 117, 88, 85, 115, 88, 88, 115, 115, 3, 3, 161, 121, 85, 140, 109, 91, 21, 14, 109, 108, 7, 112, 22, 96, 21, 8, 115, 105, 3, 2, 106, 21, 91, 98, 18, 7, 98, 109, 507, 22, 105, 7, 508, 113, 6, 88, 7, 14, 7, 85, 7, 7, 91, 96, 154, 161, 618, 757, 21, 22, 157, 91, 148, 22, 21, 21, 157, 7, 1192, 1387, 1192, 662, 126, 133, 173, 22, 117, 7, 85, 106, 155, 7, 14, 7, 88, 96, 7, 448, 7, 182, 85, 285, 133, 88, 139, 21, 14, 96, 153, 14, 133, 88, 7, 119, 18, 22, 96, 155, 855, 88, 173, 208, 3, 121, 85, 7, 7, 193, 22, 470, 11, 7, 112, 14, 85, 7, 88, 3269, 113, 22, 6, 130, 193, 91, 113, 113, 7, 195, 91, 6, 113, 209, 7, 91, 113, 6, 153, 14, 22, 7, 21, 6, 22, 7, 112, 21, 181, 6, 5, 335, 290, 112, 7, 96, 112, 91, 22, 91, 21, 7, 91, 22, 7, 85, 21, 85, 139, 7, 85, 105, 7, 7, 105, 21, 148, 7, 18, 85, 343, 130, 157, 3, 3, 121, 265, 12, 88, 113, 111, 105, 21, 106, 106, 88, 22, 7, 121, 106, 7, 108, 121, 295, 85, 85, 284, 561, 106, 85, 12, 284, 126, 7, 108, 115, 119, 7, 7, 22, 106, 7, 108, 21, 6, 109, 105, 7, 85, 109, 7, 106, 85, 6, 142, 22, 7, 22, 115, 91, 108, 7, 117, 121, 7, 229, 85, 21, 11, 141, 85, 11, 6, 11, 119, 7, 88, 149, 443, 108, 88, 7, 21, 113, 6, 8, 11, 5, 91, 130, 179, 7, 5, 96, 88, 85, 488, 112, 141, 126, 130, 6, 489, 128, 85, 7, 91, 113, 14, 21, 22, 7, 7, 85, 230, 121, 114, 109, 114, 6, 96, 22, 91, 22, 215, 109, 238, 7, 5, 172, 7, 21, 11, 130, 106, 111, 12, 6, 18, 111, 21, 22, 21, 5, 22, 108, 139, 11, 170, 85, 96, 766, 21, 179, 12, 792, 18, 129, 88, 91, 7, 22, 105, 495, 105, 22, 91, 21, 309, 7, 133, 7, 128, 149, 7, 88, 196, 140, 115, 115, 154, 161, 121, 117, 7, 7, 21, 169, 22, 91, 21, 6, 222, 106, 18, 130, 114, 114, 3, 21, 109, 6, 1379, 22, 88, 193, 150, 193, 21, 7, 193, 21, 98, 7, 469, 139, 117, 469, 7, 332, 85, 117, 12, 91, 442, 109, 21, 98, 6, 7, 7, 96, 469, 332, 85, 22, 105, 157, 22, 85, 117, 469, 145, 332, 443, 117, 221, 130, 22, 7, 22, 106, 91, 135, 5832, 2119, 91, 21, 7, 6, 105, 113, 88, 85, 22, 91, 3735, 133, 242, 155, 98, 1582, 22, 7, 98, 91, 18, 6, 193, 98, 22, 585, 21, 351, 109, 208, 85, 208, 106, 108, 98, 7, 88, 126, 7, 161, 22, 129, 105, 114, 22, 106, 146, 119, 146, 85, 7, 18, 108, 88, 128, 7, 840, 18, 85, 11, 191, 129, 121, 7, 8, 7, 169, 7, 262, 7, 105, 7, 119, 119, 119, 126, 128, 7, 85, 7, 85, 7, 165, 135, 111, 169, 85, 7, 7, 21, 7, 21, 165, 114, 5254, 3, 2, 161, 21, 7, 85, 111, 219, 85, 229, 150, 21, 109, 22, 193, 91, 11, 128, 135, 88, 8, 88, 130, 7, 191, 489, 6, 21, 22, 112, 11, 91, 130, 88, 119, 7, 130, 5, 7, 141, 128, 96, 489, 7, 7, 139, 7, 128, 21, 6, 11, 21, 219, 511, 262, 139, 21, 219, 85, 139, 21, 85, 88, 115, 115, 3, 22, 121, 108, 7, 133, 142, 96, 7, 155, 98, 21, 199, 7, 109, 113, 21, 6, 7, 402, 12, 88, 223, 117, 179, 91, 6, 85, 117, 98, 14, 22, 11, 85, 196, 7, 7, 91, 173, 106, 91, 20, 21, 22, 3, 2, 129, 133, 7, 139, 119, 7, 22, 85, 88, 7, 111, 114, 115, 114, 113, 6, 114, 108, 296, 210, 85, 203, 7, 21, 109, 140, 98, 7, 149, 11, 109, 22, 96, 7, 12, 119, 409, 279, 7, 14, 7, 91, 112, 148, 430, 85, 91, 8, 88, 140, 176, 7, 7, 91, 21, 88, 88, 153, 7, 96, 8, 11, 7, 130, 7, 128, 85, 7, 91, 10, 247, 145, 406, 172, 179, 21, 6, 7, 135, 96, 106, 21, 20, 154, 3, 7, 113, 111, 128, 7, 111, 115, 129, 112, 6, 14, 149, 129, 7, 1207, 5, 85, 115, 119, 128, 85, 112, 14, 119, 91, 22, 161, 85, 21, 139, 21, 85, 7, 873, 194, 85, 11, 873, 21, 14, 21, 22, 262, 85, 113, 7, 141, 7, 96, 113, 11, 1069, 518, 18, 91, 22, 85, 21, 113, 113, 85, 109, 7, 6, 8, 18, 109, 7, 7, 9, 7, 191, 11, 14, 873, 106, 91, 6, 142, 4427, 96, 106, 85, 7, 7, 148, 121, 7, 242, 121, 378, 85, 7, 155, 20, 21, 7, 88, 21, 22, 145, 91, 22, 145, 22, 85, 21, 171, 96, 6, 85, 88, 88, 91, 324, 7, 375, 6, 7, 106, 22, 96, 22, 148, 7, 7, 12, 85, 119, 12, 117, 119, 7, 85, 117, 7, 88, 117, 284, 119, 88, 7, 108, 3, 154, 12, 21, 140, 133, 506, 109, 115, 85, 109, 157, 85, 21, 85, 22, 7, 109, 149, 7, 106, 18, 7, 111, 91, 6, 105, 7, 91, 6, 98, 85, 7, 11, 7, 7, 7, 139, 22, 88, 96, 96, 7, 126, 114, 154, 708, 7, 117, 88, 7, 161, 232, 142, 20, 21, 85, 22, 98, 112, 91, 7, 4427, 96, 22, 7, 85, 11, 91, 128, 1077, 91, 157, 22, 470, 193, 22, 106, 150, 91, 21, 130, 150, 22, 7, 85, 7, 18, 91, 22, 7, 91, 98, 7, 169, 135, 106, 20, 88, 7, 11, 7, 197, 121, 106, 22, 106, 91, 18, 22, 96, 269, 22, 85, 113, 88, 21, 7, 22, 21, 6, 85, 85, 98, 140, 165, 22, 7, 85, 117, 259, 96, 140, 140, 7, 165, 7, 7, 6, 153, 5, 96, 140, 91, 22, 457, 105, 22, 91, 140, 6, 153, 7, 117, 85, 112, 22, 88, 204, 269, 7, 109, 96, 21, 18, 133, 22, 135, 126, 115, 7, 133, 106, 91, 155, 20, 142, 98, 6, 7, 108, 88, 88, 105, 22, 22, 199, 112, 91, 21, 22, 22, 21, 119, 96, 7, 105, 6, 5, 11, 204, 274, 140, 22, 7, 106, 155, 337, 146, 133, 85, 106, 91, 98, 22, 7, 88, 7, 11, 7, 106, 85, 85, 7, 171, 224, 96, 5, 438, 438, 7, 165, 85, 12, 105, 91, 133, 91, 135, 7, 108, 7, 88, 129, 6, 278, 85, 7, 114, 108, 7, 88, 278, 85, 295, 203, 22, 121, 96, 4499, 126, 91, 105, 21, 6, 7, 11, 128, 149, 916, 85, 22, 186, 7, 389, 96, 96, 85, 7, 139, 389, 139, 119, 149, 91, 106, 7, 91, 6, 7, 91, 21, 128, 7, 1651, 85, 7, 141, 141, 7, 96, 85, 194, 6, 91, 21, 191, 352, 96, 7, 96, 12, 105, 96, 14, 91, 96, 96, 11, 7, 126, 268, 96, 91, 465, 7, 171, 11, 128, 85, 112, 22, 108, 11, 112, 7, 119, 105, 96, 6, 14, 88, 91, 21, 117, 113, 7, 12, 7, 21, 106, 21, 91, 176, 14, 507, 108, 88, 85, 7, 105, 7, 7, 88, 7, 369, 7, 22, 7, 91, 371, 106, 738, 195, 88, 7, 106, 155, 85, 537, 150, 148, 7, 106, 91, 176, 130, 7, 98, 14, 108, 21, 91, 141, 21, 98, 14, 128, 7, 141, 85, 7, 7, 281, 119, 85, 119, 135, 98, 22, 141, 141, 141, 141, 274, 21, 196, 91, 106, 20, 22, 7, 85, 113, 121, 98, 21, 106, 22, 113, 7, 98, 112, 91, 91, 6, 96, 7, 7, 111, 187, 140, 7, 21, 98, 117, 12, 105, 7, 21, 176, 96, 106, 98, 18, 133, 142, 88, 7, 115, 105, 378, 85, 91, 21, 7, 21, 7, 105, 105, 109, 7, 85, 88, 88, 1012, 224, 96, 88, 21, 6, 111, 128, 22, 85, 128, 21, 22, 21, 85, 12, 21, 18, 109, 6, 7, 22, 115, 115, 129, 154, 3, 2, 91, 11, 734, 85, 85, 7, 109, 91, 148, 119, 96, 96, 21, 461, 145, 91, 21, 5, 91, 22, 85, 106, 7, 106, 91, 109, 135, 171, 96, 85, 22, 21, 7, 161, 153, 7, 112, 7, 7, 153, 7, 6, 128, 5, 21, 6, 117, 85, 21, 112, 119, 139, 105, 88, 130, 12, 200, 468, 182, 139, 112, 153, 91, 302, 868, 148, 3, 2, 121, 22, 21, 6, 7, 113, 114, 21, 96, 96, 7, 96, 96, 7, 7, 109, 111, 117, 121, 435, 7, 139, 204, 133, 115, 154, 6, 85, 106, 7, 22, 85, 22, 139, 21, 98, 193, 7, 150, 1909, 3, 148, 210, 121, 109, 91, 85, 22, 96, 203, 85, 98, 142, 981, 129, 109, 111, 85, 109, 85, 111, 8, 111, 6, 111, 108, 654, 3, 173, 7, 85, 155, 21, 91, 98, 7, 109, 7, 22, 96, 7, 836, 105, 139, 119, 11, 7, 85, 149, 96, 5, 11, 22, 85, 91, 21, 96, 85, 91, 96, 85, 6, 22, 22, 142, 22, 135, 91, 21, 330, 22, 133, 117, 142, 88, 105, 21, 22, 85, 21, 22, 21, 22, 6, 105, 113, 14, 7, 6, 88, 7, 88, 6, 96, 11, 7, 6, 7, 6, 5, 91, 113, 7, 7, 6, 22, 21, 22, 91, 6, 5, 876, 7, 3, 176, 161, 121, 129, 21, 7, 20, 22, 88, 21, 112, 88, 21, 13, 7, 21, 22, 21, 108, 7, 238, 85, 7, 88, 7, 128, 11, 135, 91, 4672, 7, 91, 21, 22, 117, 113, 585, 518, 7, 139, 112, 88, 88, 85, 854, 598, 4573, 7, 294, 106, 7, 106, 20, 126, 115, 129, 265, 105, 121, 105, 85, 91, 20, 91, 22, 434, 105, 22, 98, 6, 96, 91, 223, 113, 7, 7, 18, 128, 88, 128, 85, 7, 117, 88, 21, 88, 221, 7, 85, 561, 146, 21, 22, 91, 7, 21, 22, 88, 222, 581, 581, 561, 22, 7, 85, 91, 121, 7, 7, 91, 21, 113, 21, 88, 112, 22, 91, 128, 7, 91, 21, 21, 126, 140, 539, 126, 88, 98, 22, 7, 268, 7, 21, 98, 22, 133, 106, 20, 21, 135, 98, 7, 7, 115, 111, 187, 161, 109, 21, 197, 7, 7, 142, 22, 7, 88, 7, 21, 88, 88, 21, 7, 11, 112, 96, 88, 112, 88, 7, 91, 22, 126, 176, 126, 186, 176, 85, 88, 6, 14, 140, 21, 85, 176, 98, 6, 7, 7, 5, 128, 140, 203, 126, 98, 7, 165, 333, 18, 142, 135, 88, 3, 129, 7, 7, 96, 21, 105, 221, 269, 128, 88, 7, 88, 88, 196, 85, 7, 22, 111, 109, 85, 98, 445, 141, 7, 130, 141, 119, 7, 96, 14, 413, 141, 128, 11, 149, 88, 141, 7, 21, 22, 157, 7, 21, 117, 7, 88, 148, 106, 21, 142, 115, 187, 1294, 199, 161, 85, 111, 250, 141, 7, 11, 169, 7, 7, 276, 7, 7, 7, 7, 7, 285, 117, 406, 169, 7, 130, 7, 186, 7, 237, 153, 7, 310, 121, 7, 18, 18, 310, 310, 7, 310, 279, 279, 279, 7, 130, 7, 106, 7, 150, 115, 161, 7, 139, 141, 7, 130, 6, 169, 14, 6, 7, 7, 14, 876, 112, 130, 130, 21, 96, 112, 22, 141, 154, 3, 121, 230, 197, 114, 121, 7, 133, 173, 106, 155, 91, 98, 22, 105, 21, 7, 114, 88, 112, 14, 7, 11, 7, 88, 108, 22, 140, 135, 96, 91, 98, 133, 91, 21, 111, 11, 126, 230, 148, 105, 21, 176, 148, 148, 187, 161, 111, 108, 22, 1804, 96, 8, 410, 85, 119, 7, 128, 85, 96, 106, 826, 115, 154, 7, 265, 525, 452, 195, 234, 498, 150, 7, 150, 7, 22, 154, 287, 112, 91, 126, 7, 18, 22, 7, 88, 109, 7, 274, 85, 7, 11, 171, 21, 243, 11, 22, 7, 106, 108, 7, 2, 161, 200, 204, 121, 85, 139, 936, 191, 936, 105, 130, 7, 130, 130, 141, 112, 21, 7, 7, 140, 91, 153, 153, 126, 141, 88, 7, 200, 154, 88, 88]  \n",
      "masked_token_ground_truth\n",
      " [2, 98, 105, 22, 21, 7, 494, 91, 113, 22, 96, 106, 91, 22, 135, 88, 7, 142, 119, 161, 85, 169, 5, 98, 157, 22, 22, 88, 96, 96, 153, 7, 98, 133, 145, 153, 105, 7, 5, 112, 96, 5, 22, 11, 119, 109, 96, 21, 109, 7, 126, 18, 169, 91, 22, 106, 21, 18, 121, 133, 191, 140, 12, 105, 109, 22, 114, 96, 105, 7, 140, 21, 150, 7, 5, 11, 7, 554, 85, 7, 129, 91, 915, 126, 115, 115, 129, 21, 6, 18, 22, 129, 1966, 161, 7, 7, 106, 21, 142, 98, 6, 105, 7, 150, 7, 91, 221, 150, 21, 117, 21, 105, 195, 195, 106, 195, 18, 22, 7, 314, 117, 12, 106, 98, 7, 85, 85, 98, 6, 96, 106, 88, 154, 14, 113, 197, 117, 119, 7, 12, 7, 119, 108, 7, 119, 108, 284, 3, 96, 117, 109, 173, 112, 96, 22, 7, 91, 21, 6, 14, 165, 146, 96, 5, 14, 148, 126, 11, 7, 91, 21, 6, 109, 112, 22, 21, 7, 1432, 112, 6, 128, 85, 88, 91, 7, 18, 22, 115, 146, 121, 21, 133, 98, 7, 98, 7, 113, 85, 11, 21, 6, 22, 96, 7, 105, 130, 6, 113, 88, 96, 7, 155, 142, 978, 98, 108, 115, 115, 114, 115, 494, 121, 85, 85, 98, 18, 7, 146, 21, 21, 179, 96, 7, 85, 88, 91, 260, 17, 6, 11, 22, 21, 748, 85, 85, 7, 7, 748, 85, 106, 111, 18, 91, 6, 7, 91, 18, 85, 91, 7, 274, 106, 18, 85, 85, 145, 22, 22, 179, 96, 22, 117, 85, 6, 18, 7, 91, 7, 21, 7, 115, 170, 154, 222, 7, 21, 21, 88, 7, 7, 179, 263, 96, 113, 22, 179, 91, 22, 7, 237, 7, 96, 263, 91, 105, 106, 22, 88, 115, 7, 21, 121, 18, 85, 21, 6, 219, 7, 106, 22, 244, 157, 21, 119, 7, 91, 85, 140, 21, 22, 219, 117, 91, 281, 6, 109, 7, 85, 7, 91, 6, 21, 6, 5, 91, 117, 91, 119, 133, 7, 108, 154, 161, 133, 334, 106, 6, 98, 6, 7, 297, 155, 20, 142, 22, 96, 7, 22, 91, 98, 21, 22, 96, 91, 21, 98, 149, 85, 157, 85, 7, 22, 112, 21, 153, 22, 85, 128, 22, 200, 109, 200, 112, 7, 1390, 98, 6, 22, 133, 96, 88, 7, 105, 157, 4053, 3, 114, 22, 986, 6, 85, 106, 98, 88, 7, 85, 111, 2324, 22, 85, 7, 126, 527, 109, 22, 142, 115, 7, 7, 21, 91, 7450, 4032, 2, 85, 111, 130, 140, 133, 21, 114, 21, 22, 317, 85, 117, 91, 413, 145, 88, 6, 587, 169, 114, 119, 112, 91, 114, 139, 7, 21, 119, 2561, 119, 130, 18, 96, 22, 130, 22, 91, 21, 130, 139, 141, 85, 130, 7, 85, 7, 98, 7, 109, 105, 109, 126, 7, 105, 121, 121, 105, 6563, 7, 106, 22, 108, 456, 7, 88, 7, 96, 379, 96, 276, 153, 7, 98, 1170, 21, 98, 5151, 141, 179, 91, 130, 7, 128, 215, 128, 140, 126, 141, 11, 215, 114, 85, 91, 21, 142, 106, 155, 20, 1568, 8, 85, 115, 18, 22, 7318, 333, 161, 113, 197, 117, 12, 98, 85, 106, 7, 85, 98, 237, 215, 149, 171, 5, 96, 7, 130, 7, 21, 11, 130, 215, 135, 88, 113, 21, 199, 176, 6, 91, 21, 973, 7, 184, 88, 7, 117, 199, 85, 7, 224, 96, 88, 22, 7, 130, 130, 7, 21, 7, 153, 200, 112, 113, 7, 85, 109, 145, 130, 454, 85, 7, 21, 12, 7, 7, 106, 20, 22, 21, 7, 139, 85, 96, 21, 21, 96, 10, 506, 108, 85, 21, 98, 7, 21, 7, 375, 108, 14, 111, 386, 22, 7, 142, 108, 7, 343, 22, 157, 22, 85, 7, 119, 85, 85, 7, 149, 105, 111, 21, 7, 21, 654, 7, 88, 222, 91, 6, 7, 222, 22, 7, 21, 22, 85, 7, 21, 142, 98, 7, 3, 161, 105, 21, 98, 22, 650, 119, 12, 18, 10, 7, 88, 117, 121, 21, 22, 7, 11, 105, 108, 113, 108, 7, 179, 7, 119, 7, 91, 22, 179, 130, 7, 154, 22, 21, 18, 96, 8, 22, 22, 113, 7, 126, 98, 7, 85, 111, 115, 208, 2, 161, 111, 98, 11, 7, 7, 85, 12, 85, 88, 7, 91, 113, 91, 295, 85, 139, 88, 105, 11, 247, 247, 121, 111, 133, 98, 186, 171, 91, 21, 88, 126, 7, 88, 6, 140, 22, 109, 230, 186, 135, 7, 115, 148, 148, 187, 7, 85, 7, 20, 98, 105, 7, 112, 98, 6, 182, 96, 119, 7, 385, 88, 96, 141, 139, 85, 204, 88, 7, 22, 91, 21, 88, 383, 22, 378, 106, 142, 106, 6, 22, 88, 98, 22, 199, 128, 105, 11, 7, 648, 7, 88, 119, 250, 139, 439, 6, 126, 140, 1118, 187, 161, 112, 22, 113, 119, 7, 7, 119, 191, 182, 6, 96, 7, 105, 171, 6, 96, 21, 6, 91, 6, 6, 96, 14, 7, 85, 7, 129, 114, 111, 105, 85, 7, 85, 6, 85, 114, 185, 108, 187, 85, 109, 111, 187, 105, 109, 7, 21, 7, 126, 7, 109, 7, 88, 7, 106, 126, 108, 115, 129, 208, 85, 109, 153, 96, 85, 109, 319, 85, 91, 91, 6, 126, 7, 18, 105, 85, 121, 106, 91, 6, 7, 109, 281, 133, 119, 119, 21, 22, 88, 165, 85, 157, 7, 85, 98, 7, 7, 165, 182, 129, 96, 96, 7, 91, 196, 3, 418, 105, 386, 91, 343, 106, 91, 142, 22, 85, 7, 98, 112, 234, 21, 22, 150, 337, 193, 193, 91, 22, 91, 119, 7, 88, 126, 238, 98, 6, 232, 135, 7, 223, 129, 105, 114, 7, 21, 6, 18, 7, 112, 113, 7, 2004, 171, 129, 128, 199, 18, 108, 115, 154, 119, 7, 165, 7, 133, 106, 22, 7, 141, 6, 141, 6, 128, 139, 112, 7, 6, 14, 112, 91, 96, 6, 8, 7, 128, 7, 105, 133, 14, 7, 85, 165, 7, 6, 7, 5, 7, 133, 98, 114, 182, 105, 7, 21, 7, 7, 85, 105, 7, 111, 22, 91, 295, 8, 88, 148, 117, 12, 85, 113, 7, 117, 12, 176, 223, 161, 121, 21, 18, 108, 141, 14, 85, 119, 7, 117, 7, 200, 7, 85, 85, 14, 7, 119, 88, 7, 85, 117, 200, 7, 14, 141, 284, 284, 2, 7, 18, 7, 18, 7, 85, 91, 21, 22, 106, 7, 108, 7, 91, 18, 145, 88, 115, 111, 145, 7, 130, 85, 195, 153, 5, 22, 145, 7, 182, 153, 21, 21, 6, 5, 22, 128, 204, 105, 11, 352, 112, 195, 6, 98, 154, 7, 112, 145, 7, 4279, 270, 7, 7, 3904, 270, 106, 7, 109, 7, 106, 22, 85, 3889, 22, 148, 3, 1309, 105, 7, 106, 119, 14, 200, 7, 7, 11, 7, 115, 161, 121, 121, 133, 142, 88, 422, 109, 113, 7, 106, 882, 22, 7, 21, 7, 88, 21, 6, 351, 111, 85, 6, 191, 85, 21, 6, 98, 8849, 133, 20, 135, 7, 88, 3, 22, 91, 112, 21, 5, 11, 22, 141, 105, 119, 126, 219, 200, 7, 22, 126, 91, 153, 149, 154, 96, 229, 1752, 109, 687, 149, 106, 21, 109, 173, 7, 774, 85, 135, 1241, 6, 18, 91, 21, 153, 106, 91, 130, 6, 85, 113, 130, 91, 6, 106, 91, 7, 106, 91, 7, 169, 7, 21, 91, 21, 106, 154, 18, 96, 109, 85, 21, 96, 7, 96, 96, 22, 85, 130, 130, 173, 96, 106, 148, 21, 96, 157, 153, 7, 113, 7, 85, 21, 149, 21, 7, 197, 20, 21, 98, 105, 98, 221, 22, 157, 7, 204, 21, 7, 155, 108, 154, 22, 85, 111, 105, 12, 105, 21, 113, 119, 4648, 284, 85, 7, 114, 85, 119, 88, 85, 117, 88, 85, 115, 88, 88, 115, 115, 3, 3, 161, 121, 85, 140, 141, 91, 21, 14, 109, 108, 7, 112, 22, 96, 21, 8, 115, 105, 3, 2, 106, 21, 91, 98, 18, 7, 98, 109, 586, 22, 105, 7, 496, 113, 6, 88, 7, 14, 7, 8433, 7, 7, 91, 96, 154, 161, 618, 757, 21, 22, 157, 91, 1788, 22, 21, 21, 157, 7, 4065, 1387, 1192, 662, 126, 133, 173, 22, 117, 7, 85, 106, 155, 7, 14, 7, 88, 96, 7, 803, 7, 182, 85, 285, 133, 88, 88, 21, 14, 153, 153, 14, 126, 88, 7, 141, 18, 22, 96, 155, 880, 88, 173, 208, 3, 121, 85, 7, 7, 193, 22, 470, 11, 7, 112, 2020, 85, 7, 12, 2970, 130, 22, 6, 113, 193, 91, 113, 130, 7, 357, 91, 6, 130, 157, 7, 91, 130, 6, 96, 18, 22, 7, 21, 6, 22, 7, 112, 21, 191, 6, 5, 209, 105, 96, 7, 194, 112, 91, 22, 91, 21, 7, 91, 22, 7, 85, 21, 85, 1027, 7, 85, 105, 121, 7, 105, 21, 121, 7, 184, 85, 934, 130, 18, 3, 3, 121, 265, 12, 88, 113, 111, 105, 21, 106, 106, 88, 22, 7, 121, 106, 7, 108, 121, 543, 85, 85, 284, 330, 106, 85, 12, 284, 126, 7, 108, 115, 119, 7, 7, 22, 106, 7, 108, 21, 6, 109, 105, 7, 85, 109, 7, 106, 85, 6, 142, 22, 7, 22, 115, 91, 108, 7, 117, 121, 853, 200, 711, 21, 11, 200, 128, 184, 6, 226, 141, 7, 88, 204, 912, 182, 130, 7, 21, 119, 6, 8, 11, 5, 91, 130, 179, 7, 5, 130, 119, 85, 847, 112, 141, 126, 130, 6, 459, 250, 85, 7, 91, 109, 14, 21, 22, 7, 7, 182, 146, 121, 129, 146, 114, 6, 111, 22, 91, 22, 2645, 109, 199, 7, 5, 172, 7, 21, 11, 96, 106, 96, 12, 6, 18, 170, 21, 22, 21, 5, 22, 108, 139, 140, 146, 85, 96, 3617, 21, 179, 12, 792, 18, 129, 88, 91, 7, 22, 105, 197, 105, 22, 91, 21, 309, 7, 133, 7, 128, 204, 7, 119, 187, 140, 115, 115, 154, 161, 121, 117, 7, 7, 21, 153, 22, 91, 21, 6, 451, 106, 18, 130, 111, 114, 3, 21, 109, 6, 1379, 22, 88, 193, 195, 193, 21, 7, 193, 21, 98, 7, 341, 139, 117, 465, 7, 332, 85, 88, 12, 91, 435, 109, 21, 98, 6, 7, 7, 98, 439, 332, 85, 22, 105, 277, 22, 85, 117, 389, 145, 204, 1518, 117, 157, 130, 22, 7, 22, 106, 91, 135, 3721, 2119, 91, 21, 7, 6, 105, 113, 88, 85, 22, 91, 4276, 133, 557, 155, 98, 1582, 22, 7, 98, 91, 157, 6, 193, 98, 22, 585, 21, 831, 109, 457, 85, 208, 106, 108, 88, 7, 88, 126, 7, 161, 22, 129, 105, 114, 22, 106, 146, 169, 129, 85, 7, 18, 108, 88, 128, 7, 873, 18, 128, 11, 191, 5308, 121, 7, 8, 7, 114, 7, 262, 7, 105, 7, 119, 119, 117, 126, 128, 7, 85, 7, 85, 7, 210, 135, 111, 169, 85, 7, 7, 21, 7, 21, 117, 114, 3898, 3, 2, 161, 21, 7, 85, 111, 109, 85, 139, 195, 21, 219, 22, 193, 91, 157, 128, 135, 186, 8, 200, 186, 7, 191, 1794, 6, 21, 22, 112, 11, 91, 96, 140, 141, 7, 96, 5, 7, 141, 128, 96, 459, 7, 7, 200, 7, 128, 21, 6, 11, 21, 153, 929, 119, 96, 21, 194, 126, 200, 21, 85, 200, 115, 115, 3, 22, 121, 108, 7, 133, 142, 96, 7, 155, 98, 21, 199, 7, 109, 113, 21, 6, 7, 678, 12, 88, 247, 117, 179, 91, 6, 126, 117, 98, 18, 22, 11, 85, 223, 7, 7, 91, 173, 106, 91, 20, 21, 22, 3, 2, 129, 133, 7, 117, 119, 7, 22, 108, 88, 7, 111, 114, 115, 129, 113, 6, 114, 108, 296, 210, 85, 203, 7, 21, 109, 14, 98, 7, 149, 11, 111, 22, 96, 7, 12, 141, 409, 126, 7, 11, 7, 91, 112, 148, 472, 126, 91, 8, 88, 140, 176, 7, 7, 91, 21, 88, 119, 153, 7, 98, 8, 11, 7, 130, 7, 128, 85, 7, 91, 209, 238, 145, 406, 172, 179, 21, 6, 7, 135, 96, 106, 21, 20, 154, 3, 7, 96, 111, 128, 7, 111, 115, 129, 112, 6, 14, 149, 129, 7, 1279, 5, 85, 115, 117, 128, 85, 112, 14, 88, 91, 22, 161, 85, 21, 139, 21, 85, 7, 4675, 130, 126, 11, 1067, 21, 18, 21, 22, 139, 85, 109, 7, 141, 7, 3122, 200, 11, 1069, 518, 3955, 91, 22, 85, 21, 153, 109, 133, 98, 7, 6, 8, 18, 109, 7, 7, 9, 7, 204, 11, 3955, 903, 106, 91, 6, 142, 3602, 139, 112, 128, 7, 268, 148, 121, 7, 418, 121, 378, 85, 7, 155, 20, 21, 7, 88, 21, 22, 145, 91, 6, 11, 22, 85, 21, 171, 96, 6, 128, 117, 88, 91, 324, 7, 378, 6, 7, 106, 22, 98, 22, 148, 7, 7, 12, 85, 119, 15, 117, 119, 7, 85, 117, 7, 88, 117, 284, 119, 88, 7, 108, 3, 154, 117, 21, 140, 133, 386, 109, 115, 85, 109, 157, 85, 21, 85, 22, 7, 109, 149, 7, 106, 18, 7, 98, 91, 6, 105, 7, 91, 6, 98, 85, 7, 139, 7, 7, 7, 11, 22, 141, 130, 130, 7, 126, 114, 154, 708, 7, 117, 88, 7, 161, 232, 142, 20, 21, 85, 22, 194, 112, 91, 7, 4484, 96, 22, 7, 85, 153, 91, 128, 993, 91, 209, 22, 470, 193, 22, 106, 298, 91, 21, 96, 195, 22, 7, 85, 7, 142, 91, 22, 7, 91, 169, 7, 169, 135, 106, 20, 96, 7, 109, 7, 343, 121, 106, 22, 106, 91, 18, 22, 96, 295, 22, 85, 113, 88, 21, 7, 22, 21, 6, 85, 85, 98, 140, 165, 22, 7, 126, 117, 260, 96, 140, 140, 7, 140, 7, 7, 6, 113, 5, 96, 165, 91, 22, 1951, 105, 22, 91, 140, 6, 96, 7, 117, 85, 112, 22, 88, 135, 269, 7, 109, 96, 21, 18, 133, 22, 135, 126, 115, 7, 133, 106, 91, 155, 20, 142, 98, 6, 7, 108, 88, 88, 105, 22, 22, 199, 112, 91, 21, 22, 22, 21, 119, 96, 7, 105, 6, 5, 12, 204, 552, 140, 22, 7, 106, 155, 656, 146, 133, 85, 106, 91, 98, 22, 7, 88, 7, 12, 7, 106, 85, 85, 7, 171, 112, 96, 5, 333, 947, 7, 140, 85, 12, 105, 91, 133, 91, 135, 7, 108, 7, 88, 129, 6, 247, 85, 7, 114, 108, 7, 88, 260, 85, 295, 822, 22, 121, 96, 3483, 126, 91, 105, 21, 6, 7, 11, 128, 149, 1694, 85, 22, 186, 7, 410, 96, 96, 85, 7, 139, 457, 139, 119, 332, 91, 106, 7, 91, 6, 7, 91, 21, 128, 7, 1651, 85, 7, 344, 141, 7, 96, 85, 244, 6, 91, 21, 204, 171, 96, 7, 96, 12, 105, 96, 14, 91, 96, 96, 11, 7, 126, 181, 153, 91, 2340, 7, 171, 11, 128, 85, 112, 22, 85, 200, 112, 7, 141, 105, 194, 6, 14, 88, 91, 21, 200, 113, 7, 12, 7, 21, 106, 21, 91, 176, 14, 672, 108, 88, 85, 7, 105, 7, 7, 88, 7, 369, 7, 22, 7, 91, 255, 106, 534, 195, 88, 7, 106, 155, 85, 1018, 150, 148, 7, 106, 91, 187, 130, 7, 98, 14, 108, 21, 91, 141, 21, 98, 14, 128, 7, 88, 85, 7, 7, 281, 119, 85, 119, 135, 98, 22, 141, 139, 141, 11, 1007, 21, 196, 91, 106, 20, 22, 7, 85, 113, 121, 98, 21, 106, 22, 113, 7, 98, 112, 91, 91, 6, 96, 7, 7, 111, 187, 140, 7, 21, 98, 117, 12, 105, 7, 21, 176, 96, 106, 98, 18, 133, 142, 88, 7, 115, 105, 378, 85, 91, 21, 7, 21, 7, 105, 105, 109, 7, 85, 88, 88, 1012, 112, 96, 88, 21, 6, 111, 128, 22, 85, 128, 21, 22, 21, 85, 12, 21, 185, 109, 6, 7, 22, 115, 115, 129, 154, 3, 2, 91, 12, 761, 85, 250, 7, 113, 91, 135, 119, 96, 109, 21, 461, 145, 91, 21, 5, 91, 22, 85, 106, 7, 106, 91, 109, 149, 171, 96, 85, 22, 21, 7, 161, 153, 7, 112, 7, 7, 153, 7, 6, 128, 5, 21, 6, 117, 85, 21, 112, 119, 88, 105, 88, 130, 12, 200, 503, 182, 88, 112, 153, 91, 302, 1792, 148, 3, 2, 121, 22, 21, 6, 7, 96, 114, 21, 96, 96, 7, 96, 111, 7, 7, 109, 111, 117, 121, 454, 7, 139, 181, 126, 115, 154, 6, 85, 106, 7, 22, 85, 22, 139, 21, 98, 193, 7, 150, 1909, 3, 148, 210, 121, 109, 91, 85, 22, 96, 203, 85, 98, 142, 912, 129, 113, 114, 85, 109, 85, 114, 8, 146, 6, 109, 108, 1119, 3, 173, 7, 85, 155, 21, 91, 98, 7, 109, 7, 22, 330, 7, 3410, 105, 139, 119, 11, 7, 85, 149, 96, 5, 12, 22, 85, 91, 21, 96, 85, 91, 130, 85, 6, 22, 22, 142, 22, 135, 91, 21, 330, 22, 133, 139, 142, 88, 105, 21, 22, 85, 21, 22, 21, 22, 6, 105, 109, 14, 7, 6, 88, 7, 88, 6, 96, 11, 7, 6, 7, 6, 5, 91, 96, 7, 7, 6, 22, 21, 22, 91, 6, 5, 459, 7, 3, 176, 161, 121, 129, 21, 7, 20, 22, 88, 21, 112, 88, 21, 13, 7, 21, 22, 21, 108, 7, 1560, 85, 7, 88, 7, 128, 11, 135, 91, 15984, 7, 91, 21, 22, 117, 96, 585, 518, 7, 139, 112, 88, 88, 85, 598, 529, 8616, 7, 1091, 106, 7, 106, 20, 126, 115, 129, 870, 105, 121, 105, 85, 91, 20, 91, 22, 1037, 105, 22, 98, 6, 96, 91, 223, 113, 7, 7, 221, 128, 88, 128, 85, 7, 117, 88, 21, 88, 157, 7, 85, 561, 146, 21, 22, 91, 7, 21, 22, 88, 223, 581, 581, 561, 22, 7, 85, 91, 121, 7, 7, 91, 21, 113, 21, 88, 112, 22, 91, 128, 7, 91, 21, 21, 126, 140, 540, 126, 88, 98, 22, 7, 268, 7, 21, 98, 22, 133, 106, 20, 21, 135, 98, 7, 7, 115, 111, 187, 161, 109, 21, 197, 7, 7, 142, 22, 7, 88, 7, 21, 88, 88, 21, 7, 11, 112, 96, 88, 112, 88, 7, 91, 22, 126, 176, 126, 186, 176, 85, 88, 6, 14, 165, 21, 85, 176, 98, 6, 7, 7, 5, 128, 140, 203, 126, 98, 7, 165, 333, 18, 142, 135, 88, 3, 129, 7, 7, 96, 21, 105, 157, 98, 128, 88, 7, 88, 88, 493, 85, 7, 22, 146, 109, 85, 98, 413, 141, 7, 114, 141, 119, 7, 114, 11, 367, 88, 128, 11, 149, 88, 141, 7, 21, 22, 185, 7, 21, 117, 7, 88, 148, 106, 21, 142, 115, 196, 1492, 199, 161, 85, 111, 250, 200, 7, 184, 302, 7, 7, 237, 7, 7, 7, 7, 7, 173, 130, 770, 130, 7, 130, 7, 119, 7, 237, 153, 7, 85, 7, 7, 173, 18, 310, 310, 7, 13997, 279, 279, 126, 7, 130, 7, 106, 7, 150, 115, 161, 7, 15, 139, 7, 130, 6, 169, 14, 6, 7, 7, 14, 459, 112, 130, 130, 21, 96, 112, 22, 141, 154, 3, 121, 230, 265, 114, 121, 7, 133, 173, 106, 155, 91, 98, 22, 105, 21, 7, 114, 88, 112, 14, 7, 11, 7, 88, 108, 22, 18, 326, 96, 91, 98, 133, 91, 21, 111, 11, 126, 140, 148, 105, 21, 176, 148, 148, 187, 161, 111, 108, 22, 1804, 96, 8, 294, 85, 119, 7, 128, 85, 96, 106, 826, 115, 154, 7, 197, 755, 1951, 195, 606, 498, 150, 7, 195, 7, 22, 154, 287, 112, 91, 126, 7, 18, 22, 7, 88, 109, 7, 960, 85, 7, 11, 171, 21, 243, 11, 22, 7, 106, 108, 7, 2, 161, 139, 204, 121, 108, 302, 4003, 181, 4003, 105, 130, 7, 109, 130, 140, 112, 21, 7, 135, 829, 91, 153, 130, 126, 11, 302, 7, 139, 154, 200, 88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 200/200 [01:51<00:00,  1.80it/s]\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation:  Instruction F1:  0.0    v_masked_token_ F1:  0.848858677869018 v_seq_accuracy:  0.9932713812154697  v_masked_token_precision :  0.8480618293382636 v_masked_token_recall: 0.8537341031080815  V SEQ F1:  0.9931578700576241\n",
      "v_masked_token_accuracy 0.8537341031080815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "counter = 0\n",
    "\n",
    "global_instruction_metrices = []\n",
    "global_masked_token_metrices = []\n",
    "\n",
    "v_global_instruction_metrices = []\n",
    "v_global_masked_token_metrices = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\n",
    "    ###########################################\n",
    "    ###############  EVAL Validation  #########\n",
    "    ###########################################\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "#         v_predictions_all, v_ground_truths_all = None, None\n",
    "        \n",
    "        v_instruction_predictions_all, v_instruction_ground_truths_all = None, None\n",
    "        v_masked_token_predictions_all, v_masked_token_ground_truths_all = None, None\n",
    "        v_seq_predictions_all, v_seq_ground_truths_all = None, None\n",
    "    \n",
    "    \n",
    "        validation_loop = tqdm(validation_loader, leave=True)\n",
    "        for N,v_batch in enumerate(validation_loop):\n",
    "            \n",
    "            \n",
    "            \n",
    "            v_input_ids = v_batch['input_ids'].to(device)\n",
    "            v_token_type_ids = v_batch['token_type_ids'].to(device)\n",
    "            v_attention_mask = v_batch['attention_mask'].to(device)\n",
    "            v_next_sentence_label = v_batch['next_sentence_label'].to(device)\n",
    "            v_mask_arr = v_batch ['mask_arr']\n",
    "            v_labels = v_batch['labels'].to(device)\n",
    "            # process\n",
    "            v_outputs = model(v_input_ids, attention_mask=v_attention_mask,\n",
    "                            token_type_ids=v_token_type_ids,\n",
    "                            next_sentence_label=v_next_sentence_label,\n",
    "                            labels=v_labels)\n",
    "        \n",
    "            v_token_prediction = torch.argmax(v_outputs.prediction_logits, axis=-1)\n",
    "\n",
    "                    \n",
    "\n",
    "            v_batch_masks =   [ torch.flatten(bm.nonzero()).tolist()  for bm in v_mask_arr]\n",
    "            \n",
    "            v_masked_token_prediction = [ token[v_batch_masks[t]].tolist() for t,token in enumerate(v_token_prediction) ]\n",
    "            v_masked_token_prediction = list(chain.from_iterable(v_masked_token_prediction))\n",
    "            \n",
    "            v_masked_token_ground_truth   = [ token[v_batch_masks[t]].tolist() for t,token in enumerate(v_labels) ]\n",
    "            v_masked_token_ground_truth = list(chain.from_iterable(v_masked_token_ground_truth))\n",
    "\n",
    "            if N==0:\n",
    "\n",
    "                print('token_prediction\\n',v_token_prediction, '\\n input_ids \\n',v_input_ids)\n",
    "    \n",
    "                print('masked_token_prediction\\n: ',v_masked_token_prediction , ' \\nmasked_token_ground_truth\\n', v_masked_token_ground_truth)\n",
    "            \n",
    "\n",
    "            filteredP =[]\n",
    "            filteredG = []\n",
    "            for gi,g in enumerate(v_masked_token_ground_truth):\n",
    "                if g!=0:\n",
    "                    filteredG.append(v_masked_token_ground_truth [gi])\n",
    "                    filteredP.append(v_masked_token_prediction[gi])\n",
    "\n",
    "            v_masked_token_ground_truth = filteredG\n",
    "            v_masked_token_prediction = filteredP\n",
    "\n",
    "            v_seq_prediction = v_token_prediction.detach().cpu().numpy().flatten()\n",
    "            v_seq_ground_truth = v_labels.detach().cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "                \n",
    "            # token_prediction = [ token[batch_masks[t]].tolist() for t,token in enumerate(token_prediction) ]\n",
    "            # token_prediction = list(chain.from_iterable(token_prediction))\n",
    "            \n",
    "            # token_ground_truth   = [ token[batch_masks[t]].tolist() for t,token in enumerate(labels) ]\n",
    "            # token_ground_truth = list(chain.from_iterable(token_ground_truth))\n",
    "\n",
    "            \n",
    "            v_instruction_prediction = torch.argmax(v_outputs.seq_relationship_logits, axis=-1)\n",
    "            v_instruction_prediction   = v_instruction_prediction.detach().cpu().numpy().flatten()\n",
    "            v_instruction_ground_truth = v_next_sentence_label.detach().cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            if N==0:\n",
    "                v_instruction_predictions_all   = v_instruction_prediction\n",
    "                v_instruction_ground_truths_all = v_instruction_ground_truth\n",
    "\n",
    "                v_masked_token_predictions_all   = v_masked_token_prediction\n",
    "                v_masked_token_ground_truths_all = v_masked_token_ground_truth\n",
    "                \n",
    "                v_seq_predictions_all= v_seq_prediction\n",
    "                v_seq_ground_truths_all = v_seq_ground_truth\n",
    "\n",
    "        \n",
    "\n",
    "            else:\n",
    "                v_instruction_predictions_all   = np.concatenate((v_instruction_predictions_all, v_instruction_prediction))\n",
    "                v_instruction_ground_truths_all = np.concatenate((v_instruction_ground_truths_all, v_instruction_ground_truth))\n",
    "\n",
    "                v_masked_token_predictions_all   = np.concatenate((v_masked_token_predictions_all, v_masked_token_prediction ))\n",
    "                v_masked_token_ground_truths_all = np.concatenate((v_masked_token_ground_truths_all, v_masked_token_ground_truth ))\n",
    "                \n",
    "                v_seq_predictions_all =np.concatenate((v_seq_predictions_all, v_seq_prediction ))\n",
    "                v_seq_ground_truths_all =np.concatenate((v_seq_ground_truths_all, v_seq_ground_truth ))\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "        v_instruction_accuracy = (accuracy_score(v_instruction_ground_truths_all,v_instruction_predictions_all))\n",
    "        v_instruction_precision, v_instruction_recall, v_instruction_f1, _ = precision_recall_fscore_support(v_instruction_ground_truths_all,v_instruction_predictions_all, average='binary')\n",
    "\n",
    "\n",
    "        v_masked_token_accuracy = (accuracy_score(v_masked_token_ground_truths_all, v_masked_token_predictions_all))\n",
    "        v_masked_token_precision, v_masked_token_recall, v_masked_token_f1, _ = precision_recall_fscore_support(v_masked_token_ground_truths_all,v_masked_token_predictions_all,average='weighted')\n",
    "\n",
    "\n",
    "        v_seq_accuracy = (accuracy_score(v_seq_predictions_all, v_seq_ground_truths_all))\n",
    "        v_seq_precision, v_seq_recall, v_seq_f1, _ = precision_recall_fscore_support(v_seq_ground_truths_all,v_seq_predictions_all,average='weighted')\n",
    "    \n",
    "        print(epoch,\"Validation: \", \"Instruction F1: \", v_instruction_f1,  \"   v_masked_token_ F1: \",v_masked_token_f1,'v_seq_accuracy: ',v_seq_accuracy,' v_masked_token_precision : ',v_masked_token_precision,'v_masked_token_recall:', v_masked_token_recall,\" V SEQ F1: \", v_seq_f1)\n",
    "        print(\"v_masked_token_accuracy\" ,v_masked_token_accuracy)\n",
    "        v_global_instruction_metrices.append(v_instruction_f1)\n",
    "        v_global_masked_token_metrices.append(v_masked_token_f1) \n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [112, 91, 27868, 27868, 158, 116, 27868, 91, 91, 27868, 27867, 21, 27867, 27868, 27868, 27868, 91, 27868, 109, 21, 27868, 7, 27868, 22, 27867, 27868, 27867, 987, 27868, 1083, 27868, 36, 2, 7, 27868, 112, 27867, 7, 133, 7, 99, 21, 91, 6, 27867, 27867, 85, 129, 27868, 109, 27868, 27868, 27867, 27868, 7, 146, 27868, 235, 987, 27867, 27868, 7, 27868, 5, 27867, 27868, 27868, 27867, 337, 27867, 27868, 22, 987, 85, 27868, 27868, 1156, 85, 7, 27868, 99, 27868, 99, 21, 27868, 910, 27868, 20, 22, 27867, 104, 157, 27867, 27867, 27868, 27868, 7, 219, 119, 27868, 27867, 27868, 27868, 27868, 7, 27867, 762, 27867, 27868, 27868, 21, 22, 27867, 27868, 27867, 241, 109, 27868, 168, 105, 27868, 522, 27867, 85, 99, 27868, 27868, 130, 27867, 27868, 27868, 105, 118, 118, 27868, 27867, 118, 147, 168, 27868, 510, 27868, 27868, 130, 7, 168, 168, 27868, 7, 112, 133, 99, 7, 27868, 158, 27868, 27868, 22, 27868, 27868, 7, 1083, 27867, 21, 6, 27868, 7, 7, 27868, 7, 94, 228, 510, 7, 27868, 27867, 111, 349, 151, 151, 151, 151, 188, 27868, 151, 151, 241, 27868, 366, 407, 27867, 108, 27868, 119, 27868, 27867, 94, 27867, 119, 27867, 6, 99, 27867, 118, 27868, 27867, 123, 105, 116, 91, 27868, 91, 108, 27868, 21, 131, 27867, 111, 91, 920, 214, 27867, 7, 27868, 27868, 27867, 27867, 920, 27867, 27867, 920, 105, 121, 7, 27867, 27868, 236, 27868, 27868, 119, 123, 27868, 7, 27868, 129, 22, 762, 108, 21, 27868, 27868, 99, 27868, 27868, 6, 99, 22, 214, 91, 85, 203, 27868, 5, 85, 27867, 27868, 7, 91, 27867, 21, 131, 22, 91, 85, 27868, 91, 27868, 7, 27867, 21, 22, 85, 27868, 94, 21, 105, 22, 21, 22, 920, 91, 27868, 27867, 94, 27868, 6, 495, 219, 105, 117, 27867, 130, 27868, 114, 108, 109, 94, 22, 85, 27868, 91, 114, 27867, 7, 27867, 27868, 112, 910, 129, 7, 129, 203, 27868, 99, 6, 27868, 27868, 131, 27867, 85, 99, 27868, 6, 27867, 27868, 99, 20, 27867, 157, 157, 27868, 2, 27868, 27868, 112, 27867, 27868, 85, 27868, 910, 27868, 966, 27868, 91, 27868, 27868, 7, 27867, 27868, 203, 27868, 21, 27868, 948, 27867, 116, 27867, 27867, 21, 655, 214, 27868, 6, 27868, 99, 214, 27867, 85, 85, 85, 27868, 94, 85, 99, 21, 27868, 27868, 27868, 99, 91, 123, 27868, 27868, 522, 203, 27868, 27867, 27868, 27867, 85, 27868, 109, 27868, 6, 27868, 27868, 27868, 27868, 21, 27867, 21, 6, 27868, 27868, 131, 131, 146, 27868, 114, 6, 145, 109, 7, 27868, 27868, 22, 116, 119, 111, 91, 27867, 27868, 94, 27868, 7, 105, 7, 109, 94, 27868, 168, 91, 105, 27867, 118, 118, 147, 105, 119, 27867, 7, 99, 27867, 27867, 27868, 91, 7, 94, 27868, 236, 27867, 6, 27868, 105, 99, 170, 27868, 337, 171, 91, 21, 156, 22, 27867, 94, 199, 85, 91, 21, 131, 7, 27868, 20, 27867, 27868, 27867, 27867, 112, 7, 27868, 1083, 27868, 123, 109, 112, 7, 27868, 21, 6, 27868, 85, 219, 85, 27868, 94, 27868, 27867, 85, 7, 85, 27868, 7, 27867, 179, 94, 22, 27868, 6, 21, 27868, 99, 27867, 22, 115, 21, 156, 85, 22, 27868, 27868, 21, 116, 7, 7, 27868, 22, 7, 91, 99, 27868, 99, 94, 27868, 91, 27868, 27867, 156, 42, 27867, 27868, 3, 3, 109, 27868, 27868, 76, 27868, 37, 131, 109, 85, 94, 27868, 109, 158, 27868, 27868, 37, 21, 27868, 6, 37, 22, 7, 21, 21, 27868, 27868, 27868, 99, 27868, 119, 21, 99, 85, 7, 27868, 187, 187, 27868, 22, 27868, 21, 131, 129, 115, 27867, 7, 85, 27868, 156, 27868, 7, 27867, 463, 27867, 129, 91, 27868, 91, 235, 27867, 27868, 22, 138, 6, 22, 133, 27868, 27868, 157, 620, 510, 7, 27867, 27867, 27868, 495, 27868, 91, 105, 27867, 27868, 7, 21, 966, 133, 27867, 27868, 99, 7, 85, 27868, 27868, 27868, 6, 99, 27867, 91, 7, 27867, 692, 27868, 21, 5, 114, 104, 105, 27867, 91, 171, 27867, 21, 27868, 910, 109, 27867, 27867, 118, 27867, 157, 114, 108, 116, 27867, 27868, 112, 27867, 27868, 108, 27867, 3, 85, 7, 27867, 85, 27868, 27867, 27868, 21, 112, 112, 114, 112, 27868, 130, 85, 94, 112, 27868, 1380, 85, 27868, 7, 27868, 117, 27868, 117, 115, 21, 22, 115, 94, 27868, 966, 27867, 214, 21, 5, 112, 27868, 910, 27867, 156, 7, 27867, 115, 94, 6, 22, 620, 99, 27868, 27867, 27868, 27868, 27868, 7, 27868, 123, 27868, 655, 27868, 7, 118, 27868, 114, 108, 22, 91, 27868, 7, 27868, 114, 162, 21, 27868, 105, 108, 85, 112, 219, 910, 27868, 27867, 146, 27867, 27868, 112, 112, 27868, 99, 7, 203, 27868, 27868, 99, 99, 1083, 156, 7, 27868, 27867, 948, 7, 270, 27868, 156, 99, 129, 214, 131, 131, 114, 620, 114, 85, 401, 7, 27868, 27868, 156, 27868, 27868, 27867, 94, 27868, 7, 27867, 27868, 27867, 109, 21, 6, 85, 94, 20, 495, 27867, 108, 7, 6, 27868, 118, 114, 270, 99, 6, 495, 104, 2, 123, 91, 133, 109, 27868, 94, 21, 966, 111, 91, 91, 27868, 27867, 105, 105, 27868, 105, 214, 27867, 146, 7, 199, 199, 21, 27868, 522, 27868, 99, 22, 91, 27868, 21, 7, 91, 27867, 99, 85, 99, 27868, 6, 27868, 22, 94, 27868, 21, 27868, 91, 27867, 104, 74, 27868, 105, 119, 7, 112, 99, 27867, 27868, 27867, 85, 27868, 85, 27868, 27868, 146, 105, 27867, 1026, 27867, 27868, 99, 27867, 131, 27868, 6, 78, 156, 85, 27868, 85, 27868, 21, 99, 22, 7, 112, 94, 21, 27868, 27868, 27867, 7, 27868, 27868, 22, 219, 27868, 158, 85, 27868, 7, 105, 7, 85, 112, 27868, 27867, 7, 99, 27867, 27868, 27868, 203, 27868, 21, 116, 22, 655, 27867, 27867, 99, 495, 7, 27867, 27867, 920, 109, 27868, 22, 7, 109, 22, 27868, 27867, 27867, 27868, 3, 27868, 114, 27868, 27867, 99, 22, 7, 21, 237, 7, 27868, 27868, 99, 27867, 27868, 27867, 7, 7, 27868, 948, 105, 7, 105, 27867, 108, 108, 22, 27868, 85, 116, 7, 91, 27867, 495, 27868, 115, 27868, 27867, 91, 116, 6, 105, 27868, 99, 21, 133, 94, 158, 20, 22, 27867, 88, 214, 27867, 118, 27868, 27868, 27867, 104, 3, 2, 27867, 114, 111, 27868, 116, 21, 123, 27868, 27868, 91, 27868, 463, 170, 27868, 85, 116, 91, 27868, 105, 6, 22, 94, 105, 27868, 219, 27867, 7, 91, 27868, 118, 27868, 149, 3, 157, 3, 27868, 105, 22, 170, 171, 105, 22, 85, 7, 27867, 99, 85, 27867, 7, 22, 85, 170, 203, 27868, 22, 236, 27867, 99, 91, 111, 7, 920, 27867, 131, 131, 171, 27867, 116, 116, 27868, 105, 27867, 27868, 7, 27868, 27868, 22, 27867, 114, 22, 22, 27867, 121, 7, 94, 112, 114, 27867, 105, 27868, 27867, 27868, 7, 27867, 114, 105, 105, 27868, 7, 219, 27867, 157, 27868, 123, 27867, 7, 27867, 27867, 7, 27867, 7, 138, 27867, 105, 91, 85, 27867, 7, 22, 85, 27868, 27868, 111, 27867, 85, 94, 27868, 22, 27868, 27868, 7, 214, 27867, 27868, 162, 27868, 27867, 147, 130, 108, 7, 108, 7, 27868, 219, 170, 27868, 94, 21, 27867, 910, 27867, 27868, 114, 27867, 85, 510, 91, 214, 91, 7, 224, 27867, 7, 27867, 115, 27867, 7, 6, 5, 203, 27868, 21, 27867, 116, 27867, 27868, 27867, 129, 522, 7, 27868, 7, 168, 692, 27868, 116, 219, 130, 27867, 762, 27867, 7, 20, 85, 27868, 7, 91, 27867, 27868, 22, 27868, 91, 987, 94, 27868, 22, 111, 27867, 27868, 1083, 27867, 27867, 121, 121, 7, 91, 99, 121, 7, 138, 85, 91, 7, 27868, 7, 620, 7, 91, 7, 119, 27868, 27868, 27868, 99, 27868, 27868, 22, 99, 7, 219, 27867, 27868, 27867, 111, 27868, 123, 133, 94, 22, 21, 27868, 966, 114, 111, 7, 27867, 7, 116, 112, 21, 6, 145, 171, 27868, 27867, 123, 27868, 105, 140, 139, 27868, 214, 99, 27868, 910, 22, 135, 111, 114, 27868, 157, 162, 7, 201, 7, 7, 27868, 94, 112, 121, 27867, 129, 99, 121, 27867, 108, 99, 27867, 7, 21, 5, 111, 99, 99, 27867, 85, 85, 27868, 99, 27868, 7, 7, 2, 123, 27867, 117, 114, 85, 27867, 133, 27867, 27868, 158, 27867, 6, 99, 111, 7, 27868, 21, 22, 112, 108, 27868, 6, 27867, 27867, 463, 108, 7, 21, 27868, 7, 91, 108, 27868, 27867, 105, 27867, 91, 7, 27868, 27868, 7, 7, 27868, 27868, 27867, 27867, 91, 27868, 6, 22, 27867, 27868, 27867, 111, 27868, 85, 27867, 104, 7, 85, 27868, 91, 27868, 7, 99, 27868, 133, 22, 27867, 27867, 27867, 162, 27867, 27868, 130, 27867, 117, 27867, 123, 27868, 105, 27868, 85, 27868, 27868, 131, 7, 85, 109, 21, 7, 27868, 94, 188, 109, 7, 99, 7, 27867, 27867, 255, 99, 85, 94, 27868, 22, 27867, 27868, 118, 27868, 27867, 27867, 27868, 27867, 105, 111, 99, 27868, 27867, 99, 27868, 7, 105, 179, 7, 27868, 7, 27867, 7, 27867, 91, 85, 91, 27867, 166, 139, 123, 85, 27868, 27868, 91, 6, 85, 214, 27868, 133, 27867, 91, 179, 7, 171, 91, 27868, 7, 27868, 27868, 21, 6, 27868, 114, 7, 114, 27867, 27867, 7, 7, 91, 91, 27868, 7, 27867, 27868, 7, 7, 94, 27868, 99, 99, 6, 7, 21, 27867, 27868, 117, 118, 27868, 27867, 36, 2, 27868, 655, 27868, 114, 22, 219, 186, 85, 94, 910, 27868, 6, 27868, 91, 85, 108, 186, 21, 186, 27868, 27868, 99, 27868, 115, 27868, 99, 27868, 27868, 7, 105, 27868, 116, 116, 105, 27868, 109, 27868, 158, 105, 27867, 105, 118, 62, 27868, 27868, 27867, 105, 108, 112, 108, 6, 7, 27868, 6, 27868, 27868, 114, 27867, 987, 27868, 27868, 27867, 7, 27867, 27868, 27867, 105, 105, 88, 27867, 111, 118, 27868, 27867, 162, 27867, 117, 27868, 235, 121, 7, 112, 27867, 133, 27867, 27868, 27868, 91, 235, 85, 620, 108, 6, 187, 27867, 27868, 21, 27868, 27868, 119, 6, 7, 27867, 85, 27868, 187, 27868, 85, 91, 27868, 27868, 27867, 27868, 115, 27868, 21, 105, 27868, 27868, 22, 7, 91, 27867, 27868, 121, 27867, 27868, 157, 21, 27868, 7, 7, 131, 108, 6, 99, 129, 27867, 123, 27868, 85, 99, 112, 112, 85, 99, 133, 7, 27867, 118, 118, 7, 21, 6, 27868, 116, 27867, 27868, 27867, 27867, 235, 138, 117, 117, 27868, 22, 105, 85, 7, 27868, 692, 7, 27867, 7, 463, 109, 27868, 27868, 27868, 27868, 510, 85, 91, 27867, 111, 91, 85, 27868, 91, 111, 920, 118, 118, 27867, 3, 27867, 22, 91, 7, 85, 27868, 27867, 140, 111, 27868, 7, 27867, 111, 7, 91, 27868, 27868, 91, 27868, 27868, 7, 91, 91, 27867, 463, 510, 7, 27868, 27867, 27868, 119, 27867, 27868, 27867, 27868, 7, 116, 27868, 85, 7, 27868, 1083, 85, 116, 7, 111, 91, 85, 7, 27868, 114, 27867, 27867, 88, 7, 118, 27867, 157, 162, 85, 27868, 115, 27868, 91, 85, 91, 108, 27868, 21, 131, 27868, 91, 27867, 27867, 27868, 115, 22, 241, 27868, 27868, 27868, 620, 91, 27868, 76, 91, 22, 5, 76, 22, 7, 27867, 27868, 156, 116, 7, 27867, 94, 27868, 5, 151, 157, 7, 7, 692, 463, 188, 91, 27867, 116, 22, 27868, 21, 7, 115, 6, 27868, 7, 151, 762, 27868, 22, 27868, 85, 27868, 109, 27868, 146, 27867, 27868, 7, 21, 27868, 105, 27867, 7, 94, 27867, 7, 166, 117, 27867, 27867, 94, 27868, 6, 7, 105, 85, 27868, 21, 22, 166, 27867, 91, 131, 920, 27868, 105, 118, 3, 157, 162, 123, 85, 158, 20, 22, 85, 94, 21, 22, 7, 21, 27867, 27868, 27868, 27867, 114, 27867, 112, 112, 27867, 112, 91, 495, 27868, 112, 6, 27867, 108, 7, 7, 91, 27867, 111, 112, 27868, 7, 94, 121, 7, 138, 27867, 131, 203, 27868, 6, 85, 112, 91, 27868, 99, 27868, 94, 910, 22, 27867, 157, 104, 85, 167, 1083, 27868, 94, 22, 27868, 27868, 27868, 7, 167, 473, 27867, 27868, 966, 7, 85, 91, 27867, 27868, 156, 27868, 105, 27867, 167, 22, 201, 91, 91, 27868, 99, 105, 27868, 22, 85, 94, 6, 27867, 27868, 7, 27867, 27867, 522, 85, 7, 522, 27868, 21, 7, 22, 85, 156, 522, 109, 21, 109, 94, 27868, 27867, 27868, 131, 27867, 156, 131, 27867, 27868, 219, 21, 22, 27867, 27868, 131, 27868, 910, 116, 109, 94, 7, 27868, 27867, 94, 131, 27867, 27868, 27868, 27867, 522, 27868, 6, 27867, 7, 27867, 85, 910, 27867, 425, 27868, 151, 255, 27868, 6, 450, 99, 99, 27867, 118, 27868, 7, 27868, 27868, 94, 522, 27868, 27868, 22, 495, 255, 186, 27868, 7, 99, 495, 131, 22, 7, 27868, 21, 27867, 27868, 27868, 27868, 27868, 6, 22, 85, 27867, 27868, 27867, 116, 27867, 27868, 27867, 94, 27868, 6, 219, 522, 27868, 1083, 77, 7, 27867, 203, 131, 27868, 99, 522, 1083, 2, 27868, 7, 85, 109, 27868, 21, 27868, 1083, 27868, 22, 27867, 131, 910, 91, 27867, 7, 91, 27868, 27868, 1380, 27867, 27868, 91, 27867, 85, 27868, 910, 27868, 20, 1083, 27868, 131, 463, 2, 27868, 117, 116, 114, 27867, 114, 27867, 7, 108, 510, 85, 27868, 7, 27867, 7, 27867, 27867, 27868, 114, 27867, 111, 7, 91, 27867, 510, 116, 116, 27867, 121, 7, 91, 27867, 27868, 27867, 121, 111, 7, 495, 7, 27867, 3, 27867, 27867, 112, 27867, 27868, 116, 123, 27867, 27868, 139, 116, 133, 7, 21, 112, 22, 116, 22, 99, 7, 99, 7, 91, 99, 42, 27867, 21, 27868, 27867, 27868, 91, 7, 115, 6, 21, 27868, 27868, 37, 77, 27868, 203, 27868, 7, 6, 99, 27867, 118, 27868, 118, 114, 118, 117, 7, 21, 27867, 36, 162, 27867, 910, 22, 85, 99, 27868, 94, 901, 27868, 94, 27868, 27868, 6, 27867, 27868, 108, 463, 27867, 27868, 987, 21, 6, 27867, 21, 7, 920, 27867, 27868, 94, 27868, 27868, 27868, 121, 85, 99, 109, 27867, 91, 7, 94, 27868, 21, 7, 495, 27868, 133, 463, 27868, 562, 99, 27868, 157, 22, 562, 94, 463, 27867, 510, 7, 510, 7, 27867, 27867, 27867, 495, 495, 27868, 88, 27867, 157, 27867, 112, 7, 112, 99, 112, 85, 27868, 99, 94, 21, 27867, 27868, 99, 116, 21, 27868, 27868, 7, 99, 156, 156, 7, 7, 27867, 218, 27868, 257, 119, 27867, 108, 27867, 91, 91, 27867, 119, 7, 133, 27868, 108, 27868, 156, 27867, 85, 91, 91, 91, 91, 85, 7, 21, 27867, 133, 21, 219, 74, 162, 114, 27867, 27868, 201, 105, 21, 27868, 303, 27868, 27868, 7, 27867, 131, 27868, 6, 108, 27868, 99, 27868, 7, 94, 27868, 21, 27867, 27868, 21, 27868, 258, 303, 179, 108, 27868, 91, 21, 27868, 116, 27867, 156, 22, 108, 21, 112, 76, 27868, 255, 27868, 22, 151, 367, 27868, 910, 966, 27867, 27868, 6, 85, 190, 27868, 6, 99, 22, 7, 156, 6, 78, 27867, 131, 7, 7, 190, 138, 112, 108, 116, 138, 115, 219, 27867, 85, 7, 114, 27868, 27867, 27867, 7, 27867, 108, 99, 27868, 22, 99, 27868, 21, 22, 27867, 99, 27868, 131, 85, 27868, 27868, 118, 27868, 27868, 157, 7, 108, 27868, 27868, 99, 214, 235, 7, 1083, 123, 111, 91, 111, 119, 108, 116, 99, 7, 27867, 920, 27867, 27867, 91, 27868, 42, 157, 27867, 27868, 114, 910, 22, 123, 27868, 27868, 27868, 27867, 94, 116, 463, 85, 27868, 27868, 21, 7, 27868, 27868, 22, 27867, 562, 27868, 22, 562, 27867, 27868, 22, 495, 85, 91, 27868, 109, 99, 116, 27867, 94, 116, 91, 7, 123, 27868, 94, 7, 7, 91, 105, 27868, 117, 3, 162, 27867, 85, 7, 7, 7, 27868, 27868, 27867, 114, 27868, 112, 27867, 146, 27867, 7, 27868, 7, 1083, 27867, 111, 27868, 241, 22, 109, 27868, 27868, 20, 105, 88, 7, 118, 510, 114, 157, 1083, 157]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dword ptr 48088 48088 end 42 48088 ptr ptr 48088 1000014129 [ 1000014129 48088 48088 48088 ptr 48088 rdi [ 48088, 48088 ] 1000014129 48088 1000014129 4380 48088 no 48088 n [CLS], 48088 dword 1000014129, sub,ll [ ptr + 1000014129 1000014129 mov r13 48088 rdi 48088 48088 1000014129 48088, r14 48088 41 4380 1000014129 48088, 48088 * 1000014129 48088 48088 1000014129 8240 1000014129 48088 ] 4380 mov 48088 48088 4365 mov, 48088ll 48088ll [ 48088 4427 48088 : ] 1000014129 le 16 1000014129 1000014129 48088 48088, r10 edx 48088 1000014129 48088 48088 48088, 1000014129 8252 1000014129 48088 48088 [ ] 1000014129 48088 1000014129mul rdi 4808840 lea 48088 454 1000014129 movll 48088 48088 rdx 1000014129 48088 48088 lea pu pu 48088 1000014129 pu 4440 48088 8220 48088 48088 rdx,4040 48088, dword subll, 48088 end 48088 48088 ] 48088 48088, no 1000014129 [ + 48088,, 48088,word80 8220, 48088 1000014129 rbp 464etetetet 81 48088etetmul 48088omi79 1000014129 xor 48088 edx 48088 1000014129word 1000014129 edx 1000014129 +ll 1000014129 pu 48088 1000014129mm lea 42 ptr 48088 ptr xor 48088 [64 1000014129 rbp ptr307 47 1000014129, 48088 48088 1000014129 1000014129307 1000014129 1000014129307 lea push, 1000014129 48088zx 48088 48088 edxmm 48088, 48088 r13 ] 8252 xor [ 48088 48088ll 48088 48088 +ll ] 47 ptr mov 10 48088 * mov 1000014129 48088, ptr 1000014129 [64 ] ptr mov 48088 ptr 48088, 1000014129 [ ] mov 48088word [ lea ] [ ]307 ptr 48088 1000014129word 48088 + 16480 r10 lea edi 1000014129 rdx 48088 r12 xor rdiword ] mov 48088 ptr r12 1000014129, 1000014129 48088 dword 4427 r13, r13 10 48088ll + 48088 4808864 1000014129 movll 48088 + 1000014129 48088ll : 1000014129 16 16 48088 [CLS] 48088 48088 dword 1000014129 48088 mov 48088 4427 48088 cmovns 48088 ptr 48088 48088, 1000014129 48088 10 48088 [ 48088 4411 1000014129 42 1000014129 1000014129 [ 8231 47 48088 + 48088ll 47 1000014129 mov mov mov 48088word movll [ 48088 48088 48088ll ptrmm 48088 48088 454 10 48088 1000014129 48088 1000014129 mov 48088 rdi 48088 + 48088 48088 48088 48088 [ 1000014129 [ + 48088 480886464 r14 48088 r12 + test rdi, 48088 48088 ] 42 edx rbp ptr 1000014129 48088word 48088, lea, rdiword 4808840 ptr lea 1000014129 pu pu 44 lea edx 1000014129,ll 1000014129 1000014129 48088 ptr,word 48088zx 1000014129 + 48088 leall r15 48088 8240 nop ptr [24 ] 1000014129word 4272 mov ptr [64, 48088 : 1000014129 48088 1000014129 1000014129 dword, 48088 no 48088mm rdi dword, 48088 [ + 48088 mov r10 mov 48088word 48088 1000014129 mov, mov 48088, 1000014129 byteword ] 48088 + [ 48088ll 1000014129 ] pop [24 mov ] 48088 48088 [ 42,, 48088 ], ptrll 48088llword 48088 ptr 48088 100001412924 t 1000014129 48088 [SEP] [SEP] rdi 48088 48088r 48088 o64 rdi movword 48088 rdi end 48088 48088 o [ 48088 + o ], [ [ 48088 48088 48088ll 48088 edx [ll mov, 48088 4240 4240 48088 ] 48088 [64 r13 pop 1000014129, mov 4808824 48088, 1000014129np 1000014129 r13 ptr 48088 ptr 41 1000014129 48088 ] 43 + ] sub 48088 48088 16 8206 8220, 1000014129 1000014129 48088 16480 48088 ptr lea 1000014129 48088, [ cmovns sub 1000014129 48088ll, mov 48088 48088 48088 +ll 1000014129 ptr, 1000014129 8233 48088 [ * r12 le lea 1000014129 ptr nop 1000014129 [ 48088 4427 rdi 1000014129 1000014129 pu 1000014129 16 r12 xor 42 1000014129 48088 dword 1000014129 48088 xor 1000014129 [SEP] mov, 1000014129 mov 48088 1000014129 48088 [ dword dword r12 dword 48088 rdx movword dword 48088 614 mov 48088, 48088 edi 48088 edi pop [ ] popword 48088 cmovns 1000014129 47 [ * dword 48088 4427 100001412924, 1000014129 popword + ] 8206ll 48088 1000014129 48088 48088 48088, 48088mm 48088 8231 48088, pu 48088 r12 xor ] ptr 48088, 48088 r12 movs [ 48088 lea xor mov dword r10 4427 48088 1000014129 r14 1000014129 48088 dword dword 48088ll, 10 48088 48088llll no24, 48088 1000014129 4411, movsd 4808824ll r13 476464 r12 8206 r12 mov bl, 48088 4808824 48088 48088 1000014129word 48088, 1000014129 48088 1000014129 rdi [ + movword : 16480 1000014129 xor, + 48088 pu r12 movsdll + 16480 le [CLS]mm ptr sub rdi 48088word [ cmovns rbp ptr ptr 48088 1000014129 lea lea 48088 lea 47 1000014129 r14, 4272 4272 [ 48088 454 48088ll ] ptr 48088 [, ptr 1000014129ll movll 48088 + 48088 ]word 48088 [ 48088 ptr 1000014129 leu 48088 lea edx, dwordll 1000014129 48088 1000014129 mov 48088 mov 48088 48088 r14 lea 1000014129 4465 1000014129 48088ll 100001412964 48088 +k24 mov 48088 mov 48088 [ll ], dwordword [ 48088 48088 1000014129, 48088 48088 ] r10 48088 end mov 48088, lea, mov dword 48088 1000014129,ll 1000014129 48088 48088 10 48088 [ 42 ] 8231 1000014129 1000014129ll 16480, 1000014129 1000014129307 rdi 48088 ], rdi ] 48088 1000014129 1000014129 48088 [SEP] 48088 r12 48088 1000014129ll ], [ movzx, 48088 48088ll 1000014129 48088 1000014129,, 48088 4411 lea, lea 1000014129 xor xor ] 48088 mov 42, ptr 1000014129 16480 48088 pop 48088 1000014129 ptr 42 + lea 48088ll [ subword end : ] 1000014129 eax 47 1000014129 pu 48088 48088 1000014129 le [SEP] [CLS] 1000014129 r12 rbp 48088 42 [mm 48088 48088 ptr 48088np r15 48088 mov 42 ptr 48088 lea + ]word lea 48088 r10 1000014129, ptr 48088 pu 48088 jz [SEP] 16 [SEP] 48088 lea ] r15 nop lea ] mov, 1000014129ll mov 1000014129, ] mov r15 10 48088 ]zx 1000014129ll ptr rbp,307 10000141296464 nop 1000014129 42 42 48088 lea 1000014129 48088, 48088 48088 ] 1000014129 r12 ] ] 1000014129 push,word dword r12 1000014129 lea 48088 1000014129 48088, 1000014129 r12 lea lea 48088, r10 1000014129 16 48088mm 1000014129, 1000014129 1000014129, 1000014129, 43 1000014129 lea ptr mov 1000014129, ] mov 48088 48088 rbp 1000014129 movword 48088 ] 48088 48088, 47 1000014129 48088 movs 48088 1000014129 44 rdx xor, xor, 48088 r10 r15 48088word [ 1000014129 4427 1000014129 48088 r12 1000014129 mov 8220 ptr 47 ptr, word 1000014129, 1000014129 pop 1000014129, + * 10 48088 [ 1000014129 42 1000014129 48088 1000014129 r13 454, 48088,40 8233 48088 42 r10 rdx 1000014129 8252 1000014129, : mov 48088, ptr 1000014129 48088 ] 48088 ptr 4380word 48088 ] rbp 1000014129 48088 no 1000014129 1000014129 push push, ptrll push, 43 mov ptr, 48088, 8206, ptr, edx 48088 48088 48088ll 48088 48088 ]ll, r10 1000014129 48088 1000014129 rbp 48088mm subword ] [ 48088 cmovns r12 rbp, 1000014129, 42 dword [ + test nop 48088 1000014129mm 48088 lea ebx esi 48088 47ll 48088 4427 ] jnz rbp r12 48088 16 movs,32,, 48088word dword push 1000014129 r13ll push 1000014129 xorll 1000014129, [ * rbpllll 1000014129 mov mov 48088ll 48088,, [CLS]mm 1000014129 edi r12 mov 1000014129 sub 1000014129 48088 end 1000014129 +ll rbp, 48088 [ ] dword xor 48088 + 1000014129 1000014129np xor, [ 48088, ptr xor 48088 1000014129 lea 1000014129 ptr, 48088 48088,, 48088 48088 1000014129 1000014129 ptr 48088 + ] 1000014129 48088 1000014129 rbp 48088 mov 1000014129 le, mov 48088 ptr 48088,ll 48088 sub ] 1000014129 1000014129 1000014129 movs 1000014129 48088 rdx 1000014129 edi 1000014129mm 48088 lea 48088 mov 48088 4808864, mov rdi [, 48088word 81 rdi,ll, 1000014129 1000014129 pxorll movword 48088 ] 1000014129 48088 pu 48088 1000014129 1000014129 48088 1000014129 lea rbpll 48088 1000014129ll 48088, lea byte, 48088, 1000014129, 1000014129 ptr mov ptr 100001412972 esimm mov 48088 48088 ptr + mov 47 48088 sub 1000014129 ptr byte, nop ptr 48088, 48088 48088 [ + 48088 r12, r12 1000014129 1000014129,, ptr ptr 48088, 1000014129 48088,,word 48088llll +, [ 1000014129 48088 edi pu 48088 1000014129 n [CLS] 48088 8231 48088 r12 ] r10 r12d movword 4427 48088 + 48088 ptr mov xor r12d [ r12d 48088 48088ll 48088 pop 48088ll 48088 48088, lea 48088 42 42 lea 48088 rdi 48088 end lea 1000014129 lea pum 48088 48088 1000014129 lea xor dword xor +, 48088 + 48088 48088 r12 1000014129 4380 48088 48088 1000014129, 1000014129 48088 1000014129 lea lea eax 1000014129 rbp pu 48088 1000014129 movs 1000014129 edi 48088 41 push, dword 1000014129 sub 1000014129 48088 48088 ptr 41 mov 8206 xor + 4240 1000014129 48088 [ 48088 48088 edx +, 1000014129 mov 48088 4240 48088 mov ptr 48088 48088 1000014129 48088 pop 48088 [ lea 48088 48088 ], ptr 1000014129 48088 push 1000014129 48088 16 [ 48088,,64 xor +ll r13 1000014129mm 48088 movll dword dword movll sub, 1000014129 pu pu, [ + 48088 42 1000014129 48088 1000014129 1000014129 41 43 edi edi 48088 ] lea mov, 48088 8233, 1000014129,np rdi 48088 48088 48088 48088 8220 mov ptr 1000014129 rbp ptr mov 48088 ptr rbp307 pu pu 1000014129 [SEP] 1000014129 ] ptr, mov 48088 1000014129 ebx rbp 48088, 1000014129 rbp, ptr 48088 48088 ptr 48088 48088, ptr ptr 1000014129np 8220, 48088 1000014129 48088 edx 1000014129 48088 1000014129 48088, 42 48088 mov, 48088 no mov 42, rbp ptr mov, 48088 r12 1000014129 1000014129 eax, pu 1000014129 16 movs mov 48088 pop 48088 ptr mov ptr xor 48088 [64 48088 ptr 1000014129 1000014129 48088 pop ]mul 48088 48088 48088 8206 ptr 48088r ptr ] *r ], 1000014129 4808824 42, 1000014129word 48088 *et 16,, 8233np 81 ptr 1000014129 42 ] 48088 [, pop + 48088,et 8252 48088 ] 48088 mov 48088 rdi 48088 r14 1000014129 48088, [ 48088 lea 1000014129,word 1000014129,72 edi 1000014129 1000014129word 48088 +, lea mov 48088 [ ]72 1000014129 ptr64307 48088 lea pu [SEP] 16 movsmm mov end : ] movword [ ], [ 1000014129 48088 48088 1000014129 r12 1000014129 dword dword 1000014129 dword ptr 16480 48088 dword + 1000014129 xor,, ptr 1000014129 rbp dword 48088,word push, 43 100001412964 10 48088 + mov dword ptr 48088ll 48088word 4427 ] 1000014129 16 le mov56 no 48088word ] 48088 48088 48088,56 cvtsi2sd 1000014129 48088 cmovns, mov ptr 1000014129 4808824 48088 lea 100001412956 ]32 ptr ptr 48088ll lea 48088 ] movword + 1000014129 48088, 1000014129 1000014129 454 mov, 454 48088 [, ] mov24 454 rdi [ rdiword 48088 1000014129 4808864 10000141292464 1000014129 48088 r10 [ ] 1000014129 4808864 48088 4427 42 rdiword, 48088 1000014129word64 1000014129 48088 48088 1000014129 454 48088 + 1000014129, 1000014129 mov 4427 1000014129 jnc 48088et pxor 48088 + 8272llll 1000014129 pu 48088, 48088 48088word 454 48088 48088 ] 16480 pxor r12d 48088,ll 1648064 ], 48088 [ 1000014129 48088 48088 48088 48088 + ] mov 1000014129 48088 1000014129 42 1000014129 48088 1000014129word 48088 + r10 454 48088 noq, 1000014129 1064 48088ll 454 no [CLS] 48088, mov rdi 48088 [ 48088 no 48088 ] 100001412964 4427 ptr 1000014129, ptr 48088 48088 614 1000014129 48088 ptr 1000014129 mov 48088 4427 48088 : no 4808864np [CLS] 48088 edi 42 r12 1000014129 r12 1000014129, xor 8220 mov 48088, 1000014129, 1000014129 1000014129 48088 r12 1000014129 rbp, ptr 1000014129 8220 42 42 1000014129 push, ptr 1000014129 48088 1000014129 push rbp, 16480, 1000014129 [SEP] 1000014129 1000014129 dword 1000014129 48088 42mm 1000014129 48088 esi 42 sub, [ dword ] 42 ]ll,ll, ptrll t 1000014129 [ 48088 1000014129 48088 ptr, pop + [ 48088 48088 oq 48088 10 48088, +ll 1000014129 pu 48088 pu r12 pu edi, [ 1000014129 n movs 1000014129 4427 ] movll 48088word 229 48088word 48088 48088 + 1000014129 48088 xornp 1000014129 48088 4380 [ + 1000014129 [,307 1000014129 48088word 48088 48088 48088 push movll rdi 1000014129 ptr,word 48088 [, 16480 48088 subnp 48088000ll 48088 16 ]000wordnp 1000014129 8220, 8220, 1000014129 1000014129 1000014129 16480 16480 48088 eax 1000014129 16 1000014129 dword, dwordll dword mov 48088llword [ 1000014129 48088ll 42 [ 48088 48088,ll2424,, 100001412920 48088 cv edx 1000014129 xor 1000014129 ptr ptr 1000014129 edx, sub 48088 xor 4808824 1000014129 mov ptr ptr ptr ptr mov, [ 1000014129 sub [ r10u movs r12 1000014129 4808832 lea [ 4808891 48088 48088, 100001412964 48088 + xor 48088ll 48088,word 48088 [ 1000014129 48088 [ 48088 cvt91 byte xor 48088 ptr [ 48088 42 100001412924 ] xor [ dwordr 48088 pxor 48088 ]et 4512 48088 4427 cmovns 1000014129 48088 + mov88 48088 +ll ],24 +k 100001412964,,88 43 dword xor 42 43 pop r10 1000014129 mov, r12 48088 1000014129 1000014129, 1000014129 xorll 48088 ]ll 48088 [ ] 1000014129ll 4808864 mov 48088 48088 pu 48088 48088 16, xor 48088 48088ll 47 41, nomm rbp ptr rbp edx xor 42ll, 1000014129307 1000014129 1000014129 ptr 48088 t 16 1000014129 48088 r12 4427 ]mm 48088 48088 48088 1000014129word 42np mov 48088 48088 [, 48088 48088 ] 1000014129000 48088 ]000 1000014129 48088 ] 16480 mov ptr 48088 rdill 42 1000014129word 42 ptr,mm 48088word,, ptr lea 48088 edi [SEP] movs 1000014129 mov,,, 48088 48088 1000014129 r12 48088 dword 1000014129 r14 1000014129, 48088, no 1000014129 rbp 48088mul ] rdi 48088 48088 : lea eax, pu 8220 r12 16 no 16\n"
     ]
    }
   ],
   "source": [
    "# Convert token IDs to tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "# Detokenize the tokens into a string\n",
    "text = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
