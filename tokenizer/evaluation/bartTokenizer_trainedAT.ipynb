{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b4579f-cccb-4423-a0d9-f48d35e374bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys,os\n",
    "from elftools.elf.elffile import ELFFile\n",
    "from elftools.elf.segments import Segment\n",
    "from capstone import *\n",
    "from capstone.x86 import *\n",
    "\n",
    "import os\n",
    "import json \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import sys,os\n",
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f9e3ca-865a-481a-ae4c-89a5c21cac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_TOKEN_LEN = 1024\n",
    "BATCH_SIZE = 50\n",
    "epochs = 100\n",
    "\n",
    "EXPERIMENT_NAME = 'cusTokenizer_bartOwn_35k_ATW_signature'\n",
    "new_vocab_size = 35000\n",
    "# disassembly_decimal disassembly_all_number_to_words disassembly_decimal \n",
    "data_key = \"disassembly_all_number_to_words\"\n",
    "\n",
    "\n",
    "from transformers import BartTokenizer,BertTokenizer, BertForNextSentencePrediction,BertForPreTraining,BertConfig,AutoModelForMaskedLM,get_linear_schedule_with_warmup ,BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast ,AutoModelForPreTraining ,BertGenerationEncoder, BartForConditionalGeneration,BertGenerationDecoder,EncoderDecoderModel\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"./../../models/bart_AT\" )\n",
    "print(tokenizer.pad_token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd5a1a9-36cf-459c-afe7-40a1b3d2bb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions Count:  80000 \n",
      "\n",
      "['int', '*struct']\n",
      "30 \n",
      " ['*uint', 'long', '*char', 'uchar', 'longlong', 'ulong', '*double', '*short', 'double', 'short', '*int', 'int', '*longlong', 'bool', 'None', 'uint', '*void', 'void', '*long', '*struct', '*ulonglong', 'char', 'ulonglong', 'ushort', '*float', '*ulong', '*ushort', '*bool', '*uchar', 'float']\n"
     ]
    }
   ],
   "source": [
    "# def generate_param_string(param_list):\n",
    "\n",
    "\n",
    "#     if len(param_list)==0:\n",
    "#         return 'void'\n",
    "#     else:\n",
    "#         return \" \".join(param_list)\n",
    "# def unique_items(list_of_lists):\n",
    "#     # Flatten the list of lists\n",
    "#     print(list_of_lists[3].split())\n",
    "#     flattened_list = [item for sublist in list_of_lists for item in sublist.split()]\n",
    "#     # Convert the flattened list to a set to remove duplicates\n",
    "#     unique_elements = set(flattened_list)\n",
    "#     return list(unique_elements)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DATA_PATH = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions'\n",
    "\n",
    "# TRAIN_DATA_PATH  ='/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/train/'\n",
    "\n",
    "# TEST_DATA_PATH   = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/test/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_json_files = [os.path.join(TRAIN_DATA_PATH, f) for f in os.listdir(TRAIN_DATA_PATH) ]\n",
    "\n",
    "# test_json_files = [os.path.join(TEST_DATA_PATH, f) for f in os.listdir(TEST_DATA_PATH) ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def read_corpus(json_files):\n",
    "\n",
    "#     all = []\n",
    "#     all_params = []\n",
    "#     all_returns = []\n",
    "#     all_signatures = []\n",
    "\n",
    "#     for k, j_file in enumerate(json_files):\n",
    "#         # if k>1000:\n",
    "#         #     break\n",
    "#         try:\n",
    "\n",
    "#             with open(j_file, 'r') as file:\n",
    "#                 data = json.load(file)\n",
    "                \n",
    "#                 funct = data[data_key]['input']\n",
    "\n",
    "#                 # if len(data['signature']['paramTypes'])<5:\n",
    "#                 all.append(funct)\n",
    "#                 param_string = generate_param_string( data['signature']['paramTypes'] )\n",
    "#                 all_params.append( param_string )\n",
    "#                 all_returns.append(  data['signature']['returnType']  )\n",
    "#                 all_signatures.append(param_string + \" [SEP] \"+ data['signature']['returnType'] )\n",
    "#         except Exception as e: \n",
    "#             print(e)\n",
    "#     return all , all_params , all_returns , all_signatures\n",
    "    \n",
    "\n",
    "\n",
    "# train_text , train_params , train_returns, train_signatures = read_corpus(train_json_files)\n",
    "\n",
    "# test_text  , test_params  , test_returns , test_signatures = read_corpus(test_json_files)\n",
    "\n",
    "\n",
    "        \n",
    "# # text = text[0:5000]\n",
    "# print(\"Functions Count: \",len(train_text), '\\n')\n",
    "# example = train_text[10]\n",
    "# text = train_text + test_text\n",
    "\n",
    "\n",
    "# unique_types = unique_items(train_params)\n",
    "# print(len(unique_types), '\\n',unique_types)\n",
    "MAX_TOKEN_LEN = 1024\n",
    "\n",
    "\n",
    "\n",
    "# disassembly_decimal disassembly_all_number_to_words disassembly_decimal  disassembly_addresses_to_words\n",
    "data_key = \"disassembly_addresses_to_words\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def generate_param_string(param_list):\n",
    "\n",
    "\n",
    "#     if len(param_list)==0:\n",
    "#         return 'void'\n",
    "#     else:\n",
    "#         return \" \".join(param_list)\n",
    "def unique_items(list_of_lists):\n",
    "    # Flatten the list of lists\n",
    "    print(list_of_lists[3].split())\n",
    "    flattened_list = [item for sublist in list_of_lists for item in sublist.split()]\n",
    "    # Convert the flattened list to a set to remove duplicates\n",
    "    unique_elements = set(flattened_list)\n",
    "    return list(unique_elements)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DATA_PATH = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions'\n",
    "\n",
    "TRAIN_DATA_PATH  ='/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/train/'\n",
    "\n",
    "TEST_DATA_PATH   = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/test/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_json_files = [os.path.join(TRAIN_DATA_PATH, f) for f in os.listdir(TRAIN_DATA_PATH) ]\n",
    "\n",
    "test_json_files = [os.path.join(TEST_DATA_PATH, f) for f in os.listdir(TEST_DATA_PATH) ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_types = ['*short', '*ulong', 'int', 'ulonglong', '*longlong', 'None', 'short', 'char', '*struct', '*long', '*char', 'double', 'uint', 'uchar', '*float', 'longlong', 'long', '*void', 'float', 'bool', '*int', '*ushort', '*ulonglong', '*uchar', 'ushort', 'void', '*double', 'ulong', '*uint', '*bool']\n",
    "all_types_dict = {}\n",
    "for k,type in enumerate(all_types):\n",
    "    all_types_dict[type] = str(k)\n",
    "\n",
    "def read_corpus(json_files):\n",
    "\n",
    "    all = []\n",
    "    all_params = []\n",
    "    all_returns = []\n",
    "    all_signatures = []\n",
    "\n",
    "    for k, j_file in enumerate(json_files):\n",
    "        # if k>1000:\n",
    "        #     break\n",
    "        try:\n",
    "\n",
    "            with open(j_file, 'r') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                funct = data[data_key]['input']\n",
    "                \n",
    "                all.append(funct)\n",
    "\n",
    "                param_string = \"\"\n",
    "\n",
    "                if len(data['signature']['paramTypes'])==0:\n",
    "                    param_string = all_types_dict['void']\n",
    "                    \n",
    "                for v,type in enumerate(data['signature']['paramTypes']):\n",
    "                    if v==0:\n",
    "                        param_string =  all_types_dict[type]\n",
    "                    else:\n",
    "                        param_string = param_string + sep_token +all_types_dict[type]\n",
    "                \n",
    "                all_params.append( param_string )\n",
    "                all_returns.append(  data['signature']['returnType']  )\n",
    "                all_signatures.append(param_string + sep_token + all_types_dict[data['signature']['returnType']] )\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "    return all , all_params , all_returns , all_signatures\n",
    "    \n",
    "\n",
    "\n",
    "train_text , train_params , train_returns, train_signatures = read_corpus(train_json_files)\n",
    "\n",
    "test_text  , test_params  , test_returns , test_signatures = read_corpus(test_json_files)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33d9aa6-251c-4ace-a92a-7037d7c36ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "train_source_encodings = tokenize(train_text)\n",
    "train_target_encodings = tokenize(train_signatures)\n",
    "\n",
    "\n",
    "test_source_encodings = tokenize(test_text)\n",
    "test_target_encodings = tokenize(test_signatures)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(train_source_encodings['input_ids'], train_source_encodings['attention_mask'], train_target_encodings['input_ids'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "test_dataset = TensorDataset(test_source_encodings['input_ids'], test_source_encodings['attention_mask'], test_target_encodings['input_ids'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37578ca9-ed2d-4861-b0ef-e190a62c9b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8630a7f-854d-49a3-990f-e91f1c12151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"./../../models/\"+EXPERIMENT_NAME+\"_model.ckpt\", forced_bos_token_id=0)\n",
    "# tok = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "# example_english_phrase = \"UN Chief Says There Is No <mask> in Syria\"\n",
    "# batch = tok(example_english_phrase, return_tensors=\"pt\")\n",
    "# generated_ids = model.generate(batch[\"input_ids\"])\n",
    "# assert tok.batch_decode(generated_ids, skip_special_tokens=True) == [\n",
    "#     \"UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2fd5b8-233c-439d-94a7-96902bc2a488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358add79-a5dc-4b80-8ba7-73cbbf3de4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LongformerModel, LongformerTokenizer\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "# model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "# # Example of how to prepare inputs\n",
    "# inputs = tokenizer(\"Example of a long input sequence \" * 50, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "# output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf9ba31-668a-4b0c-ad01-182c3976105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token_id = tokenizer.sep_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "\n",
    "\n",
    "\n",
    "# def align_output(pred, gt):\n",
    "#     params_pred = []\n",
    "#     params_gt = []\n",
    "#     ret_pred = []\n",
    "#     ret_gt = []\n",
    "#     for i,gid in enumerate(gt):\n",
    "#         if gid!= cls_token_id:\n",
    "            \n",
    "            \n",
    "#             aligned_pred.append(pred[i])\n",
    "#             aligned_gt.append(gid)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61a0e9fb-9576-447e-9229-b1199ec4b83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00d79dec-3b92-42a4-8fd9-762179256ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # ## # ## # ## # ## # ## # ## # ## # ## # ## # #\n",
      "epoch:  0  n:  399\n",
      "171704 171704\n",
      "[347, 18304, 14, 409, 18304, 63, 353, 65, 18304, 347]\n",
      "[363, 18304, 18304, 18304, 18304, 63, 18304, 18304, 1, 1]\n",
      "TEST precision, recall, f1 0.38389326321737166 0.3150305176349998 0.2827457299835836\n",
      "[0, 470, 18304, 63, 353, 65, 18304, 363, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 347, 18304, 18304, 18304, 1, 63, 18304, 18304, 18304, 18304, 1, 1, 1, 63, 1, 1, 1, 63, 18304, 63, 353, 18304, 18304, 1, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 65, 1, 1, 1, 63, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 63, 1, 1, 1, 65, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 1, 1, 63, 1, 65, 1, 1, 1, 1, 1, 63, 1, 63, 1, 63, 353, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 63, 1, 65, 1, 1, 1, 1, 1, 63, 1, 63, 2, 63, 353, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 1, 1, 63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "right 0   wrong :  20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1):  # Number of epochs\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_all_ground_truth = []\n",
    "        test_all_prediction = []\n",
    "        right = 0\n",
    "        wrong = 0\n",
    "        for n,test_batch in enumerate(test_loader):\n",
    "    \n",
    "            test_input_ids, test_attention_mask, test_labels = [x.to(device) for x in test_batch]\n",
    "            # print(input_ids.shape , attention_mask.shape , labels.shape )\n",
    "            test_outputs = model(input_ids=test_input_ids, attention_mask=test_attention_mask)\n",
    "            test_token_predictions = torch.argmax(test_outputs.logits, axis=-1)\n",
    "    \n",
    "            test_labels_list = test_labels.tolist()\n",
    "            test_token_predictions_list = test_token_predictions.tolist()\n",
    "            \n",
    "            for  j,test_label in enumerate(test_labels_list):\n",
    "                if test_label==test_token_predictions_list[j]:\n",
    "                    right = right +1\n",
    "                else:\n",
    "                    wrong = wrong+ 1\n",
    "                for i, lid in enumerate(test_label):\n",
    "                    if lid not in  [cls_token_id , sep_token_id , pad_token_id]:\n",
    "                        test_all_ground_truth.append(lid)\n",
    "                        test_all_prediction.append(test_token_predictions_list[j][i])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        print(\"# # #\" *10)\n",
    "        # test_pred_tokens = tokenizer.convert_ids_to_tokens(test_token_predictions[0].tolist())\n",
    "        # test_gt_tokens = tokenizer.convert_ids_to_tokens(test_labels[0].tolist())\n",
    "        # print('test_pred_tokens' , test_pred_tokens)\n",
    "        # print('test_gt_tokens' ,test_gt_tokens)\n",
    "        print ('epoch: ',epoch , ' n: ', n )\n",
    "        print(len(test_all_ground_truth) , len(test_all_prediction))\n",
    "        print(test_all_ground_truth[-10:])\n",
    "        print(test_all_prediction[-10:])\n",
    "        test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(test_all_ground_truth,test_all_prediction ,average='weighted')\n",
    "        print('TEST precision, recall, f1' ,test_precision, test_recall, test_f1)\n",
    "    \n",
    "        print(test_labels_list[0])\n",
    "        print(test_token_predictions_list[0])\n",
    "        print(\"right\",right, \"  wrong : \", wrong)\n",
    "    \n",
    "        # Save the model\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a71d99f0-476e-4a01-a056-d3cec482ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 0\n"
     ]
    }
   ],
   "source": [
    "print(sep_token_id ,pad_token_id,cls_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00ed88a9-4f8c-444f-99f8-f214df133a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 63, 2, 2, 353, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [0, 65, 2]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('[', 'SEP', ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c1246-ab3a-4bb7-978d-2e2c188a4008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
