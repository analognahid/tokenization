{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b4579f-cccb-4423-a0d9-f48d35e374bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys,os\n",
    "from elftools.elf.elffile import ELFFile\n",
    "from elftools.elf.segments import Segment\n",
    "from capstone import *\n",
    "from capstone.x86 import *\n",
    "\n",
    "import os\n",
    "import json \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import sys,os\n",
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f9e3ca-865a-481a-ae4c-89a5c21cac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_TOKEN_LEN = 1024\n",
    "BATCH_SIZE = 110\n",
    "epochs = 100\n",
    "\n",
    "EXPERIMENT_NAME = 'cusTokenizer_WP_35k_ATW_signature'\n",
    "new_vocab_size = 35000\n",
    "# disassembly_decimal disassembly_all_number_to_words disassembly_decimal \n",
    "data_key = \"disassembly_decimal\"\n",
    "\n",
    "\n",
    "from transformers import BartTokenizer,BertTokenizer, BertForNextSentencePrediction,BertForPreTraining,BertConfig,AutoModelForMaskedLM,get_linear_schedule_with_warmup ,BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast ,AutoModelForPreTraining ,BertGenerationEncoder, BartForConditionalGeneration,BertGenerationDecoder,EncoderDecoderModel\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"./../models/\" + EXPERIMENT_NAME)\n",
    "print(tokenizer.pad_token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd5a1a9-36cf-459c-afe7-40a1b3d2bb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions Count:  80000 \n",
      "\n",
      "['int', '*struct']\n",
      "30 \n",
      " ['*longlong', '*double', 'longlong', '*char', '*ushort', 'long', 'float', 'int', 'uint', '*bool', 'ulonglong', 'char', '*struct', '*ulonglong', '*short', 'bool', '*uchar', '*int', 'void', 'double', 'uchar', '*float', '*ulong', '*void', 'ushort', 'None', '*long', 'short', '*uint', 'ulong']\n"
     ]
    }
   ],
   "source": [
    "def generate_param_string(param_list):\n",
    "\n",
    "\n",
    "    if len(param_list)==0:\n",
    "        return 'void'\n",
    "    else:\n",
    "        return \" \".join(param_list)\n",
    "def unique_items(list_of_lists):\n",
    "    # Flatten the list of lists\n",
    "    print(list_of_lists[3].split())\n",
    "    flattened_list = [item for sublist in list_of_lists for item in sublist.split()]\n",
    "    # Convert the flattened list to a set to remove duplicates\n",
    "    unique_elements = set(flattened_list)\n",
    "    return list(unique_elements)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DATA_PATH = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions'\n",
    "\n",
    "TRAIN_DATA_PATH  ='/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/train/'\n",
    "\n",
    "TEST_DATA_PATH   = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/test/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_json_files = [os.path.join(TRAIN_DATA_PATH, f) for f in os.listdir(TRAIN_DATA_PATH) ]\n",
    "\n",
    "test_json_files = [os.path.join(TEST_DATA_PATH, f) for f in os.listdir(TEST_DATA_PATH) ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_corpus(json_files):\n",
    "\n",
    "    all = []\n",
    "    all_params = []\n",
    "    all_returns = []\n",
    "    all_signatures = []\n",
    "\n",
    "    for k, j_file in enumerate(json_files):\n",
    "        # if k>1000:\n",
    "        #     break\n",
    "        try:\n",
    "\n",
    "            with open(j_file, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                funct = data[data_key]['input']\n",
    "\n",
    "                # if len(data['signature']['paramTypes'])<5:\n",
    "                all.append(funct)\n",
    "                param_string = generate_param_string( data['signature']['paramTypes'] )\n",
    "                all_params.append( param_string )\n",
    "                all_returns.append(  data['signature']['returnType']  )\n",
    "                all_signatures.append(param_string + \" [SEP] \"+ data['signature']['returnType'] )\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "    return all , all_params , all_returns , all_signatures\n",
    "    \n",
    "\n",
    "\n",
    "train_text , train_params , train_returns, train_signatures = read_corpus(train_json_files)\n",
    "\n",
    "test_text  , test_params  , test_returns , test_signatures = read_corpus(test_json_files)\n",
    "\n",
    "\n",
    "        \n",
    "# text = text[0:5000]\n",
    "print(\"Functions Count: \",len(train_text), '\\n')\n",
    "example = train_text[10]\n",
    "text = train_text + test_text\n",
    "\n",
    "\n",
    "unique_types = unique_items(train_params)\n",
    "print(len(unique_types), '\\n',unique_types)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33d9aa6-251c-4ace-a92a-7037d7c36ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "train_source_encodings = tokenize(train_text)\n",
    "train_target_encodings = tokenize(train_signatures)\n",
    "\n",
    "\n",
    "test_source_encodings = tokenize(test_text)\n",
    "test_target_encodings = tokenize(test_signatures)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(train_source_encodings['input_ids'], train_source_encodings['attention_mask'], train_target_encodings['input_ids'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "test_dataset = TensorDataset(test_source_encodings['input_ids'], test_source_encodings['attention_mask'], test_target_encodings['input_ids'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37578ca9-ed2d-4861-b0ef-e190a62c9b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8630a7f-854d-49a3-990f-e91f1c12151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\", forced_bos_token_id=0)\n",
    "# tok = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "# example_english_phrase = \"UN Chief Says There Is No <mask> in Syria\"\n",
    "# batch = tok(example_english_phrase, return_tensors=\"pt\")\n",
    "# generated_ids = model.generate(batch[\"input_ids\"])\n",
    "# assert tok.batch_decode(generated_ids, skip_special_tokens=True) == [\n",
    "#     \"UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2fd5b8-233c-439d-94a7-96902bc2a488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358add79-a5dc-4b80-8ba7-73cbbf3de4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LongformerModel, LongformerTokenizer\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "# model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "# # Example of how to prepare inputs\n",
    "# inputs = tokenizer(\"Example of a long input sequence \" * 50, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "# output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf9ba31-668a-4b0c-ad01-182c3976105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token_id = tokenizer.sep_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "\n",
    "\n",
    "\n",
    "# def align_output(pred, gt):\n",
    "#     params_pred = []\n",
    "#     params_gt = []\n",
    "#     ret_pred = []\n",
    "#     ret_gt = []\n",
    "#     for i,gid in enumerate(gt):\n",
    "#         if gid!= cls_token_id:\n",
    "            \n",
    "            \n",
    "#             aligned_pred.append(pred[i])\n",
    "#             aligned_gt.append(gid)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61a0e9fb-9576-447e-9229-b1199ec4b83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d79dec-3b92-42a4-8fd9-762179256ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_tokens:  ['[CLS]', 'void', '[SEP]', 'int', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "gt_tokens:  ['[CLS]', 'void', '[SEP]', 'int', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "epoch:  0  n:  200  loss  0.09616214781999588\n",
      "68216 68216\n",
      "[178, 165, 178, 165, 178, 165, 178, 165, 178, 165]\n",
      "[178, 165, 178, 165, 178, 165, 178, 178, 178, 178]\n",
      "Train precision, recall, f1 0.4911262675386279 0.4268060279113404 0.4480499531876857\n",
      "[2, 178, 3, 165, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 178, 3, 165, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_tokens:  ['[CLS]', 'void', '[SEP]', 'struct', '[SEP]', 'int', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "gt_tokens:  ['[CLS]', 'int', '*', 'struct', '[SEP]', 'int', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "epoch:  0  n:  400  loss  0.050350528210401535\n",
      "136041 136041\n",
      "[178, 165, 178, 165, 178, 165, 178, 178, 178, 165]\n",
      "[178, 165, 178, 165, 178, 165, 178, 165, 178, 165]\n",
      "Train precision, recall, f1 0.5725804879603942 0.512764534221301 0.5308191428048432\n",
      "[2, 165, 5, 215, 3, 165, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 178, 3, 215, 3, 165, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):  # Number of epochs\n",
    "    model.train()\n",
    "    all_prediction = []\n",
    "    all_ground_truth=[]\n",
    "    for n,batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "        # print(input_ids.shape , attention_mask.shape , labels.shape )\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        token_predictions = torch.argmax(outputs.logits, axis=-1)\n",
    "\n",
    "\n",
    "                    \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        labels_list = labels.tolist()\n",
    "        token_predictions_list = token_predictions.tolist()\n",
    "        \n",
    "        for  j,label in enumerate(labels_list):\n",
    "            for i, lid in enumerate(label):\n",
    "                if lid not in  [cls_token_id , sep_token_id , pad_token_id]:\n",
    "\n",
    "                    all_ground_truth.append(lid)\n",
    "                    all_prediction.append(token_predictions_list[j][i])\n",
    "                            \n",
    "\n",
    "        if n%200==0 and n!=0:\n",
    "            # print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "            \n",
    "            # print(labels[0].tolist())\n",
    "            # print(token_predictions[0])\n",
    "\n",
    "            \n",
    "            pred_tokens = tokenizer.convert_ids_to_tokens(token_predictions[0].tolist())\n",
    "            gt_tokens = tokenizer.convert_ids_to_tokens(labels[0].tolist())\n",
    "        \n",
    "            print(\"pred_tokens: \",pred_tokens)\n",
    "            print(\"gt_tokens: \",gt_tokens)\n",
    "        \n",
    "\n",
    "            print ('epoch: ',epoch , ' n: ', n , ' loss ',loss.item())\n",
    "            print(len(all_ground_truth) , len(all_prediction))\n",
    "            print(all_ground_truth[-10:])\n",
    "            print(all_prediction[-10:])\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(all_ground_truth,all_prediction ,average='weighted')\n",
    "            print('Train precision, recall, f1' ,precision, recall, f1)\n",
    "        \n",
    "            print(labels_list[0])\n",
    "            print(token_predictions_list[0])\n",
    "    \n",
    "\n",
    "    model.save_pretrained(\"./../models/\"+EXPERIMENT_NAME+\"_model.ckpt\")\n",
    "    print('saved')\n",
    "    continue\n",
    "##########################################\n",
    "    #eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_all_ground_truth = []\n",
    "        test_all_prediction = []\n",
    "        right = 0\n",
    "        wrong = 0\n",
    "        for n,test_batch in enumerate(test_loader):\n",
    "    \n",
    "            test_input_ids, test_attention_mask, test_labels = [x.to(device) for x in test_batch]\n",
    "            # print(input_ids.shape , attention_mask.shape , labels.shape )\n",
    "            test_outputs = model(input_ids=test_input_ids, attention_mask=test_attention_mask)\n",
    "            test_token_predictions = torch.argmax(test_outputs.logits, axis=-1)\n",
    "    \n",
    "            test_labels_list = test_labels.tolist()\n",
    "            test_token_predictions_list = test_token_predictions.tolist()\n",
    "            \n",
    "            for  j,test_label in enumerate(test_labels_list):\n",
    "                if test_label==test_token_predictions_list[j]:\n",
    "                    right = right +1\n",
    "                else:\n",
    "                    wrong = wrong+ 1\n",
    "                for i, lid in enumerate(test_label):\n",
    "                    if lid not in  [cls_token_id , sep_token_id , pad_token_id]:\n",
    "                        test_all_ground_truth.append(lid)\n",
    "                        test_all_prediction.append(test_token_predictions_list[j][i])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        print(\"# # #\" *10)\n",
    "        # test_pred_tokens = tokenizer.convert_ids_to_tokens(test_token_predictions[0].tolist())\n",
    "        # test_gt_tokens = tokenizer.convert_ids_to_tokens(test_labels[0].tolist())\n",
    "        # print('test_pred_tokens' , test_pred_tokens)\n",
    "        # print('test_gt_tokens' ,test_gt_tokens)\n",
    "        print ('epoch: ',epoch , ' n: ', n )\n",
    "        print(len(test_all_ground_truth) , len(test_all_prediction))\n",
    "        print(test_all_ground_truth[-10:])\n",
    "        print(test_all_prediction[-10:])\n",
    "        test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(test_all_ground_truth,test_all_prediction ,average='weighted')\n",
    "        print('TEST precision, recall, f1' ,test_precision, test_recall, test_f1)\n",
    "    \n",
    "        print(test_labels_list[0])\n",
    "        print(test_token_predictions_list[0])\n",
    "        print(\"right\",right, \"  wrong : \", wrong)\n",
    "    \n",
    "        # Save the model\n",
    "    \n",
    "        model.save_pretrained(\"./../models/\"+EXPERIMENT_NAME+\"_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d99f0-476e-4a01-a056-d3cec482ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token_id = tokenizer.sep_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ed88a9-4f8c-444f-99f8-f214df133a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
