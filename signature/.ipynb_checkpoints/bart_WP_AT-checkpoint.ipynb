{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48b4579f-cccb-4423-a0d9-f48d35e374bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys,os\n",
    "from elftools.elf.elffile import ELFFile\n",
    "from elftools.elf.segments import Segment\n",
    "from capstone import *\n",
    "from capstone.x86 import *\n",
    "\n",
    "import os\n",
    "import json \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import sys,os\n",
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f9e3ca-865a-481a-ae4c-89a5c21cac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_TOKEN_LEN = 1024\n",
    "BATCH_SIZE = 150\n",
    "epochs = 100\n",
    "\n",
    "EXPERIMENT_NAME = 'cusTokenizer_WP_35k_ATW_signature'\n",
    "new_vocab_size = 35000\n",
    "# disassembly_decimal disassembly_all_number_to_words disassembly_decimal \n",
    "data_key = \"disassembly_decimal\"\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction,BertForPreTraining,BertConfig,AutoModelForMaskedLM,get_linear_schedule_with_warmup ,BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast ,AutoModelForPreTraining ,BertGenerationEncoder, BartForConditionalGeneration,BertGenerationDecoder,EncoderDecoderModel\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"./../models/\" + EXPERIMENT_NAME)\n",
    "print(tokenizer.pad_token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd5a1a9-36cf-459c-afe7-40a1b3d2bb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions Count:  4943 \n",
      "\n",
      "['int', '*struct']\n",
      "25 \n",
      " ['float', '*void', '*double', 'None', 'ulong', 'longlong', 'double', 'void', '*longlong', '*char', 'int', 'uchar', 'uint', '*long', 'ulonglong', 'long', '*uint', '*int', 'ushort', 'char', '*struct', 'bool', '*uchar', 'short', '*float']\n"
     ]
    }
   ],
   "source": [
    "def generate_param_string(param_list):\n",
    "\n",
    "\n",
    "    if len(param_list)==0:\n",
    "        return 'void'\n",
    "    else:\n",
    "        return \" \".join(param_list)\n",
    "def unique_items(list_of_lists):\n",
    "    # Flatten the list of lists\n",
    "    print(list_of_lists[3].split())\n",
    "    flattened_list = [item for sublist in list_of_lists for item in sublist.split()]\n",
    "    # Convert the flattened list to a set to remove duplicates\n",
    "    unique_elements = set(flattened_list)\n",
    "    return list(unique_elements)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DATA_PATH = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions'\n",
    "\n",
    "TRAIN_DATA_PATH  ='/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/train/'\n",
    "\n",
    "TEST_DATA_PATH   = '/home/raisul/ANALYSED_DATA/tokenization_data_single_functions/test/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_json_files = [os.path.join(TRAIN_DATA_PATH, f) for f in os.listdir(TRAIN_DATA_PATH) ]\n",
    "\n",
    "test_json_files = [os.path.join(TEST_DATA_PATH, f) for f in os.listdir(TEST_DATA_PATH) ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_corpus(json_files):\n",
    "\n",
    "    all = []\n",
    "    all_params = []\n",
    "    all_returns = []\n",
    "    all_signatures = []\n",
    "\n",
    "    for k, j_file in enumerate(json_files):\n",
    "        if k>5000:\n",
    "            break\n",
    "        try:\n",
    "\n",
    "            with open(j_file, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                funct = data[data_key]['input']\n",
    "\n",
    "                if len(data['signature']['paramTypes'])<5:\n",
    "                    all.append(funct)\n",
    "                    param_string = generate_param_string( data['signature']['paramTypes'] )\n",
    "                    all_params.append( param_string )\n",
    "                    all_returns.append(  data['signature']['returnType']  )\n",
    "                    all_signatures.append(param_string + \" [SEP] \"+ data['signature']['returnType'] )\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "    return all , all_params , all_returns , all_signatures\n",
    "    \n",
    "\n",
    "\n",
    "train_text , train_params , train_returns, train_signatures = read_corpus(train_json_files)\n",
    "\n",
    "test_text  , test_params  , test_returns , test_signatures = read_corpus(test_json_files)\n",
    "\n",
    "\n",
    "        \n",
    "# text = text[0:5000]\n",
    "print(\"Functions Count: \",len(train_text), '\\n')\n",
    "example = train_text[10]\n",
    "text = train_text + test_text\n",
    "\n",
    "\n",
    "unique_types = unique_items(train_params)\n",
    "print(len(unique_types), '\\n',unique_types)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33d9aa6-251c-4ace-a92a-7037d7c36ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "train_source_encodings = tokenize(train_text)\n",
    "train_target_encodings = tokenize(train_signatures)\n",
    "\n",
    "\n",
    "test_source_encodings = tokenize(test_text)\n",
    "test_target_encodings = tokenize(test_signatures)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(train_target_encodings['input_ids'], train_target_encodings['attention_mask'], train_target_encodings['input_ids'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "test_dataset = TensorDataset(test_target_encodings['input_ids'], test_target_encodings['attention_mask'], test_target_encodings['input_ids'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37578ca9-ed2d-4861-b0ef-e190a62c9b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8630a7f-854d-49a3-990f-e91f1c12151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\", forced_bos_token_id=0)\n",
    "# tok = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "# example_english_phrase = \"UN Chief Says There Is No <mask> in Syria\"\n",
    "# batch = tok(example_english_phrase, return_tensors=\"pt\")\n",
    "# generated_ids = model.generate(batch[\"input_ids\"])\n",
    "# assert tok.batch_decode(generated_ids, skip_special_tokens=True) == [\n",
    "#     \"UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2fd5b8-233c-439d-94a7-96902bc2a488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358add79-a5dc-4b80-8ba7-73cbbf3de4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LongformerModel, LongformerTokenizer\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "# model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "# # Example of how to prepare inputs\n",
    "# inputs = tokenizer(\"Example of a long input sequence \" * 50, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "# output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdf9ba31-668a-4b0c-ad01-182c3976105d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1171030825.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    def align_output():\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "sep_token_id = tokenizer.sep_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "\n",
    "\n",
    "\n",
    "# def align_output(pred, gt):\n",
    "#     params_pred = []\n",
    "#     params_gt = []\n",
    "#     ret_pred = []\n",
    "#     ret_gt = []\n",
    "#     for i,gid in enumerate(gt):\n",
    "#         if gid!= cls_token_id:\n",
    "            \n",
    "            \n",
    "#             aligned_pred.append(pred[i])\n",
    "#             aligned_gt.append(gid)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00d79dec-3b92-42a4-8fd9-762179256ae3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3560078474.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 25\u001b[0;36m\u001b[0m\n\u001b[0;31m    for\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):  # Number of epochs\n",
    "\n",
    "    all_prediction = []\n",
    "    all_ground_truth=[]\n",
    "    for n,batch in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "        # print(input_ids.shape , attention_mask.shape , labels.shape )\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        token_predictions = torch.argmax(outputs.logits, axis=-1)\n",
    "\n",
    "        for  label in labels:\n",
    "            for i, lid in enumerate(label):\n",
    "                if lid not in  [sep_token_id ,cls_token_id]:\n",
    "                    all_ground_truth.append(lid)\n",
    "                    all_prediction.append(token_predictions[i])\n",
    "                    \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if n%100:\n",
    "            # print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "            \n",
    "            # print(labels[0].tolist())\n",
    "            # print(token_predictions[0])\n",
    "\n",
    "            \n",
    "            # pred_tokens = tokenizer.convert_ids_to_tokens(token_predictions[0].tolist())\n",
    "\n",
    "            # gt_tokens = tokenizer.convert_ids_to_tokens(labels[0].tolist())\n",
    "\n",
    "            # print(\"pred_tokens: \",pred_tokens)\n",
    "            # print(\"gt_tokens: \",gt_tokens)\n",
    "\n",
    "            print'epoch: ',epoch , ' n: ', n , ' loss ',loss.item())\n",
    "            precision_recall_fscore_support(v_masked_token_ground_truths_all,v_masked_token_predictions_all,average='weighted')\n",
    "\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a71d99f0-476e-4a01-a056-d3cec482ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1 2\n"
     ]
    }
   ],
   "source": [
    "sep_token_id = tokenizer.sep_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c035342-310a-49bd-8e0d-4b3b7867e11a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
